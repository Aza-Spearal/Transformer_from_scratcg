{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multihead(nn.Module):\n",
    "    def __init__(self, dmodel, h):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dk=dmodel/h #queries & keys dimension\n",
    "\n",
    "        assert (\n",
    "            self.dk*h == dmodel\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.dk = int(self.dk)\n",
    "        self.dv = self.dk #values dimension\n",
    "\n",
    "        self.h = h\n",
    "\n",
    "        self.Wq = nn.Linear(dmodel, dmodel, bias=False)\n",
    "        self.Wk = nn.Linear(dmodel, dmodel, bias=False)\n",
    "        self.Wv = nn.Linear(dmodel, dmodel, bias=False)\n",
    "        self.Wo = nn.Linear(dmodel, dmodel, bias=False)\n",
    "\n",
    "    def attention(self, q, k, v, mask=None): #vect de dim 64\n",
    "        product = torch.matmul(q, k.transpose(-2, -1)) # (2, 8, 9, 64) . (2, 8, 64, 9) = (2, 8, 9, 9)\n",
    "        \n",
    "        if mask is not None:\n",
    "            product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "            \n",
    "        product = product / math.sqrt(self.dk)\n",
    "        score = torch.softmax(product, dim=-1)\n",
    "        return torch.matmul(score, v)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        h = self.h\n",
    "        dv= self.dv\n",
    "        dk= self.dk\n",
    "\n",
    "        b, t, dmod = k.size()\n",
    "        t_q= q.size(1)\n",
    "\n",
    "        q = self.Wq(q)\n",
    "        k = self.Wk(k)\n",
    "        v = self.Wv(v)\n",
    "        \n",
    "        q = q.view(b, t_q, h, dk).transpose(1, 2)\n",
    "        k = k.view(b, t, h, dk).transpose(1, 2)\n",
    "        v = v.view(b, t, h, dv).transpose(1, 2)\n",
    "\n",
    "        out = self.attention(q, k, v, mask)\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t_q, dmod)\n",
    "        return self.Wo(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feedForward(nn.Module):\n",
    "    def __init__(self, dmodel, dff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(dmodel, dff)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(dff, dmodel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dmodel, h, dff, pdrop):\n",
    "        super().__init__()\n",
    "        self.multihead = multihead(dmodel, h)\n",
    "        self.FF = feedForward(dmodel, dff)\n",
    "        self.layerNorm1 = nn.LayerNorm(dmodel)\n",
    "        self.layerNorm2 = nn.LayerNorm(dmodel)\n",
    "        self.dropout = nn.Dropout(pdrop)\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        subL1 = self.multihead(q, k, v, mask)\n",
    "        y = self.layerNorm1(q + self.dropout(subL1))\n",
    "        subL2 = self.FF(y)\n",
    "        y = self.layerNorm2(y + self.dropout(subL2))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dmodel, h, dff, pdrop):\n",
    "        super().__init__()\n",
    "        self.multihead = multihead(dmodel, h)\n",
    "        self.norm = nn.LayerNorm(dmodel)\n",
    "        self.dropout = nn.Dropout(pdrop)\n",
    "        self.block = TransformerBlock(dmodel, h, dff, pdrop)\n",
    "        \n",
    "    def forward(self, x, enc_output, trg_mask, src_mask):\n",
    "        mask_att = self.multihead(x, x, x, trg_mask)\n",
    "        y = self.norm(x + self.dropout(mask_att))\n",
    "        y = self.block(y, enc_output, enc_output, src_mask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N, dmodel, h, dff, pdrop, src_vocab_size, trg_vocab_size, max_seq_length):\n",
    "        super().__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, dmodel)\n",
    "        self.trg_embedding = nn.Embedding(trg_vocab_size, dmodel)\n",
    "        self.positional_encoding = PositionalEncoding(dmodel, max_seq_length)\n",
    "\n",
    "        self.encoder = nn.ModuleList([TransformerBlock(dmodel, h, dff, pdrop) for _ in range(N)])\n",
    "        self.decoder = nn.ModuleList([Decoder(dmodel, h, dff, pdrop) for _ in range(N)])\n",
    "\n",
    "        self.linear = nn.Linear(dmodel, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(pdrop)\n",
    "\n",
    "\n",
    "    def create_mask(self, src, trg):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        trg_mask = (trg != 0).unsqueeze(1).unsqueeze(3)\n",
    "        trg_len = trg.size(1)\n",
    "        diago = torch.tril(torch.ones(1, trg_len, trg_len)).bool()\n",
    "        trg_mask = trg_mask & diago\n",
    "        return src_mask, trg_mask\n",
    "\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask, trg_mask = self.create_mask(src, trg)\n",
    "\n",
    "        src_embed = self.dropout(self.src_embedding(src) + self.positional_encoding(src))\n",
    "        trg_embed = self.dropout(self.trg_embedding(trg) + self.positional_encoding(trg))\n",
    "\n",
    "        enc_out = src_embed\n",
    "        for enc_layer in self.encoder:\n",
    "            enc_out = enc_layer(enc_out, enc_out, enc_out, src_mask)\n",
    "\n",
    "        dec_out = trg_embed\n",
    "        for dec_layer in self.decoder:\n",
    "            dec_out = dec_layer(dec_out, enc_out, trg_mask, src_mask)\n",
    "\n",
    "        output = self.linear(dec_out)\n",
    "        return torch.softmax(output, dim=2)\n",
    "    \n",
    "    def infer(self, src, trg):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self(src, trg[:,:-1])\n",
    "            #print(\"trg[:,1:]:\", trg[:,1:])\n",
    "            #print(\"Probas token:\", output[0])\n",
    "            print(\"Tokens:\", torch.argmax(output,dim=-1))\n",
    "\n",
    "    def trainer(self, src, trg, epoch):\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        optimizer = optim.Adam(self.parameters())\n",
    "        errors =  []\n",
    "        \n",
    "        src = src.type(torch.LongTensor)\n",
    "        trg = trg.type(torch.LongTensor)\n",
    "\n",
    "        self.train()\n",
    "        for epoch in range(epoch):\n",
    "            pred = self(src, trg[:,:-1])\n",
    "            #loss = criterion(pred.reshape(src.size(0)*(max_seq_length-1), vocab_size), trg[:,1:].reshape(src.size(0)*(max_seq_length-1)))\n",
    "            #loss = criterion(output.contiguous().view(-1, vocab_size), trg[:, 1:].contiguous().view(-1))\n",
    "            #loss = criterion(output.contiguous().view(-1, vocab_size), trg.contiguous().view(-1))\n",
    "            loss = criterion(pred[0], trg[0,1:])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            errors.append(loss.item())\n",
    "            print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "        plt.plot(errors)\n",
    "        plt.title(\"Errors\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "N=6\n",
    "dmodel=512\n",
    "h=8\n",
    "dff = 2048\n",
    "pdrop = 0.1\n",
    "\n",
    "vocab_size=2\n",
    "#size = 11\n",
    "#src = torch.zeros(size).unsqueeze(0).type(torch.LongTensor)\n",
    "#src = torch.ones(size).unsqueeze(0).type(torch.LongTensor)\n",
    "src = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 0, 0 , 0, 0, 0]]).type(torch.LongTensor)\n",
    "#trg = torch.ones(size).unsqueeze(0).type(torch.LongTensor)\n",
    "trg = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0]]).type(torch.LongTensor)\n",
    "max_seq_length = src.size(1)\n",
    "\n",
    "transfo = Transformer(N, dmodel, h, dff, pdrop, vocab_size, vocab_size, max_seq_length)\n",
    "transfo.infer(src, trg)\n",
    "#transfo.trainer(src, trg, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
