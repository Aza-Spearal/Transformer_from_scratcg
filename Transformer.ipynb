{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(precision=2)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multihead(nn.Module):\n",
    "    def __init__(self, dmodel, h):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dk=dmodel/h #queries & keys dimension\n",
    "\n",
    "        assert (\n",
    "            self.dk*h == dmodel\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.dk = int(self.dk)\n",
    "        self.dv = self.dk\n",
    "\n",
    "        self.h = h\n",
    "\n",
    "        self.Wq = nn.Linear(dmodel, dmodel, bias=False)\n",
    "        self.Wk = nn.Linear(dmodel, dmodel, bias=False)\n",
    "        self.Wv = nn.Linear(dmodel, dmodel, bias=False)\n",
    "        self.Wo = nn.Linear(dmodel, dmodel, bias=False)\n",
    "\n",
    "    def attention(self, q, k, v, mask=None): #vect de dim 64\n",
    "        product = torch.matmul(q, k.transpose(-2, -1)) # (2, 8, 9, 64) . (2, 8, 64, 9) = (2, 8, 9, 9)\n",
    "        if mask is not None:\n",
    "            product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "            \n",
    "        product = product / math.sqrt(self.dk)\n",
    "        score = torch.softmax(product, dim=-1)\n",
    "        return torch.matmul(score, v)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        h = self.h\n",
    "        dv= self.dv\n",
    "        dk= self.dk\n",
    "\n",
    "        b, t, dmod = k.size()\n",
    "        t_q= q.size(1)\n",
    "\n",
    "        q = self.Wq(q)\n",
    "        k = self.Wk(k)\n",
    "        v = self.Wv(v)\n",
    "\n",
    "        q = q.view(b, t_q, h, dk).transpose(1, 2)\n",
    "        k = k.view(b, t, h, dk).transpose(1, 2)\n",
    "        v = v.view(b, t, h, dv).transpose(1, 2)\n",
    "\n",
    "        out = self.attention(q, k, v, mask)\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t_q, dmod)\n",
    "        return self.Wo(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feedForward(nn.Module):\n",
    "    def __init__(self, dmodel, dff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(dmodel, dff)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(dff, dmodel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dmodel, h, dff, pdrop):\n",
    "        super().__init__()\n",
    "        self.multihead = multihead(dmodel, h)\n",
    "        self.FF = feedForward(dmodel, dff)\n",
    "        self.layerNorm1 = nn.LayerNorm(dmodel)\n",
    "        self.layerNorm2 = nn.LayerNorm(dmodel)\n",
    "        self.dropout1 = nn.Dropout(pdrop)\n",
    "        self.dropout2 = nn.Dropout(pdrop)\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        subL1 = self.multihead(q, k, v, mask)\n",
    "        y = self.layerNorm1(q + self.dropout1(subL1))\n",
    "        subL2 = self.FF(y)\n",
    "        y = self.layerNorm2(y + self.dropout2(subL2))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dmodel, h, dff, pdrop):\n",
    "        super().__init__()\n",
    "        self.multihead = multihead(dmodel, h)\n",
    "        self.norm = nn.LayerNorm(dmodel)\n",
    "        self.dropout = nn.Dropout(pdrop)\n",
    "        self.block = TransformerBlock(dmodel, h, dff, pdrop)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, trg_mask=None):\n",
    "        mask_att = self.multihead(x, x, x, trg_mask)\n",
    "        y = self.norm(x + self.dropout(mask_att))\n",
    "        y = self.block(y, enc_output, enc_output, src_mask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toTokens(logits):\n",
    "    return torch.argmax(logits,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler(_LRScheduler):\n",
    "    def __init__(self, optim, dmodel, warmup_steps):\n",
    "        self.dmodel = dmodel\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_param_groups = len(optim.param_groups)\n",
    "        super().__init__(optim)\n",
    "   \n",
    "    def get_lr(self):\n",
    "        step_num = self._step_count\n",
    "        dmodel = self.dmodel\n",
    "        warmup_steps = self.warmup_steps\n",
    "    \n",
    "        lrate = dmodel**(-0.5) * min(step_num**(-0.5), step_num*warmup_steps**(-1.5))\n",
    "        return [lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N, dmodel, h, dff, pdrop, src_vocab_size, trg_vocab_size, max_seq_length, BOS, EOS, PAD, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dmodel = dmodel\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.BOS = torch.tensor([BOS], dtype=torch.long, device=device)\n",
    "        self.EOS = torch.tensor([EOS], dtype=torch.long, device=device)\n",
    "        self.PAD = torch.tensor([PAD], dtype=torch.long, device=device)\n",
    "\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, dmodel)\n",
    "        self.trg_embedding = nn.Embedding(trg_vocab_size, dmodel)\n",
    "        self.positional_encoding = PositionalEncoding(dmodel, max_seq_length)\n",
    "\n",
    "        self.encoder = nn.ModuleList([TransformerBlock(dmodel, h, dff, pdrop) for _ in range(N)])\n",
    "        self.decoder = nn.ModuleList([Decoder(dmodel, h, dff, pdrop) for _ in range(N)])\n",
    "\n",
    "        self.linear = nn.Linear(dmodel, trg_vocab_size)\n",
    "        self.dropout1 = nn.Dropout(pdrop)\n",
    "        self.dropout2 = nn.Dropout(pdrop)\n",
    "\n",
    "\n",
    "    def create_mask(self, src, trg=None):\n",
    "        src_mask = (src != self.PAD).unsqueeze(1).unsqueeze(2)\n",
    "        if trg is None:\n",
    "            return src_mask\n",
    "        else:\n",
    "            trg_mask = (trg != self.PAD).unsqueeze(1).unsqueeze(3)\n",
    "            trg_len = trg.size(1)\n",
    "            diago = torch.tril(torch.ones(1, trg_len, trg_len)).bool()\n",
    "            trg_mask = trg_mask & diago\n",
    "            return src_mask, trg_mask\n",
    "\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "\n",
    "        assert (\n",
    "            src.size(0) == trg.size(0)\n",
    "        ), \"batch src and batch trg are not equals\"\n",
    "\n",
    "        src_mask, trg_mask = self.create_mask(src, trg)\n",
    "        \n",
    "        src_embed = self.dropout1(self.src_embedding(src) + self.positional_encoding(src))\n",
    "        trg_embed = self.dropout2(self.trg_embedding(trg) + self.positional_encoding(trg))\n",
    "\n",
    "        enc_out = src_embed\n",
    "        for enc_layer in self.encoder:\n",
    "            enc_out = enc_layer(enc_out, enc_out, enc_out, src_mask)\n",
    "\n",
    "        dec_out = trg_embed\n",
    "        for dec_layer in self.decoder:\n",
    "            dec_out = dec_layer(dec_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        output = self.linear(dec_out)\n",
    "        return torch.softmax(output, dim=2)\n",
    "\n",
    "\n",
    "    def trainer(self, src, trg, epoch=1, printersrc='My father'):\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=self.PAD, label_smoothing = 0.1)\n",
    "        optimizer = optim.Adam(self.parameters() ,betas = (0.9, 0.98), eps = 1.0e-9)\n",
    "        scheduler = Scheduler(optimizer, self.dmodel, warmup_steps = 4000)\n",
    "\n",
    "        errors_list =  []\n",
    "\n",
    "        self.train()\n",
    "        for epoch in range(epoch):\n",
    "            error = 0\n",
    "            for batch in range(src.size(0)):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                pred = self(src[batch].unsqueeze(0), trg[batch,:-1].unsqueeze(0))\n",
    "                loss = criterion(pred[0], trg[batch,1:])\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "                error += loss.item()\n",
    "\n",
    "            print(f\"Epoch: {epoch+1}, Loss: {error}\")\n",
    "            errors_list.append(error)\n",
    "\n",
    "        plt.plot(errors_list)\n",
    "        plt.title(\"Errors\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def predict(self, src):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            BOS = self.BOS\n",
    "            Tokens_ids = BOS.repeat(src.size(0), 1)#create the batch of sentence predict, each batch begin with BOS\n",
    "\n",
    "            src_mask = self.create_mask(src)\n",
    "            src_embed = self.dropout1(self.src_embedding(src) + self.positional_encoding(src))\n",
    "\n",
    "            enc_out = src_embed\n",
    "            \n",
    "            for enc_layer in self.encoder:\n",
    "                enc_out = enc_layer(enc_out, enc_out, enc_out, src_mask)\n",
    "\n",
    "            for i in range(self.max_seq_length-1):\n",
    "            \n",
    "                dec_out = self.dropout2(self.trg_embedding(Tokens_ids) + self.positional_encoding(Tokens_ids))\n",
    "                for dec_layer in self.decoder:\n",
    "                    dec_out = dec_layer(dec_out, enc_out, src_mask)\n",
    "\n",
    "                dec_out = self.linear(dec_out)\n",
    "                transfo_out = torch.softmax(dec_out, dim=2)\n",
    "\n",
    "                transfo_out = transfo_out[:,-1,:]#we take the last token infered\n",
    "\n",
    "                tok = toTokens(transfo_out)\n",
    "\n",
    "                Tokens_ids = torch.cat((Tokens_ids, tok.unsqueeze(1)), dim=1)\n",
    "            \n",
    "            return Tokens_ids[:, 1:]# vire le bos\n",
    "\n",
    "\n",
    "    def infer(self, src, trg):#infer the last token of the trg, knowing the src and all except the last token of trg\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self(src, trg[:,:-1])\n",
    "            return toTokens(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=6; dmodel=512; h=8; dff=2048; pdrop=0.1; device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.tensor([[1, 2, 3, 4, 0], [1, 3, 4, 5, 0], [1, 4, 5, 6, 0], [1, 5, 6, 7, 0], [1, 6, 7, 8, 0], [1, 7, 8, 9, 0]], dtype=torch.long, device=device)\n",
    "trg = torch.tensor([[1, 4, 6, 8, 0], [1, 6, 8, 10, 0], [1, 8, 10, 12, 0], [1, 10, 12, 14, 0], [1, 12, 14, 16, 0], [1, 14, 16, 18, 0]], dtype=torch.long, device=device)\n",
    "\n",
    "BOS=1; EOS=0; PAD=0\n",
    "max_length=5\n",
    "\n",
    "src_vocab_size = 20\n",
    "trg_vocab_size = 20\n",
    "\n",
    "transfo = Transformer(N, dmodel, h, dff, pdrop, src_vocab_size, trg_vocab_size, max_length, BOS, EOS, PAD, device)\n",
    "transfo.trainer(src, trg, 50)\n",
    "transfo.predict(src)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
