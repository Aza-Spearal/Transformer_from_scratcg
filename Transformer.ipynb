{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class multihead(nn.Module):\n",
    "    def __init__(self, dmodel, h):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dk=dmodel/h #queries & keys dimension\n",
    "\n",
    "        assert (\n",
    "            self.dk*h == dmodel\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.dk = int(self.dk)\n",
    "        self.dv = self.dk\n",
    "\n",
    "        self.h = h\n",
    "\n",
    "        self.Wq = nn.Linear(dmodel, dmodel, bias=False)\n",
    "        self.Wk = nn.Linear(dmodel, dmodel, bias=False)\n",
    "        self.Wv = nn.Linear(dmodel, dmodel, bias=False)\n",
    "        self.Wo = nn.Linear(dmodel, dmodel, bias=False)\n",
    "\n",
    "        self.attn_pattern = None\n",
    "\n",
    "    def attention(self, q, k, v, mask=None): #vect of size dmodel/h\n",
    "        product = torch.matmul(q, k.transpose(-2, -1)) # [batch, h, seq, dmodel/h] . [batch, h, dmodel/h, seq] = [batch, h, seq, seq]\n",
    "\n",
    "        if mask is not None:\n",
    "            product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "            \n",
    "        product = product / math.sqrt(self.dk)\n",
    "        score = torch.softmax(product, dim=-1)\n",
    "        \n",
    "        return torch.matmul(score, v) # [batch, h, seq, seq] . [batch, h, seq, dmodel/h] = [batch, h, seq, dmodel/h]\n",
    "\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        h = self.h\n",
    "        dv= self.dv\n",
    "        dk= self.dk\n",
    "\n",
    "        b, t, dmod = k.size()\n",
    "        t_q= q.size(1)\n",
    "\n",
    "        q = self.Wq(q)\n",
    "        k = self.Wk(k)\n",
    "        v = self.Wv(v)\n",
    "\n",
    "        q = q.view(b, t_q, h, dk).transpose(1, 2) # [batch, seq, dmodel] -> [batch, h, seq, dmodel/h]\n",
    "        k = k.view(b, t, h, dk).transpose(1, 2)\n",
    "        v = v.view(b, t, h, dv).transpose(1, 2)\n",
    "        \n",
    "        out = self.attention(q, k, v, mask)\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t_q, dmod) # [batch, h, seq, dmodel/h] -> [batch, seq, dmodel]\n",
    "        out = self.Wo(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class feedForward(nn.Module):\n",
    "    def __init__(self, dmodel, dff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(dmodel, dff)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(dff, dmodel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dmodel, h, dff, pdrop):\n",
    "        super().__init__()\n",
    "        self.multihead = multihead(dmodel, h)\n",
    "        self.layerNorm1 = nn.LayerNorm(dmodel)\n",
    "        self.FF = feedForward(dmodel, dff)\n",
    "        self.layerNorm2 = nn.LayerNorm(dmodel)\n",
    "        self.dropout1 = nn.Dropout(pdrop)\n",
    "        self.dropout2 = nn.Dropout(pdrop)\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        subL1 = self.multihead(q, k, v, mask)\n",
    "        y = self.layerNorm1(q + self.dropout1(subL1))\n",
    "        subL2 = self.FF(y)\n",
    "        y = self.layerNorm2(y + self.dropout2(subL2))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dmodel, h, dff, pdrop):\n",
    "        super().__init__()\n",
    "        self.multihead = multihead(dmodel, h)\n",
    "        self.norm = nn.LayerNorm(dmodel)\n",
    "        self.dropout = nn.Dropout(pdrop)\n",
    "        self.block = TransformerBlock(dmodel, h, dff, pdrop)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, trg_mask=None):\n",
    "        mask_att = self.multihead(x, x, x, trg_mask)\n",
    "        y = self.norm(x + self.dropout(mask_att))\n",
    "        y = self.block(y, enc_output, enc_output, src_mask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Scheduler(_LRScheduler): #mainly inspired by https://kikaben.com/transformers-training-details/\n",
    "    def __init__(self, optim, dmodel, warmup_steps):\n",
    "        self.dmodel = dmodel\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_param_groups = len(optim.param_groups)\n",
    "        super().__init__(optim)\n",
    "   \n",
    "    def get_lr(self):\n",
    "        step_num = self._step_count\n",
    "        dmodel = self.dmodel\n",
    "        warmup_steps = self.warmup_steps\n",
    "    \n",
    "        lrate = (dmodel**(-0.5) * min(step_num**(-0.5), step_num*warmup_steps**(-1.5)))*1\n",
    "        return [lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def toTokens(logits):\n",
    "    return torch.argmax(logits,dim=-1)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N, dmodel, h, dff, pdrop, src_vocab_size, trg_vocab_size, max_seq_length, PAD_src, PAD_trg, BOS_trg, EOS_trg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dmodel = dmodel\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        self.PAD_src = PAD_src\n",
    "        self.PAD_trg = PAD_trg\n",
    "        self.BOS_trg = BOS_trg\n",
    "        self.EOS_trg = EOS_trg\n",
    "\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, dmodel)\n",
    "        self.trg_embedding = nn.Embedding(trg_vocab_size, dmodel)\n",
    "        self.positional_encoding = PositionalEncoding(dmodel, max_seq_length)\n",
    "\n",
    "        self.encoder = nn.ModuleList([TransformerBlock(dmodel, h, dff, pdrop) for _ in range(N)])\n",
    "        self.decoder = nn.ModuleList([Decoder(dmodel, h, dff, pdrop) for _ in range(N)])\n",
    "\n",
    "        self.linear = nn.Linear(dmodel, trg_vocab_size)\n",
    "        self.dropout1 = nn.Dropout(pdrop)\n",
    "        self.dropout2 = nn.Dropout(pdrop)\n",
    "\n",
    "\n",
    "    def create_mask(self, src, trg=None):\n",
    "        src_mask = (src != self.PAD_src).unsqueeze(1).unsqueeze(2)\n",
    "        if trg is None:\n",
    "            return src_mask\n",
    "        else:\n",
    "            trg_mask = (trg != self.PAD_trg).unsqueeze(1).unsqueeze(3)\n",
    "            trg_len = trg.size(1)\n",
    "            diago = torch.tril(torch.ones(1, trg_len, trg_len, device=device)).bool()\n",
    "            trg_mask = trg_mask & diago\n",
    "            return src_mask, trg_mask\n",
    "\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "\n",
    "        assert (\n",
    "            src.size(0) == trg.size(0)\n",
    "        ), \"batch src and batch trg are not equals\"\n",
    "\n",
    "        src_mask, trg_mask = self.create_mask(src, trg)\n",
    "        \n",
    "        src_embed = self.dropout1(self.src_embedding(src) + self.positional_encoding(src))\n",
    "        trg_embed = self.dropout2(self.trg_embedding(trg) + self.positional_encoding(trg))\n",
    "\n",
    "        enc_out = src_embed\n",
    "        for enc_layer in self.encoder:\n",
    "            enc_out = enc_layer(enc_out, enc_out, enc_out, src_mask)\n",
    "\n",
    "        dec_out = trg_embed\n",
    "        for dec_layer in self.decoder:\n",
    "            dec_out = dec_layer(dec_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        output = self.linear(dec_out)\n",
    "        \n",
    "        return output \n",
    "\n",
    "\n",
    "    def infer(self, src, trg):#infer the last token of the trg, knowing the src and all except the last token of trg\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self(src, trg[:,:-1])\n",
    "            return toTokens(output)\n",
    "\n",
    "\n",
    "    def trainer(self, dataloader, epoch=1):\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=self.PAD_trg, label_smoothing = 0.1)\n",
    "        optimizer = optim.Adam(self.parameters() ,betas = (0.9, 0.98), eps = 1.0e-9)\n",
    "        scheduler = Scheduler(optimizer, self.dmodel, warmup_steps = 4000)\n",
    "\n",
    "        errors_list =  []\n",
    "\n",
    "        self.train()\n",
    "        for epoch in range(epoch):\n",
    "            error = 0\n",
    "\n",
    "            for i, (src, trg) in enumerate(dataloader):\n",
    "                optimizer.zero_grad()\n",
    "                pred = self(src, trg[:,:-1])\n",
    "                pred = pred.contiguous().view(-1, trg_vocab_size)\n",
    "                y = trg[:,1:].contiguous().view(-1)\n",
    "                loss = criterion(pred, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "                error += loss.item()\n",
    "\n",
    "                print('Batch', i, ', Loss', error)\n",
    "                errors_list.append(error)\n",
    "                error = 0\n",
    "\n",
    "        plt.plot(errors_list)\n",
    "        plt.title(\"Errors\")\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, src):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            src = src_tokenizer(src, max_length=self.max_seq_length, padding='max_length', truncation=True)\n",
    "            src = torch.tensor(src['input_ids'], device=device)\n",
    "            enc_out = self.dropout1(self.src_embedding(src) + self.positional_encoding(src))\n",
    "            \n",
    "            src_mask = self.create_mask(src)\n",
    "            for enc_layer in self.encoder:\n",
    "                enc_out = enc_layer(enc_out, enc_out, enc_out, src_mask)\n",
    "\n",
    "            Tokens_ids = torch.tensor(self.BOS_trg).repeat(1, 1).to(device) #beginning of the sentence to predict\n",
    "\n",
    "            for _ in range(self.max_seq_length):\n",
    "\n",
    "                dec_out = self.dropout2(self.trg_embedding(Tokens_ids) + self.positional_encoding(Tokens_ids))\n",
    "\n",
    "                _, trg_mask = self.create_mask(src, Tokens_ids)\n",
    "                for dec_layer in self.decoder:\n",
    "                    dec_out = dec_layer(dec_out, enc_out, src_mask, trg_mask)\n",
    "                    \n",
    "                transfo_out = self.linear(dec_out)\n",
    "\n",
    "                transfo_out = transfo_out[:,-1:]\n",
    "                tok = toTokens(transfo_out)\n",
    "\n",
    "                if tok.item() == self.EOS_trg:\n",
    "                    break\n",
    "\n",
    "                Tokens_ids = torch.cat((Tokens_ids, tok), dim=1)\n",
    "\n",
    "            out = Tokens_ids[:, 1:]# delete the bos\n",
    "            return trg_tokenizer.batch_decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "src_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "trg_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "src_vocab_size = src_tokenizer.vocab_size\n",
    "trg_vocab_size = trg_tokenizer.vocab_size\n",
    "\n",
    "PAD_src = src_tokenizer.pad_token_id\n",
    "PAD_trg = trg_tokenizer.pad_token_id\n",
    "#BOS_trg = trg_tokenizer.bos_token_id #trg_tokenizer.cls_token_id\n",
    "#EOS_trg = trg_tokenizer.eos_token_id #trg_tokenizer.sep_token_id\n",
    "BOS_trg = trg_tokenizer.cls_token_id\n",
    "EOS_trg = trg_tokenizer.sep_token_id\n",
    "\n",
    "N=6; dmodel=512; h=8; dff=2048; pdrop=0.1; max_length = 40; device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from datasets import load_dataset #original size=127085\n",
    "books = load_dataset(\"opus_books\", \"en-fr\")['train']['translation']#[:100]\n",
    "en_books = [example['en'] for example in books]\n",
    "fr_books = [example['fr'] for example in books]\n",
    "\n",
    "src = src_tokenizer(en_books, max_length=max_length, padding='max_length', truncation=True)\n",
    "trg = trg_tokenizer(fr_books, max_length=max_length, padding='max_length', truncation=True)\n",
    "\n",
    "src = torch.tensor(src['input_ids'], device=device)\n",
    "trg = torch.tensor(trg['input_ids'], device=device)\n",
    "\n",
    "dataset = TensorDataset(src, trg)\n",
    "train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 , Loss 10.448636054992676\n",
      "Batch 1 , Loss 10.466205596923828\n",
      "Batch 2 , Loss 10.432369232177734\n",
      "Batch 3 , Loss 10.440464973449707\n",
      "Batch 4 , Loss 10.44710922241211\n",
      "Batch 5 , Loss 10.42172908782959\n",
      "Batch 6 , Loss 10.403752326965332\n",
      "Batch 7 , Loss 10.403276443481445\n",
      "Batch 8 , Loss 10.402520179748535\n",
      "Batch 9 , Loss 10.38390064239502\n",
      "Batch 10 , Loss 10.389504432678223\n",
      "Batch 11 , Loss 10.371484756469727\n",
      "Batch 12 , Loss 10.35954475402832\n",
      "Batch 13 , Loss 10.334527969360352\n",
      "Batch 14 , Loss 10.296371459960938\n",
      "Batch 15 , Loss 10.288613319396973\n",
      "Batch 16 , Loss 10.262613296508789\n",
      "Batch 17 , Loss 10.270700454711914\n",
      "Batch 18 , Loss 10.23958683013916\n",
      "Batch 19 , Loss 10.22644329071045\n",
      "Batch 20 , Loss 10.155227661132812\n",
      "Batch 21 , Loss 10.130334854125977\n",
      "Batch 22 , Loss 10.13602352142334\n",
      "Batch 23 , Loss 10.086795806884766\n",
      "Batch 24 , Loss 10.055224418640137\n",
      "Batch 25 , Loss 10.080991744995117\n",
      "Batch 26 , Loss 10.030160903930664\n",
      "Batch 27 , Loss 10.015820503234863\n",
      "Batch 28 , Loss 9.952250480651855\n",
      "Batch 29 , Loss 9.957539558410645\n",
      "Batch 30 , Loss 9.900020599365234\n",
      "Batch 31 , Loss 9.87381649017334\n",
      "Batch 32 , Loss 9.852499961853027\n",
      "Batch 33 , Loss 9.880561828613281\n",
      "Batch 34 , Loss 9.842884063720703\n",
      "Batch 35 , Loss 9.791192054748535\n",
      "Batch 36 , Loss 9.731156349182129\n",
      "Batch 37 , Loss 9.732488632202148\n",
      "Batch 38 , Loss 9.739501953125\n",
      "Batch 39 , Loss 9.73200511932373\n",
      "Batch 40 , Loss 9.73046588897705\n",
      "Batch 41 , Loss 9.683298110961914\n",
      "Batch 42 , Loss 9.733453750610352\n",
      "Batch 43 , Loss 9.615460395812988\n",
      "Batch 44 , Loss 9.540735244750977\n",
      "Batch 45 , Loss 9.610883712768555\n",
      "Batch 46 , Loss 9.578664779663086\n",
      "Batch 47 , Loss 9.581791877746582\n",
      "Batch 48 , Loss 9.534543991088867\n",
      "Batch 49 , Loss 9.590190887451172\n",
      "Batch 50 , Loss 9.478261947631836\n",
      "Batch 51 , Loss 9.49506950378418\n",
      "Batch 52 , Loss 9.493148803710938\n",
      "Batch 53 , Loss 9.512039184570312\n",
      "Batch 54 , Loss 9.4688081741333\n",
      "Batch 55 , Loss 9.468803405761719\n",
      "Batch 56 , Loss 9.369690895080566\n",
      "Batch 57 , Loss 9.4540376663208\n",
      "Batch 58 , Loss 9.448589324951172\n",
      "Batch 59 , Loss 9.470434188842773\n",
      "Batch 60 , Loss 9.472222328186035\n",
      "Batch 61 , Loss 9.3192720413208\n",
      "Batch 62 , Loss 9.368614196777344\n",
      "Batch 63 , Loss 9.390331268310547\n",
      "Batch 64 , Loss 9.403180122375488\n",
      "Batch 65 , Loss 9.380684852600098\n",
      "Batch 66 , Loss 9.351487159729004\n",
      "Batch 67 , Loss 9.35434341430664\n",
      "Batch 68 , Loss 9.339731216430664\n",
      "Batch 69 , Loss 9.242679595947266\n",
      "Batch 70 , Loss 9.367780685424805\n",
      "Batch 71 , Loss 9.337642669677734\n",
      "Batch 72 , Loss 9.2892484664917\n",
      "Batch 73 , Loss 9.331966400146484\n",
      "Batch 74 , Loss 9.213672637939453\n",
      "Batch 75 , Loss 9.217262268066406\n",
      "Batch 76 , Loss 9.227591514587402\n",
      "Batch 77 , Loss 9.247519493103027\n",
      "Batch 78 , Loss 9.266852378845215\n",
      "Batch 79 , Loss 9.327733039855957\n",
      "Batch 80 , Loss 9.209246635437012\n",
      "Batch 81 , Loss 9.139837265014648\n",
      "Batch 82 , Loss 9.08721923828125\n",
      "Batch 83 , Loss 9.194947242736816\n",
      "Batch 84 , Loss 9.140084266662598\n",
      "Batch 85 , Loss 9.150083541870117\n",
      "Batch 86 , Loss 9.193443298339844\n",
      "Batch 87 , Loss 9.013749122619629\n",
      "Batch 88 , Loss 9.087539672851562\n",
      "Batch 89 , Loss 9.079370498657227\n",
      "Batch 90 , Loss 9.148262023925781\n",
      "Batch 91 , Loss 9.039263725280762\n",
      "Batch 92 , Loss 9.102381706237793\n",
      "Batch 93 , Loss 8.992427825927734\n",
      "Batch 94 , Loss 9.085010528564453\n",
      "Batch 95 , Loss 9.048211097717285\n",
      "Batch 96 , Loss 8.941728591918945\n",
      "Batch 97 , Loss 9.008962631225586\n",
      "Batch 98 , Loss 9.046636581420898\n",
      "Batch 99 , Loss 9.015050888061523\n",
      "Batch 100 , Loss 8.944924354553223\n",
      "Batch 101 , Loss 9.000638008117676\n",
      "Batch 102 , Loss 8.950299263000488\n",
      "Batch 103 , Loss 8.894501686096191\n",
      "Batch 104 , Loss 8.97495174407959\n",
      "Batch 105 , Loss 8.996428489685059\n",
      "Batch 106 , Loss 8.900769233703613\n",
      "Batch 107 , Loss 8.764801979064941\n",
      "Batch 108 , Loss 8.923418045043945\n",
      "Batch 109 , Loss 8.915857315063477\n",
      "Batch 110 , Loss 8.795775413513184\n",
      "Batch 111 , Loss 8.91142463684082\n",
      "Batch 112 , Loss 8.882491111755371\n",
      "Batch 113 , Loss 8.79422664642334\n",
      "Batch 114 , Loss 8.864930152893066\n",
      "Batch 115 , Loss 8.801644325256348\n",
      "Batch 116 , Loss 8.747897148132324\n",
      "Batch 117 , Loss 8.729290008544922\n",
      "Batch 118 , Loss 8.747098922729492\n",
      "Batch 119 , Loss 8.841079711914062\n",
      "Batch 120 , Loss 8.768600463867188\n",
      "Batch 121 , Loss 8.716482162475586\n",
      "Batch 122 , Loss 8.709946632385254\n",
      "Batch 123 , Loss 8.746843338012695\n",
      "Batch 124 , Loss 8.710992813110352\n",
      "Batch 125 , Loss 8.587642669677734\n",
      "Batch 126 , Loss 8.615118026733398\n",
      "Batch 127 , Loss 8.598203659057617\n",
      "Batch 128 , Loss 8.690671920776367\n",
      "Batch 129 , Loss 8.637249946594238\n",
      "Batch 130 , Loss 8.595183372497559\n",
      "Batch 131 , Loss 8.646148681640625\n",
      "Batch 132 , Loss 8.54706859588623\n",
      "Batch 133 , Loss 8.537259101867676\n",
      "Batch 134 , Loss 8.58301067352295\n",
      "Batch 135 , Loss 8.574989318847656\n",
      "Batch 136 , Loss 8.508533477783203\n",
      "Batch 137 , Loss 8.566883087158203\n",
      "Batch 138 , Loss 8.510457992553711\n",
      "Batch 139 , Loss 8.42409610748291\n",
      "Batch 140 , Loss 8.550803184509277\n",
      "Batch 141 , Loss 8.36413288116455\n",
      "Batch 142 , Loss 8.376657485961914\n",
      "Batch 143 , Loss 8.426667213439941\n",
      "Batch 144 , Loss 8.543505668640137\n",
      "Batch 145 , Loss 8.325350761413574\n",
      "Batch 146 , Loss 8.29665756225586\n",
      "Batch 147 , Loss 8.335965156555176\n",
      "Batch 148 , Loss 8.346759796142578\n",
      "Batch 149 , Loss 8.243413925170898\n",
      "Batch 150 , Loss 8.412240982055664\n",
      "Batch 151 , Loss 8.316967010498047\n",
      "Batch 152 , Loss 8.238351821899414\n",
      "Batch 153 , Loss 8.34691333770752\n",
      "Batch 154 , Loss 8.4089937210083\n",
      "Batch 155 , Loss 8.245681762695312\n",
      "Batch 156 , Loss 8.178665161132812\n",
      "Batch 157 , Loss 8.207399368286133\n",
      "Batch 158 , Loss 8.200809478759766\n",
      "Batch 159 , Loss 8.132499694824219\n",
      "Batch 160 , Loss 8.243560791015625\n",
      "Batch 161 , Loss 8.047133445739746\n",
      "Batch 162 , Loss 8.15367603302002\n",
      "Batch 163 , Loss 8.085773468017578\n",
      "Batch 164 , Loss 8.011580467224121\n",
      "Batch 165 , Loss 7.973369121551514\n",
      "Batch 166 , Loss 7.910253524780273\n",
      "Batch 167 , Loss 8.114179611206055\n",
      "Batch 168 , Loss 7.983129501342773\n",
      "Batch 169 , Loss 8.078018188476562\n",
      "Batch 170 , Loss 7.899970054626465\n",
      "Batch 171 , Loss 7.988620281219482\n",
      "Batch 172 , Loss 7.944435119628906\n",
      "Batch 173 , Loss 8.042115211486816\n",
      "Batch 174 , Loss 7.978940963745117\n",
      "Batch 175 , Loss 7.826703071594238\n",
      "Batch 176 , Loss 7.942679405212402\n",
      "Batch 177 , Loss 7.782100200653076\n",
      "Batch 178 , Loss 7.764007568359375\n",
      "Batch 179 , Loss 7.7923479080200195\n",
      "Batch 180 , Loss 7.888835906982422\n",
      "Batch 181 , Loss 7.7519707679748535\n",
      "Batch 182 , Loss 7.693109035491943\n",
      "Batch 183 , Loss 7.934530735015869\n",
      "Batch 184 , Loss 7.740288734436035\n",
      "Batch 185 , Loss 7.778765678405762\n",
      "Batch 186 , Loss 7.767931938171387\n",
      "Batch 187 , Loss 7.669827938079834\n",
      "Batch 188 , Loss 7.6718034744262695\n",
      "Batch 189 , Loss 7.677280426025391\n",
      "Batch 190 , Loss 7.629785060882568\n",
      "Batch 191 , Loss 7.6808342933654785\n",
      "Batch 192 , Loss 7.47037935256958\n",
      "Batch 193 , Loss 7.709620475769043\n",
      "Batch 194 , Loss 7.585529327392578\n",
      "Batch 195 , Loss 7.516400337219238\n",
      "Batch 196 , Loss 7.432501792907715\n",
      "Batch 197 , Loss 7.446183204650879\n",
      "Batch 198 , Loss 7.550185680389404\n",
      "Batch 199 , Loss 7.358780384063721\n",
      "Batch 200 , Loss 7.42396354675293\n",
      "Batch 201 , Loss 7.533801078796387\n",
      "Batch 202 , Loss 7.426438808441162\n",
      "Batch 203 , Loss 7.314582824707031\n",
      "Batch 204 , Loss 7.347284317016602\n",
      "Batch 205 , Loss 7.465053081512451\n",
      "Batch 206 , Loss 7.349875450134277\n",
      "Batch 207 , Loss 7.338294506072998\n",
      "Batch 208 , Loss 7.380422115325928\n",
      "Batch 209 , Loss 7.271820068359375\n",
      "Batch 210 , Loss 7.253018379211426\n",
      "Batch 211 , Loss 7.253756999969482\n",
      "Batch 212 , Loss 7.285438537597656\n",
      "Batch 213 , Loss 7.227209568023682\n",
      "Batch 214 , Loss 7.29454231262207\n",
      "Batch 215 , Loss 7.098530292510986\n",
      "Batch 216 , Loss 7.190287113189697\n",
      "Batch 217 , Loss 7.102825164794922\n",
      "Batch 218 , Loss 7.186765193939209\n",
      "Batch 219 , Loss 7.075984954833984\n",
      "Batch 220 , Loss 7.1900739669799805\n",
      "Batch 221 , Loss 6.942713260650635\n",
      "Batch 222 , Loss 7.145007610321045\n",
      "Batch 223 , Loss 7.011757850646973\n",
      "Batch 224 , Loss 7.126521110534668\n",
      "Batch 225 , Loss 7.1643571853637695\n",
      "Batch 226 , Loss 6.986425876617432\n",
      "Batch 227 , Loss 7.080560207366943\n",
      "Batch 228 , Loss 7.061849594116211\n",
      "Batch 229 , Loss 6.9480390548706055\n",
      "Batch 230 , Loss 6.885617256164551\n",
      "Batch 231 , Loss 6.978748798370361\n",
      "Batch 232 , Loss 6.948437690734863\n",
      "Batch 233 , Loss 6.989058494567871\n",
      "Batch 234 , Loss 7.090635776519775\n",
      "Batch 235 , Loss 6.845235824584961\n",
      "Batch 236 , Loss 6.968648433685303\n",
      "Batch 237 , Loss 6.8978447914123535\n",
      "Batch 238 , Loss 7.047357082366943\n",
      "Batch 239 , Loss 6.917728424072266\n",
      "Batch 240 , Loss 6.784430503845215\n",
      "Batch 241 , Loss 6.773991584777832\n",
      "Batch 242 , Loss 6.860043048858643\n",
      "Batch 243 , Loss 6.853235721588135\n",
      "Batch 244 , Loss 6.826228618621826\n",
      "Batch 245 , Loss 6.913147926330566\n",
      "Batch 246 , Loss 6.813004970550537\n",
      "Batch 247 , Loss 6.939777374267578\n",
      "Batch 248 , Loss 6.544108867645264\n",
      "Batch 249 , Loss 6.826050758361816\n",
      "Batch 250 , Loss 6.688911437988281\n",
      "Batch 251 , Loss 6.687363624572754\n",
      "Batch 252 , Loss 6.540584564208984\n",
      "Batch 253 , Loss 6.750463008880615\n",
      "Batch 254 , Loss 6.731877326965332\n",
      "Batch 255 , Loss 6.721652984619141\n",
      "Batch 256 , Loss 6.716289520263672\n",
      "Batch 257 , Loss 6.556643009185791\n",
      "Batch 258 , Loss 6.69816780090332\n",
      "Batch 259 , Loss 6.6138505935668945\n",
      "Batch 260 , Loss 6.531673431396484\n",
      "Batch 261 , Loss 6.570641994476318\n",
      "Batch 262 , Loss 6.497819423675537\n",
      "Batch 263 , Loss 6.594045639038086\n",
      "Batch 264 , Loss 6.554105281829834\n",
      "Batch 265 , Loss 6.564789772033691\n",
      "Batch 266 , Loss 6.655434608459473\n",
      "Batch 267 , Loss 6.437811851501465\n",
      "Batch 268 , Loss 6.530769348144531\n",
      "Batch 269 , Loss 6.5787458419799805\n",
      "Batch 270 , Loss 6.498972415924072\n",
      "Batch 271 , Loss 6.6341447830200195\n",
      "Batch 272 , Loss 6.499111652374268\n",
      "Batch 273 , Loss 6.457925796508789\n",
      "Batch 274 , Loss 6.314615726470947\n",
      "Batch 275 , Loss 6.43257999420166\n",
      "Batch 276 , Loss 6.457742691040039\n",
      "Batch 277 , Loss 6.45175838470459\n",
      "Batch 278 , Loss 6.532392501831055\n",
      "Batch 279 , Loss 6.239579200744629\n",
      "Batch 280 , Loss 6.4388933181762695\n",
      "Batch 281 , Loss 6.290347576141357\n",
      "Batch 282 , Loss 6.3694868087768555\n",
      "Batch 283 , Loss 6.408848762512207\n",
      "Batch 284 , Loss 6.373141288757324\n",
      "Batch 285 , Loss 6.327569007873535\n",
      "Batch 286 , Loss 6.439567565917969\n",
      "Batch 287 , Loss 6.316989421844482\n",
      "Batch 288 , Loss 6.38510799407959\n",
      "Batch 289 , Loss 6.486457347869873\n",
      "Batch 290 , Loss 6.505463600158691\n",
      "Batch 291 , Loss 6.355664253234863\n",
      "Batch 292 , Loss 6.37476921081543\n",
      "Batch 293 , Loss 6.332474708557129\n",
      "Batch 294 , Loss 6.282761573791504\n",
      "Batch 295 , Loss 6.3673553466796875\n",
      "Batch 296 , Loss 6.288167953491211\n",
      "Batch 297 , Loss 6.5652055740356445\n",
      "Batch 298 , Loss 6.262767314910889\n",
      "Batch 299 , Loss 6.240206241607666\n",
      "Batch 300 , Loss 6.208954811096191\n",
      "Batch 301 , Loss 6.429201126098633\n",
      "Batch 302 , Loss 6.196444034576416\n",
      "Batch 303 , Loss 6.34519624710083\n",
      "Batch 304 , Loss 6.274085521697998\n",
      "Batch 305 , Loss 6.059998512268066\n",
      "Batch 306 , Loss 6.4257988929748535\n",
      "Batch 307 , Loss 6.200691223144531\n",
      "Batch 308 , Loss 6.2583184242248535\n",
      "Batch 309 , Loss 6.291128635406494\n",
      "Batch 310 , Loss 6.388759136199951\n",
      "Batch 311 , Loss 6.129122257232666\n",
      "Batch 312 , Loss 6.144030570983887\n",
      "Batch 313 , Loss 5.978818893432617\n",
      "Batch 314 , Loss 6.052707672119141\n",
      "Batch 315 , Loss 6.482989311218262\n",
      "Batch 316 , Loss 6.323611736297607\n",
      "Batch 317 , Loss 5.9995927810668945\n",
      "Batch 318 , Loss 6.131887435913086\n",
      "Batch 319 , Loss 6.3218865394592285\n",
      "Batch 320 , Loss 6.307867050170898\n",
      "Batch 321 , Loss 6.295357704162598\n",
      "Batch 322 , Loss 5.994963645935059\n",
      "Batch 323 , Loss 6.199129581451416\n",
      "Batch 324 , Loss 6.232881546020508\n",
      "Batch 325 , Loss 6.231466770172119\n",
      "Batch 326 , Loss 6.061387538909912\n",
      "Batch 327 , Loss 5.983710765838623\n",
      "Batch 328 , Loss 6.126557350158691\n",
      "Batch 329 , Loss 6.089330196380615\n",
      "Batch 330 , Loss 6.283356189727783\n",
      "Batch 331 , Loss 6.192137718200684\n",
      "Batch 332 , Loss 5.949806213378906\n",
      "Batch 333 , Loss 6.096789360046387\n",
      "Batch 334 , Loss 6.069770812988281\n",
      "Batch 335 , Loss 6.1160054206848145\n",
      "Batch 336 , Loss 6.020337104797363\n",
      "Batch 337 , Loss 6.2324419021606445\n",
      "Batch 338 , Loss 5.972854137420654\n",
      "Batch 339 , Loss 5.92213249206543\n",
      "Batch 340 , Loss 6.211263656616211\n",
      "Batch 341 , Loss 6.23917818069458\n",
      "Batch 342 , Loss 5.906353950500488\n",
      "Batch 343 , Loss 6.091872215270996\n",
      "Batch 344 , Loss 6.050061225891113\n",
      "Batch 345 , Loss 5.895503044128418\n",
      "Batch 346 , Loss 6.266341686248779\n",
      "Batch 347 , Loss 6.139631271362305\n",
      "Batch 348 , Loss 5.914024829864502\n",
      "Batch 349 , Loss 6.071317672729492\n",
      "Batch 350 , Loss 5.949275493621826\n",
      "Batch 351 , Loss 5.883647918701172\n",
      "Batch 352 , Loss 5.92519998550415\n",
      "Batch 353 , Loss 6.0275163650512695\n",
      "Batch 354 , Loss 5.8843278884887695\n",
      "Batch 355 , Loss 6.237368106842041\n",
      "Batch 356 , Loss 5.983264923095703\n",
      "Batch 357 , Loss 6.002587795257568\n",
      "Batch 358 , Loss 5.786255836486816\n",
      "Batch 359 , Loss 5.874178409576416\n",
      "Batch 360 , Loss 6.103825092315674\n",
      "Batch 361 , Loss 5.902619361877441\n",
      "Batch 362 , Loss 5.956212043762207\n",
      "Batch 363 , Loss 6.389732360839844\n",
      "Batch 364 , Loss 6.347464561462402\n",
      "Batch 365 , Loss 5.7932891845703125\n",
      "Batch 366 , Loss 6.06435489654541\n",
      "Batch 367 , Loss 5.773504257202148\n",
      "Batch 368 , Loss 5.847935199737549\n",
      "Batch 369 , Loss 6.275839805603027\n",
      "Batch 370 , Loss 6.0683064460754395\n",
      "Batch 371 , Loss 5.789963245391846\n",
      "Batch 372 , Loss 5.759601593017578\n",
      "Batch 373 , Loss 5.880043983459473\n",
      "Batch 374 , Loss 6.009124279022217\n",
      "Batch 375 , Loss 5.908205032348633\n",
      "Batch 376 , Loss 5.9741668701171875\n",
      "Batch 377 , Loss 5.584471702575684\n",
      "Batch 378 , Loss 6.03913688659668\n",
      "Batch 379 , Loss 5.947108268737793\n",
      "Batch 380 , Loss 5.96284294128418\n",
      "Batch 381 , Loss 5.950628280639648\n",
      "Batch 382 , Loss 5.809036731719971\n",
      "Batch 383 , Loss 5.9332475662231445\n",
      "Batch 384 , Loss 5.821703910827637\n",
      "Batch 385 , Loss 5.952619552612305\n",
      "Batch 386 , Loss 6.009516716003418\n",
      "Batch 387 , Loss 5.967589378356934\n",
      "Batch 388 , Loss 6.188393592834473\n",
      "Batch 389 , Loss 5.733827114105225\n",
      "Batch 390 , Loss 5.711812496185303\n",
      "Batch 391 , Loss 5.846658229827881\n",
      "Batch 392 , Loss 5.845328330993652\n",
      "Batch 393 , Loss 5.873349189758301\n",
      "Batch 394 , Loss 5.75117301940918\n",
      "Batch 395 , Loss 5.674084663391113\n",
      "Batch 396 , Loss 5.889181613922119\n",
      "Batch 397 , Loss 5.852553367614746\n",
      "Batch 398 , Loss 5.744071960449219\n",
      "Batch 399 , Loss 5.783362865447998\n",
      "Batch 400 , Loss 5.894745826721191\n",
      "Batch 401 , Loss 6.112026214599609\n",
      "Batch 402 , Loss 5.7733473777771\n",
      "Batch 403 , Loss 6.017555236816406\n",
      "Batch 404 , Loss 5.71787691116333\n",
      "Batch 405 , Loss 5.781065940856934\n",
      "Batch 406 , Loss 5.85459566116333\n",
      "Batch 407 , Loss 5.962052345275879\n",
      "Batch 408 , Loss 5.658588886260986\n",
      "Batch 409 , Loss 6.006233215332031\n",
      "Batch 410 , Loss 5.753774642944336\n",
      "Batch 411 , Loss 5.581721782684326\n",
      "Batch 412 , Loss 5.615846633911133\n",
      "Batch 413 , Loss 5.67973518371582\n",
      "Batch 414 , Loss 5.624845027923584\n",
      "Batch 415 , Loss 5.6630659103393555\n",
      "Batch 416 , Loss 5.670520305633545\n",
      "Batch 417 , Loss 5.9439802169799805\n",
      "Batch 418 , Loss 5.872530937194824\n",
      "Batch 419 , Loss 5.885926246643066\n",
      "Batch 420 , Loss 5.831964015960693\n",
      "Batch 421 , Loss 5.556735038757324\n",
      "Batch 422 , Loss 5.598254203796387\n",
      "Batch 423 , Loss 5.632100582122803\n",
      "Batch 424 , Loss 5.809669017791748\n",
      "Batch 425 , Loss 5.731560230255127\n",
      "Batch 426 , Loss 5.748904228210449\n",
      "Batch 427 , Loss 5.5917649269104\n",
      "Batch 428 , Loss 5.594189643859863\n",
      "Batch 429 , Loss 5.657820224761963\n",
      "Batch 430 , Loss 6.012408256530762\n",
      "Batch 431 , Loss 5.615947723388672\n",
      "Batch 432 , Loss 5.7838454246521\n",
      "Batch 433 , Loss 5.725611209869385\n",
      "Batch 434 , Loss 5.6752471923828125\n",
      "Batch 435 , Loss 5.692352771759033\n",
      "Batch 436 , Loss 5.853321075439453\n",
      "Batch 437 , Loss 5.665033340454102\n",
      "Batch 438 , Loss 5.723313331604004\n",
      "Batch 439 , Loss 5.585373401641846\n",
      "Batch 440 , Loss 5.653504371643066\n",
      "Batch 441 , Loss 5.474681377410889\n",
      "Batch 442 , Loss 5.641424179077148\n",
      "Batch 443 , Loss 5.750044822692871\n",
      "Batch 444 , Loss 5.623477935791016\n",
      "Batch 445 , Loss 5.936551570892334\n",
      "Batch 446 , Loss 5.6422624588012695\n",
      "Batch 447 , Loss 5.77729606628418\n",
      "Batch 448 , Loss 5.696232795715332\n",
      "Batch 449 , Loss 5.487433910369873\n",
      "Batch 450 , Loss 5.6719970703125\n",
      "Batch 451 , Loss 5.566474914550781\n",
      "Batch 452 , Loss 5.940019607543945\n",
      "Batch 453 , Loss 5.68458366394043\n",
      "Batch 454 , Loss 5.69148063659668\n",
      "Batch 455 , Loss 5.814057350158691\n",
      "Batch 456 , Loss 5.7856950759887695\n",
      "Batch 457 , Loss 5.601120948791504\n",
      "Batch 458 , Loss 5.618867874145508\n",
      "Batch 459 , Loss 5.676650524139404\n",
      "Batch 460 , Loss 5.600142478942871\n",
      "Batch 461 , Loss 5.6236443519592285\n",
      "Batch 462 , Loss 5.69942569732666\n",
      "Batch 463 , Loss 5.722379207611084\n",
      "Batch 464 , Loss 5.796872615814209\n",
      "Batch 465 , Loss 5.560203552246094\n",
      "Batch 466 , Loss 5.683359146118164\n",
      "Batch 467 , Loss 5.649434566497803\n",
      "Batch 468 , Loss 5.621629238128662\n",
      "Batch 469 , Loss 5.597464561462402\n",
      "Batch 470 , Loss 5.635313034057617\n",
      "Batch 471 , Loss 5.7316670417785645\n",
      "Batch 472 , Loss 5.4521074295043945\n",
      "Batch 473 , Loss 5.400440216064453\n",
      "Batch 474 , Loss 5.394056797027588\n",
      "Batch 475 , Loss 5.475808143615723\n",
      "Batch 476 , Loss 5.710353851318359\n",
      "Batch 477 , Loss 5.53284215927124\n",
      "Batch 478 , Loss 5.3936357498168945\n",
      "Batch 479 , Loss 5.592190742492676\n",
      "Batch 480 , Loss 5.493717670440674\n",
      "Batch 481 , Loss 5.685525894165039\n",
      "Batch 482 , Loss 5.366420745849609\n",
      "Batch 483 , Loss 5.418598651885986\n",
      "Batch 484 , Loss 5.321605682373047\n",
      "Batch 485 , Loss 5.567944526672363\n",
      "Batch 486 , Loss 5.381836891174316\n",
      "Batch 487 , Loss 5.363065719604492\n",
      "Batch 488 , Loss 5.669917106628418\n",
      "Batch 489 , Loss 5.452371120452881\n",
      "Batch 490 , Loss 5.6722869873046875\n",
      "Batch 491 , Loss 5.451202392578125\n",
      "Batch 492 , Loss 5.506847381591797\n",
      "Batch 493 , Loss 5.426816463470459\n",
      "Batch 494 , Loss 5.533534049987793\n",
      "Batch 495 , Loss 5.533268451690674\n",
      "Batch 496 , Loss 5.358006477355957\n",
      "Batch 497 , Loss 5.487608909606934\n",
      "Batch 498 , Loss 5.656692028045654\n",
      "Batch 499 , Loss 5.4730353355407715\n",
      "Batch 500 , Loss 5.38828182220459\n",
      "Batch 501 , Loss 5.413427829742432\n",
      "Batch 502 , Loss 5.405575752258301\n",
      "Batch 503 , Loss 5.5682806968688965\n",
      "Batch 504 , Loss 5.304460525512695\n",
      "Batch 505 , Loss 5.249736785888672\n",
      "Batch 506 , Loss 5.256812572479248\n",
      "Batch 507 , Loss 5.343399524688721\n",
      "Batch 508 , Loss 5.443009376525879\n",
      "Batch 509 , Loss 5.349362850189209\n",
      "Batch 510 , Loss 5.258213043212891\n",
      "Batch 511 , Loss 5.480722427368164\n",
      "Batch 512 , Loss 5.331725597381592\n",
      "Batch 513 , Loss 5.306244373321533\n",
      "Batch 514 , Loss 5.234379768371582\n",
      "Batch 515 , Loss 5.649680137634277\n",
      "Batch 516 , Loss 5.391244888305664\n",
      "Batch 517 , Loss 5.397434234619141\n",
      "Batch 518 , Loss 5.465304374694824\n",
      "Batch 519 , Loss 5.449370861053467\n",
      "Batch 520 , Loss 5.415262222290039\n",
      "Batch 521 , Loss 5.466671943664551\n",
      "Batch 522 , Loss 5.514927387237549\n",
      "Batch 523 , Loss 5.568110942840576\n",
      "Batch 524 , Loss 5.448525905609131\n",
      "Batch 525 , Loss 5.329990386962891\n",
      "Batch 526 , Loss 5.416501045227051\n",
      "Batch 527 , Loss 5.321717739105225\n",
      "Batch 528 , Loss 5.660065174102783\n",
      "Batch 529 , Loss 5.384937763214111\n",
      "Batch 530 , Loss 5.5240678787231445\n",
      "Batch 531 , Loss 5.377853870391846\n",
      "Batch 532 , Loss 5.415289878845215\n",
      "Batch 533 , Loss 5.628076553344727\n",
      "Batch 534 , Loss 5.269834518432617\n",
      "Batch 535 , Loss 5.617516040802002\n",
      "Batch 536 , Loss 5.296174049377441\n",
      "Batch 537 , Loss 5.1886982917785645\n",
      "Batch 538 , Loss 5.404316425323486\n",
      "Batch 539 , Loss 5.377433776855469\n",
      "Batch 540 , Loss 5.395984649658203\n",
      "Batch 541 , Loss 5.361526966094971\n",
      "Batch 542 , Loss 5.446846961975098\n",
      "Batch 543 , Loss 5.466252326965332\n",
      "Batch 544 , Loss 5.3270978927612305\n",
      "Batch 545 , Loss 5.524981498718262\n",
      "Batch 546 , Loss 5.337193012237549\n",
      "Batch 547 , Loss 5.397695541381836\n",
      "Batch 548 , Loss 5.25557804107666\n",
      "Batch 549 , Loss 5.702659606933594\n",
      "Batch 550 , Loss 5.151891231536865\n",
      "Batch 551 , Loss 5.216691970825195\n",
      "Batch 552 , Loss 5.1726908683776855\n",
      "Batch 553 , Loss 5.363884925842285\n",
      "Batch 554 , Loss 5.222284317016602\n",
      "Batch 555 , Loss 5.198300838470459\n",
      "Batch 556 , Loss 5.4225850105285645\n",
      "Batch 557 , Loss 5.240702152252197\n",
      "Batch 558 , Loss 5.619070053100586\n",
      "Batch 559 , Loss 5.289310932159424\n",
      "Batch 560 , Loss 5.056143283843994\n",
      "Batch 561 , Loss 5.220813751220703\n",
      "Batch 562 , Loss 5.41590690612793\n",
      "Batch 563 , Loss 5.233602523803711\n",
      "Batch 564 , Loss 5.372926235198975\n",
      "Batch 565 , Loss 5.60670280456543\n",
      "Batch 566 , Loss 5.053963661193848\n",
      "Batch 567 , Loss 5.487736701965332\n",
      "Batch 568 , Loss 5.242593765258789\n",
      "Batch 569 , Loss 5.245998382568359\n",
      "Batch 570 , Loss 5.161096572875977\n",
      "Batch 571 , Loss 5.204652786254883\n",
      "Batch 572 , Loss 5.205598831176758\n",
      "Batch 573 , Loss 5.620410442352295\n",
      "Batch 574 , Loss 5.31782865524292\n",
      "Batch 575 , Loss 5.279972076416016\n",
      "Batch 576 , Loss 5.084084510803223\n",
      "Batch 577 , Loss 5.224243640899658\n",
      "Batch 578 , Loss 5.209244728088379\n",
      "Batch 579 , Loss 5.253097057342529\n",
      "Batch 580 , Loss 5.371259689331055\n",
      "Batch 581 , Loss 5.232805252075195\n",
      "Batch 582 , Loss 5.343121528625488\n",
      "Batch 583 , Loss 5.092430114746094\n",
      "Batch 584 , Loss 5.2098236083984375\n",
      "Batch 585 , Loss 5.489468574523926\n",
      "Batch 586 , Loss 5.364605903625488\n",
      "Batch 587 , Loss 5.219395637512207\n",
      "Batch 588 , Loss 5.154664993286133\n",
      "Batch 589 , Loss 5.339868545532227\n",
      "Batch 590 , Loss 5.268894672393799\n",
      "Batch 591 , Loss 5.34029483795166\n",
      "Batch 592 , Loss 5.266777992248535\n",
      "Batch 593 , Loss 5.222333908081055\n",
      "Batch 594 , Loss 5.354486465454102\n",
      "Batch 595 , Loss 5.353418350219727\n",
      "Batch 596 , Loss 5.3999834060668945\n",
      "Batch 597 , Loss 5.425271511077881\n",
      "Batch 598 , Loss 5.31962776184082\n",
      "Batch 599 , Loss 5.191564559936523\n",
      "Batch 600 , Loss 5.124849319458008\n",
      "Batch 601 , Loss 5.193426609039307\n",
      "Batch 602 , Loss 5.232466697692871\n",
      "Batch 603 , Loss 5.407488822937012\n",
      "Batch 604 , Loss 5.114401817321777\n",
      "Batch 605 , Loss 5.208641052246094\n",
      "Batch 606 , Loss 5.337704658508301\n",
      "Batch 607 , Loss 5.05974006652832\n",
      "Batch 608 , Loss 5.2570953369140625\n",
      "Batch 609 , Loss 5.1548004150390625\n",
      "Batch 610 , Loss 5.397259712219238\n",
      "Batch 611 , Loss 5.278924942016602\n",
      "Batch 612 , Loss 5.3379011154174805\n",
      "Batch 613 , Loss 5.21852445602417\n",
      "Batch 614 , Loss 5.361579418182373\n",
      "Batch 615 , Loss 5.199719429016113\n",
      "Batch 616 , Loss 5.126035213470459\n",
      "Batch 617 , Loss 5.345279216766357\n",
      "Batch 618 , Loss 5.254402160644531\n",
      "Batch 619 , Loss 5.045628070831299\n",
      "Batch 620 , Loss 5.277575492858887\n",
      "Batch 621 , Loss 5.292778491973877\n",
      "Batch 622 , Loss 5.24897575378418\n",
      "Batch 623 , Loss 5.138791084289551\n",
      "Batch 624 , Loss 5.173354625701904\n",
      "Batch 625 , Loss 5.261965274810791\n",
      "Batch 626 , Loss 5.126832008361816\n",
      "Batch 627 , Loss 5.382861137390137\n",
      "Batch 628 , Loss 5.042088985443115\n",
      "Batch 629 , Loss 5.327607154846191\n",
      "Batch 630 , Loss 4.884961128234863\n",
      "Batch 631 , Loss 5.259449481964111\n",
      "Batch 632 , Loss 5.389219760894775\n",
      "Batch 633 , Loss 5.119473457336426\n",
      "Batch 634 , Loss 5.1899495124816895\n",
      "Batch 635 , Loss 5.130508899688721\n",
      "Batch 636 , Loss 5.316909313201904\n",
      "Batch 637 , Loss 4.985933780670166\n",
      "Batch 638 , Loss 5.341176986694336\n",
      "Batch 639 , Loss 5.2381391525268555\n",
      "Batch 640 , Loss 4.925089359283447\n",
      "Batch 641 , Loss 5.125671863555908\n",
      "Batch 642 , Loss 5.319021701812744\n",
      "Batch 643 , Loss 5.13750696182251\n",
      "Batch 644 , Loss 5.025828838348389\n",
      "Batch 645 , Loss 5.179654121398926\n",
      "Batch 646 , Loss 4.959566116333008\n",
      "Batch 647 , Loss 4.913209438323975\n",
      "Batch 648 , Loss 5.012313365936279\n",
      "Batch 649 , Loss 4.9862823486328125\n",
      "Batch 650 , Loss 4.779129505157471\n",
      "Batch 651 , Loss 5.209120750427246\n",
      "Batch 652 , Loss 5.048178672790527\n",
      "Batch 653 , Loss 5.087123870849609\n",
      "Batch 654 , Loss 5.256430625915527\n",
      "Batch 655 , Loss 5.1945295333862305\n",
      "Batch 656 , Loss 5.138375759124756\n",
      "Batch 657 , Loss 5.102763652801514\n",
      "Batch 658 , Loss 5.149850368499756\n",
      "Batch 659 , Loss 4.993619918823242\n",
      "Batch 660 , Loss 5.181424617767334\n",
      "Batch 661 , Loss 5.08229923248291\n",
      "Batch 662 , Loss 5.49199914932251\n",
      "Batch 663 , Loss 5.066061019897461\n",
      "Batch 664 , Loss 4.938704490661621\n",
      "Batch 665 , Loss 5.1177778244018555\n",
      "Batch 666 , Loss 5.297696113586426\n",
      "Batch 667 , Loss 4.995582580566406\n",
      "Batch 668 , Loss 5.011819362640381\n",
      "Batch 669 , Loss 5.199373722076416\n",
      "Batch 670 , Loss 4.786026477813721\n",
      "Batch 671 , Loss 4.903666019439697\n",
      "Batch 672 , Loss 4.736807346343994\n",
      "Batch 673 , Loss 5.07761812210083\n",
      "Batch 674 , Loss 5.226031303405762\n",
      "Batch 675 , Loss 5.213234901428223\n",
      "Batch 676 , Loss 5.148711681365967\n",
      "Batch 677 , Loss 4.881768226623535\n",
      "Batch 678 , Loss 5.089206695556641\n",
      "Batch 679 , Loss 4.986869812011719\n",
      "Batch 680 , Loss 5.120196342468262\n",
      "Batch 681 , Loss 5.081775665283203\n",
      "Batch 682 , Loss 5.0281081199646\n",
      "Batch 683 , Loss 4.97837495803833\n",
      "Batch 684 , Loss 5.193883895874023\n",
      "Batch 685 , Loss 5.46041202545166\n",
      "Batch 686 , Loss 5.2645063400268555\n",
      "Batch 687 , Loss 5.206413745880127\n",
      "Batch 688 , Loss 4.865832328796387\n",
      "Batch 689 , Loss 4.85902214050293\n",
      "Batch 690 , Loss 5.017575263977051\n",
      "Batch 691 , Loss 5.134004592895508\n",
      "Batch 692 , Loss 5.284736633300781\n",
      "Batch 693 , Loss 5.16556978225708\n",
      "Batch 694 , Loss 5.111224174499512\n",
      "Batch 695 , Loss 4.894815444946289\n",
      "Batch 696 , Loss 5.038673400878906\n",
      "Batch 697 , Loss 4.951209545135498\n",
      "Batch 698 , Loss 4.958490371704102\n",
      "Batch 699 , Loss 4.76400089263916\n",
      "Batch 700 , Loss 4.835710525512695\n",
      "Batch 701 , Loss 5.080146789550781\n",
      "Batch 702 , Loss 5.304690837860107\n",
      "Batch 703 , Loss 4.963080883026123\n",
      "Batch 704 , Loss 4.954206466674805\n",
      "Batch 705 , Loss 5.168684482574463\n",
      "Batch 706 , Loss 5.289170742034912\n",
      "Batch 707 , Loss 4.90476655960083\n",
      "Batch 708 , Loss 5.043591499328613\n",
      "Batch 709 , Loss 5.100499153137207\n",
      "Batch 710 , Loss 4.86275577545166\n",
      "Batch 711 , Loss 4.9341654777526855\n",
      "Batch 712 , Loss 5.069855690002441\n",
      "Batch 713 , Loss 5.1234354972839355\n",
      "Batch 714 , Loss 5.027809143066406\n",
      "Batch 715 , Loss 5.14535665512085\n",
      "Batch 716 , Loss 5.069478988647461\n",
      "Batch 717 , Loss 5.039632320404053\n",
      "Batch 718 , Loss 5.027515888214111\n",
      "Batch 719 , Loss 5.290767669677734\n",
      "Batch 720 , Loss 4.603123664855957\n",
      "Batch 721 , Loss 5.061447620391846\n",
      "Batch 722 , Loss 4.918178558349609\n",
      "Batch 723 , Loss 5.157236576080322\n",
      "Batch 724 , Loss 5.120316028594971\n",
      "Batch 725 , Loss 5.0224456787109375\n",
      "Batch 726 , Loss 4.915567398071289\n",
      "Batch 727 , Loss 5.033194065093994\n",
      "Batch 728 , Loss 5.019826889038086\n",
      "Batch 729 , Loss 5.023515224456787\n",
      "Batch 730 , Loss 4.8880205154418945\n",
      "Batch 731 , Loss 4.94751501083374\n",
      "Batch 732 , Loss 5.062840938568115\n",
      "Batch 733 , Loss 5.079109191894531\n",
      "Batch 734 , Loss 5.091972827911377\n",
      "Batch 735 , Loss 5.043728351593018\n",
      "Batch 736 , Loss 5.315192222595215\n",
      "Batch 737 , Loss 4.786542892456055\n",
      "Batch 738 , Loss 4.9620680809021\n",
      "Batch 739 , Loss 4.724902153015137\n",
      "Batch 740 , Loss 4.884803295135498\n",
      "Batch 741 , Loss 4.86699104309082\n",
      "Batch 742 , Loss 5.174681663513184\n",
      "Batch 743 , Loss 4.824960231781006\n",
      "Batch 744 , Loss 4.812865257263184\n",
      "Batch 745 , Loss 4.951098442077637\n",
      "Batch 746 , Loss 4.6547064781188965\n",
      "Batch 747 , Loss 4.888946056365967\n",
      "Batch 748 , Loss 5.053131580352783\n",
      "Batch 749 , Loss 4.945191383361816\n",
      "Batch 750 , Loss 4.940710544586182\n",
      "Batch 751 , Loss 4.836826324462891\n",
      "Batch 752 , Loss 5.066498756408691\n",
      "Batch 753 , Loss 4.86891508102417\n",
      "Batch 754 , Loss 5.089067459106445\n",
      "Batch 755 , Loss 5.1518683433532715\n",
      "Batch 756 , Loss 5.033124923706055\n",
      "Batch 757 , Loss 5.00471830368042\n",
      "Batch 758 , Loss 5.289405822753906\n",
      "Batch 759 , Loss 5.058040618896484\n",
      "Batch 760 , Loss 5.098259925842285\n",
      "Batch 761 , Loss 4.874941825866699\n",
      "Batch 762 , Loss 5.2146453857421875\n",
      "Batch 763 , Loss 4.985482215881348\n",
      "Batch 764 , Loss 4.767681121826172\n",
      "Batch 765 , Loss 4.7246880531311035\n",
      "Batch 766 , Loss 5.221772193908691\n",
      "Batch 767 , Loss 4.950107574462891\n",
      "Batch 768 , Loss 5.104004859924316\n",
      "Batch 769 , Loss 5.0360260009765625\n",
      "Batch 770 , Loss 4.775520324707031\n",
      "Batch 771 , Loss 4.90252685546875\n",
      "Batch 772 , Loss 4.950179100036621\n",
      "Batch 773 , Loss 4.987885475158691\n",
      "Batch 774 , Loss 4.873898983001709\n",
      "Batch 775 , Loss 4.9098005294799805\n",
      "Batch 776 , Loss 5.105166435241699\n",
      "Batch 777 , Loss 4.8981781005859375\n",
      "Batch 778 , Loss 4.9798383712768555\n",
      "Batch 779 , Loss 4.609164237976074\n",
      "Batch 780 , Loss 4.881609916687012\n",
      "Batch 781 , Loss 5.072916030883789\n",
      "Batch 782 , Loss 4.823426246643066\n",
      "Batch 783 , Loss 5.167543411254883\n",
      "Batch 784 , Loss 4.88827657699585\n",
      "Batch 785 , Loss 4.789546966552734\n",
      "Batch 786 , Loss 4.677180290222168\n",
      "Batch 787 , Loss 5.16427755355835\n",
      "Batch 788 , Loss 5.164095878601074\n",
      "Batch 789 , Loss 4.808256149291992\n",
      "Batch 790 , Loss 4.723270416259766\n",
      "Batch 791 , Loss 5.081254005432129\n",
      "Batch 792 , Loss 5.178388595581055\n",
      "Batch 793 , Loss 4.840367317199707\n",
      "Batch 794 , Loss 5.108762264251709\n",
      "Batch 795 , Loss 4.9775848388671875\n",
      "Batch 796 , Loss 5.013472080230713\n",
      "Batch 797 , Loss 4.756616115570068\n",
      "Batch 798 , Loss 4.928232192993164\n",
      "Batch 799 , Loss 4.7281174659729\n",
      "Batch 800 , Loss 4.783029079437256\n",
      "Batch 801 , Loss 4.7661590576171875\n",
      "Batch 802 , Loss 4.650373935699463\n",
      "Batch 803 , Loss 4.954753875732422\n",
      "Batch 804 , Loss 4.676786422729492\n",
      "Batch 805 , Loss 4.957179069519043\n",
      "Batch 806 , Loss 4.8893961906433105\n",
      "Batch 807 , Loss 4.826735019683838\n",
      "Batch 808 , Loss 4.947305679321289\n",
      "Batch 809 , Loss 4.9079203605651855\n",
      "Batch 810 , Loss 4.874453067779541\n",
      "Batch 811 , Loss 4.881284236907959\n",
      "Batch 812 , Loss 4.588587760925293\n",
      "Batch 813 , Loss 4.921311855316162\n",
      "Batch 814 , Loss 5.190043926239014\n",
      "Batch 815 , Loss 4.856649398803711\n",
      "Batch 816 , Loss 4.899210453033447\n",
      "Batch 817 , Loss 5.155128479003906\n",
      "Batch 818 , Loss 4.993398666381836\n",
      "Batch 819 , Loss 4.708826541900635\n",
      "Batch 820 , Loss 4.737563610076904\n",
      "Batch 821 , Loss 4.834071636199951\n",
      "Batch 822 , Loss 4.939177989959717\n",
      "Batch 823 , Loss 5.334125518798828\n",
      "Batch 824 , Loss 4.645191192626953\n",
      "Batch 825 , Loss 4.7767744064331055\n",
      "Batch 826 , Loss 4.959212303161621\n",
      "Batch 827 , Loss 4.978682518005371\n",
      "Batch 828 , Loss 4.916787147521973\n",
      "Batch 829 , Loss 4.727325439453125\n",
      "Batch 830 , Loss 4.888828754425049\n",
      "Batch 831 , Loss 4.96195125579834\n",
      "Batch 832 , Loss 4.946996688842773\n",
      "Batch 833 , Loss 4.617314338684082\n",
      "Batch 834 , Loss 4.995090484619141\n",
      "Batch 835 , Loss 4.791611671447754\n",
      "Batch 836 , Loss 4.949609279632568\n",
      "Batch 837 , Loss 4.918421745300293\n",
      "Batch 838 , Loss 4.925795555114746\n",
      "Batch 839 , Loss 4.670795440673828\n",
      "Batch 840 , Loss 4.90750789642334\n",
      "Batch 841 , Loss 4.718195915222168\n",
      "Batch 842 , Loss 4.711667060852051\n",
      "Batch 843 , Loss 4.95695161819458\n",
      "Batch 844 , Loss 4.940781116485596\n",
      "Batch 845 , Loss 4.84765100479126\n",
      "Batch 846 , Loss 4.895496845245361\n",
      "Batch 847 , Loss 4.700767517089844\n",
      "Batch 848 , Loss 4.842541217803955\n",
      "Batch 849 , Loss 5.007572174072266\n",
      "Batch 850 , Loss 4.8320631980896\n",
      "Batch 851 , Loss 4.684022426605225\n",
      "Batch 852 , Loss 4.8567681312561035\n",
      "Batch 853 , Loss 4.709436416625977\n",
      "Batch 854 , Loss 4.621264934539795\n",
      "Batch 855 , Loss 4.645163536071777\n",
      "Batch 856 , Loss 4.761416435241699\n",
      "Batch 857 , Loss 4.722784996032715\n",
      "Batch 858 , Loss 4.599193572998047\n",
      "Batch 859 , Loss 4.820732116699219\n",
      "Batch 860 , Loss 4.815609931945801\n",
      "Batch 861 , Loss 4.609283447265625\n",
      "Batch 862 , Loss 5.005048751831055\n",
      "Batch 863 , Loss 5.015997409820557\n",
      "Batch 864 , Loss 4.856564998626709\n",
      "Batch 865 , Loss 4.704270839691162\n",
      "Batch 866 , Loss 4.805649757385254\n",
      "Batch 867 , Loss 4.806025981903076\n",
      "Batch 868 , Loss 4.660297870635986\n",
      "Batch 869 , Loss 4.693264961242676\n",
      "Batch 870 , Loss 4.759593486785889\n",
      "Batch 871 , Loss 5.068273544311523\n",
      "Batch 872 , Loss 4.793646335601807\n",
      "Batch 873 , Loss 4.797726631164551\n",
      "Batch 874 , Loss 5.125802993774414\n",
      "Batch 875 , Loss 5.1612138748168945\n",
      "Batch 876 , Loss 4.852888584136963\n",
      "Batch 877 , Loss 4.522579193115234\n",
      "Batch 878 , Loss 4.696715354919434\n",
      "Batch 879 , Loss 4.708580493927002\n",
      "Batch 880 , Loss 4.68489933013916\n",
      "Batch 881 , Loss 4.725386619567871\n",
      "Batch 882 , Loss 4.815883636474609\n",
      "Batch 883 , Loss 4.827988147735596\n",
      "Batch 884 , Loss 4.848957061767578\n",
      "Batch 885 , Loss 5.083105564117432\n",
      "Batch 886 , Loss 4.785741329193115\n",
      "Batch 887 , Loss 4.468988418579102\n",
      "Batch 888 , Loss 4.832388401031494\n",
      "Batch 889 , Loss 5.088197708129883\n",
      "Batch 890 , Loss 5.006455421447754\n",
      "Batch 891 , Loss 4.506201267242432\n",
      "Batch 892 , Loss 5.132632732391357\n",
      "Batch 893 , Loss 4.536124229431152\n",
      "Batch 894 , Loss 4.741055488586426\n",
      "Batch 895 , Loss 4.66653299331665\n",
      "Batch 896 , Loss 4.750967025756836\n",
      "Batch 897 , Loss 4.94767951965332\n",
      "Batch 898 , Loss 4.746657371520996\n",
      "Batch 899 , Loss 5.038471221923828\n",
      "Batch 900 , Loss 4.776297092437744\n",
      "Batch 901 , Loss 4.642770290374756\n",
      "Batch 902 , Loss 4.637120723724365\n",
      "Batch 903 , Loss 4.670709609985352\n",
      "Batch 904 , Loss 4.864847183227539\n",
      "Batch 905 , Loss 4.580268859863281\n",
      "Batch 906 , Loss 4.669147491455078\n",
      "Batch 907 , Loss 4.8026323318481445\n",
      "Batch 908 , Loss 4.828711032867432\n",
      "Batch 909 , Loss 4.5560479164123535\n",
      "Batch 910 , Loss 4.642526626586914\n",
      "Batch 911 , Loss 4.62678337097168\n",
      "Batch 912 , Loss 4.727977275848389\n",
      "Batch 913 , Loss 4.780180931091309\n",
      "Batch 914 , Loss 4.611847877502441\n",
      "Batch 915 , Loss 4.865522384643555\n",
      "Batch 916 , Loss 5.051910400390625\n",
      "Batch 917 , Loss 4.784605026245117\n",
      "Batch 918 , Loss 4.706501483917236\n",
      "Batch 919 , Loss 4.901191234588623\n",
      "Batch 920 , Loss 5.017290115356445\n",
      "Batch 921 , Loss 4.648175239562988\n",
      "Batch 922 , Loss 4.848048686981201\n",
      "Batch 923 , Loss 4.92399787902832\n",
      "Batch 924 , Loss 4.63544225692749\n",
      "Batch 925 , Loss 4.786928653717041\n",
      "Batch 926 , Loss 4.677726745605469\n",
      "Batch 927 , Loss 4.601556301116943\n",
      "Batch 928 , Loss 4.597008228302002\n",
      "Batch 929 , Loss 4.745026588439941\n",
      "Batch 930 , Loss 4.720404148101807\n",
      "Batch 931 , Loss 4.8855366706848145\n",
      "Batch 932 , Loss 4.593586444854736\n",
      "Batch 933 , Loss 4.978547096252441\n",
      "Batch 934 , Loss 4.755926609039307\n",
      "Batch 935 , Loss 4.769865989685059\n",
      "Batch 936 , Loss 4.794684886932373\n",
      "Batch 937 , Loss 4.790214538574219\n",
      "Batch 938 , Loss 4.349490642547607\n",
      "Batch 939 , Loss 4.639249801635742\n",
      "Batch 940 , Loss 4.8403825759887695\n",
      "Batch 941 , Loss 4.736473560333252\n",
      "Batch 942 , Loss 4.776456832885742\n",
      "Batch 943 , Loss 4.696427345275879\n",
      "Batch 944 , Loss 4.742214679718018\n",
      "Batch 945 , Loss 4.5416975021362305\n",
      "Batch 946 , Loss 4.688655376434326\n",
      "Batch 947 , Loss 4.806253910064697\n",
      "Batch 948 , Loss 5.116751670837402\n",
      "Batch 949 , Loss 4.639880180358887\n",
      "Batch 950 , Loss 4.807883262634277\n",
      "Batch 951 , Loss 4.735664367675781\n",
      "Batch 952 , Loss 4.916346549987793\n",
      "Batch 953 , Loss 4.647890567779541\n",
      "Batch 954 , Loss 4.810206890106201\n",
      "Batch 955 , Loss 4.690196514129639\n",
      "Batch 956 , Loss 4.833779811859131\n",
      "Batch 957 , Loss 4.765517234802246\n",
      "Batch 958 , Loss 4.777646064758301\n",
      "Batch 959 , Loss 4.716412544250488\n",
      "Batch 960 , Loss 4.856449127197266\n",
      "Batch 961 , Loss 4.4425368309021\n",
      "Batch 962 , Loss 4.688755035400391\n",
      "Batch 963 , Loss 4.544116973876953\n",
      "Batch 964 , Loss 4.751389503479004\n",
      "Batch 965 , Loss 4.614119052886963\n",
      "Batch 966 , Loss 4.623941898345947\n",
      "Batch 967 , Loss 4.724972724914551\n",
      "Batch 968 , Loss 4.652383804321289\n",
      "Batch 969 , Loss 4.835622787475586\n",
      "Batch 970 , Loss 4.422394275665283\n",
      "Batch 971 , Loss 4.688292026519775\n",
      "Batch 972 , Loss 4.586582183837891\n",
      "Batch 973 , Loss 4.636341094970703\n",
      "Batch 974 , Loss 4.632637977600098\n",
      "Batch 975 , Loss 4.631893634796143\n",
      "Batch 976 , Loss 4.287145614624023\n",
      "Batch 977 , Loss 5.10151481628418\n",
      "Batch 978 , Loss 4.85420036315918\n",
      "Batch 979 , Loss 4.6136250495910645\n",
      "Batch 980 , Loss 4.5943779945373535\n",
      "Batch 981 , Loss 4.61872673034668\n",
      "Batch 982 , Loss 4.631374359130859\n",
      "Batch 983 , Loss 4.528400897979736\n",
      "Batch 984 , Loss 4.70689582824707\n",
      "Batch 985 , Loss 4.785641670227051\n",
      "Batch 986 , Loss 4.624433517456055\n",
      "Batch 987 , Loss 4.700629234313965\n",
      "Batch 988 , Loss 4.687053680419922\n",
      "Batch 989 , Loss 4.483642578125\n",
      "Batch 990 , Loss 4.618342399597168\n",
      "Batch 991 , Loss 4.8225789070129395\n",
      "Batch 992 , Loss 4.811883449554443\n",
      "Batch 993 , Loss 4.736902713775635\n",
      "Batch 994 , Loss 5.016186714172363\n",
      "Batch 995 , Loss 4.904362678527832\n",
      "Batch 996 , Loss 4.427694320678711\n",
      "Batch 997 , Loss 4.59818172454834\n",
      "Batch 998 , Loss 4.532761573791504\n",
      "Batch 999 , Loss 4.43220853805542\n",
      "Batch 1000 , Loss 4.461979866027832\n",
      "Batch 1001 , Loss 4.593100547790527\n",
      "Batch 1002 , Loss 4.732388019561768\n",
      "Batch 1003 , Loss 4.79133415222168\n",
      "Batch 1004 , Loss 4.5531206130981445\n",
      "Batch 1005 , Loss 5.003754615783691\n",
      "Batch 1006 , Loss 4.3253374099731445\n",
      "Batch 1007 , Loss 4.574104309082031\n",
      "Batch 1008 , Loss 4.637569904327393\n",
      "Batch 1009 , Loss 4.76708459854126\n",
      "Batch 1010 , Loss 4.752135276794434\n",
      "Batch 1011 , Loss 4.709296226501465\n",
      "Batch 1012 , Loss 5.058119297027588\n",
      "Batch 1013 , Loss 4.824648857116699\n",
      "Batch 1014 , Loss 4.740813255310059\n",
      "Batch 1015 , Loss 4.63031005859375\n",
      "Batch 1016 , Loss 4.5500640869140625\n",
      "Batch 1017 , Loss 4.635035991668701\n",
      "Batch 1018 , Loss 4.751870632171631\n",
      "Batch 1019 , Loss 4.801584720611572\n",
      "Batch 1020 , Loss 4.56197452545166\n",
      "Batch 1021 , Loss 4.931826114654541\n",
      "Batch 1022 , Loss 4.777287483215332\n",
      "Batch 1023 , Loss 4.592719554901123\n",
      "Batch 1024 , Loss 4.466291427612305\n",
      "Batch 1025 , Loss 4.678600311279297\n",
      "Batch 1026 , Loss 5.060265064239502\n",
      "Batch 1027 , Loss 4.743589401245117\n",
      "Batch 1028 , Loss 4.700450420379639\n",
      "Batch 1029 , Loss 4.653084754943848\n",
      "Batch 1030 , Loss 4.797964096069336\n",
      "Batch 1031 , Loss 4.444158554077148\n",
      "Batch 1032 , Loss 4.636361598968506\n",
      "Batch 1033 , Loss 4.6782307624816895\n",
      "Batch 1034 , Loss 4.651718616485596\n",
      "Batch 1035 , Loss 4.875925064086914\n",
      "Batch 1036 , Loss 4.759121894836426\n",
      "Batch 1037 , Loss 4.455420970916748\n",
      "Batch 1038 , Loss 4.6099982261657715\n",
      "Batch 1039 , Loss 4.720833778381348\n",
      "Batch 1040 , Loss 4.681844711303711\n",
      "Batch 1041 , Loss 4.916693687438965\n",
      "Batch 1042 , Loss 4.719085216522217\n",
      "Batch 1043 , Loss 4.721075057983398\n",
      "Batch 1044 , Loss 4.3581132888793945\n",
      "Batch 1045 , Loss 4.820466995239258\n",
      "Batch 1046 , Loss 4.585063934326172\n",
      "Batch 1047 , Loss 4.568510055541992\n",
      "Batch 1048 , Loss 4.513983726501465\n",
      "Batch 1049 , Loss 4.749195098876953\n",
      "Batch 1050 , Loss 4.407604217529297\n",
      "Batch 1051 , Loss 4.703431606292725\n",
      "Batch 1052 , Loss 4.7588982582092285\n",
      "Batch 1053 , Loss 4.5509819984436035\n",
      "Batch 1054 , Loss 4.740573883056641\n",
      "Batch 1055 , Loss 4.59141731262207\n",
      "Batch 1056 , Loss 4.626784324645996\n",
      "Batch 1057 , Loss 4.617690086364746\n",
      "Batch 1058 , Loss 4.8597002029418945\n",
      "Batch 1059 , Loss 4.775156021118164\n",
      "Batch 1060 , Loss 4.620467662811279\n",
      "Batch 1061 , Loss 4.569318771362305\n",
      "Batch 1062 , Loss 4.584928035736084\n",
      "Batch 1063 , Loss 4.730499267578125\n",
      "Batch 1064 , Loss 4.577933311462402\n",
      "Batch 1065 , Loss 4.8070831298828125\n",
      "Batch 1066 , Loss 4.878631114959717\n",
      "Batch 1067 , Loss 4.506866931915283\n",
      "Batch 1068 , Loss 4.352532386779785\n",
      "Batch 1069 , Loss 4.448299884796143\n",
      "Batch 1070 , Loss 4.6623215675354\n",
      "Batch 1071 , Loss 4.65455436706543\n",
      "Batch 1072 , Loss 4.662301063537598\n",
      "Batch 1073 , Loss 4.98441219329834\n",
      "Batch 1074 , Loss 4.728851318359375\n",
      "Batch 1075 , Loss 4.56550931930542\n",
      "Batch 1076 , Loss 4.321832656860352\n",
      "Batch 1077 , Loss 4.581119537353516\n",
      "Batch 1078 , Loss 4.789853572845459\n",
      "Batch 1079 , Loss 4.459845542907715\n",
      "Batch 1080 , Loss 4.5790910720825195\n",
      "Batch 1081 , Loss 4.428676128387451\n",
      "Batch 1082 , Loss 4.450455665588379\n",
      "Batch 1083 , Loss 4.432501792907715\n",
      "Batch 1084 , Loss 4.4462714195251465\n",
      "Batch 1085 , Loss 4.650691986083984\n",
      "Batch 1086 , Loss 4.623238563537598\n",
      "Batch 1087 , Loss 4.750057697296143\n",
      "Batch 1088 , Loss 4.518180847167969\n",
      "Batch 1089 , Loss 4.465112209320068\n",
      "Batch 1090 , Loss 4.643192291259766\n",
      "Batch 1091 , Loss 4.46201753616333\n",
      "Batch 1092 , Loss 4.624137878417969\n",
      "Batch 1093 , Loss 4.456892967224121\n",
      "Batch 1094 , Loss 4.393383026123047\n",
      "Batch 1095 , Loss 4.733766078948975\n",
      "Batch 1096 , Loss 4.91204833984375\n",
      "Batch 1097 , Loss 4.326405048370361\n",
      "Batch 1098 , Loss 4.547360420227051\n",
      "Batch 1099 , Loss 4.642307758331299\n",
      "Batch 1100 , Loss 4.552454948425293\n",
      "Batch 1101 , Loss 4.33111572265625\n",
      "Batch 1102 , Loss 4.681202411651611\n",
      "Batch 1103 , Loss 4.502317428588867\n",
      "Batch 1104 , Loss 4.658202648162842\n",
      "Batch 1105 , Loss 4.483274936676025\n",
      "Batch 1106 , Loss 4.725008487701416\n",
      "Batch 1107 , Loss 4.522321701049805\n",
      "Batch 1108 , Loss 4.431042671203613\n",
      "Batch 1109 , Loss 4.49931526184082\n",
      "Batch 1110 , Loss 4.796659469604492\n",
      "Batch 1111 , Loss 4.637732028961182\n",
      "Batch 1112 , Loss 4.584231376647949\n",
      "Batch 1113 , Loss 4.674723148345947\n",
      "Batch 1114 , Loss 4.847154140472412\n",
      "Batch 1115 , Loss 4.267420768737793\n",
      "Batch 1116 , Loss 4.6580915451049805\n",
      "Batch 1117 , Loss 4.300472736358643\n",
      "Batch 1118 , Loss 4.677934169769287\n",
      "Batch 1119 , Loss 4.611600399017334\n",
      "Batch 1120 , Loss 4.506251335144043\n",
      "Batch 1121 , Loss 4.626558303833008\n",
      "Batch 1122 , Loss 4.386480808258057\n",
      "Batch 1123 , Loss 4.61017370223999\n",
      "Batch 1124 , Loss 4.443264484405518\n",
      "Batch 1125 , Loss 4.473934173583984\n",
      "Batch 1126 , Loss 4.580691337585449\n",
      "Batch 1127 , Loss 4.659183502197266\n",
      "Batch 1128 , Loss 4.4437575340271\n",
      "Batch 1129 , Loss 4.589356422424316\n",
      "Batch 1130 , Loss 4.628711223602295\n",
      "Batch 1131 , Loss 4.510946273803711\n",
      "Batch 1132 , Loss 4.570404052734375\n",
      "Batch 1133 , Loss 4.446804046630859\n",
      "Batch 1134 , Loss 4.368744373321533\n",
      "Batch 1135 , Loss 4.433365821838379\n",
      "Batch 1136 , Loss 4.453464031219482\n",
      "Batch 1137 , Loss 4.47749137878418\n",
      "Batch 1138 , Loss 4.697471618652344\n",
      "Batch 1139 , Loss 4.3195414543151855\n",
      "Batch 1140 , Loss 4.391985893249512\n",
      "Batch 1141 , Loss 4.348683834075928\n",
      "Batch 1142 , Loss 4.6187520027160645\n",
      "Batch 1143 , Loss 4.340004920959473\n",
      "Batch 1144 , Loss 4.483341693878174\n",
      "Batch 1145 , Loss 4.564052581787109\n",
      "Batch 1146 , Loss 4.423264503479004\n",
      "Batch 1147 , Loss 4.412289142608643\n",
      "Batch 1148 , Loss 4.576014995574951\n",
      "Batch 1149 , Loss 4.608095169067383\n",
      "Batch 1150 , Loss 4.530067443847656\n",
      "Batch 1151 , Loss 4.769018173217773\n",
      "Batch 1152 , Loss 4.586440563201904\n",
      "Batch 1153 , Loss 4.7420806884765625\n",
      "Batch 1154 , Loss 4.529688835144043\n",
      "Batch 1155 , Loss 4.435487747192383\n",
      "Batch 1156 , Loss 4.3073506355285645\n",
      "Batch 1157 , Loss 4.579702377319336\n",
      "Batch 1158 , Loss 4.571133613586426\n",
      "Batch 1159 , Loss 4.525184154510498\n",
      "Batch 1160 , Loss 4.522774696350098\n",
      "Batch 1161 , Loss 4.539811134338379\n",
      "Batch 1162 , Loss 4.328239440917969\n",
      "Batch 1163 , Loss 4.518649101257324\n",
      "Batch 1164 , Loss 4.572234630584717\n",
      "Batch 1165 , Loss 4.33200740814209\n",
      "Batch 1166 , Loss 4.619053840637207\n",
      "Batch 1167 , Loss 4.117136478424072\n",
      "Batch 1168 , Loss 4.448040008544922\n",
      "Batch 1169 , Loss 4.586202144622803\n",
      "Batch 1170 , Loss 4.5822672843933105\n",
      "Batch 1171 , Loss 4.604630470275879\n",
      "Batch 1172 , Loss 4.7988152503967285\n",
      "Batch 1173 , Loss 4.403164386749268\n",
      "Batch 1174 , Loss 4.759577751159668\n",
      "Batch 1175 , Loss 4.357039928436279\n",
      "Batch 1176 , Loss 4.470871448516846\n",
      "Batch 1177 , Loss 4.395132064819336\n",
      "Batch 1178 , Loss 4.633301734924316\n",
      "Batch 1179 , Loss 4.385561943054199\n",
      "Batch 1180 , Loss 4.348636627197266\n",
      "Batch 1181 , Loss 4.4127020835876465\n",
      "Batch 1182 , Loss 4.611481666564941\n",
      "Batch 1183 , Loss 4.522472858428955\n",
      "Batch 1184 , Loss 4.667941570281982\n",
      "Batch 1185 , Loss 4.584568023681641\n",
      "Batch 1186 , Loss 4.319693565368652\n",
      "Batch 1187 , Loss 4.746606826782227\n",
      "Batch 1188 , Loss 4.686400413513184\n",
      "Batch 1189 , Loss 4.401354789733887\n",
      "Batch 1190 , Loss 4.354781150817871\n",
      "Batch 1191 , Loss 4.347897529602051\n",
      "Batch 1192 , Loss 4.494366645812988\n",
      "Batch 1193 , Loss 4.664796829223633\n",
      "Batch 1194 , Loss 4.313992977142334\n",
      "Batch 1195 , Loss 4.425553321838379\n",
      "Batch 1196 , Loss 4.542418956756592\n",
      "Batch 1197 , Loss 4.7146687507629395\n",
      "Batch 1198 , Loss 4.353671073913574\n",
      "Batch 1199 , Loss 4.69964599609375\n",
      "Batch 1200 , Loss 4.495833396911621\n",
      "Batch 1201 , Loss 4.636159420013428\n",
      "Batch 1202 , Loss 4.349992752075195\n",
      "Batch 1203 , Loss 4.720285415649414\n",
      "Batch 1204 , Loss 4.5767107009887695\n",
      "Batch 1205 , Loss 4.4563307762146\n",
      "Batch 1206 , Loss 4.580916404724121\n",
      "Batch 1207 , Loss 4.408935070037842\n",
      "Batch 1208 , Loss 4.659890174865723\n",
      "Batch 1209 , Loss 4.557100772857666\n",
      "Batch 1210 , Loss 4.359395980834961\n",
      "Batch 1211 , Loss 4.731481075286865\n",
      "Batch 1212 , Loss 4.327615737915039\n",
      "Batch 1213 , Loss 4.578627586364746\n",
      "Batch 1214 , Loss 4.507867813110352\n",
      "Batch 1215 , Loss 4.499512672424316\n",
      "Batch 1216 , Loss 4.332308769226074\n",
      "Batch 1217 , Loss 4.452409267425537\n",
      "Batch 1218 , Loss 4.436084270477295\n",
      "Batch 1219 , Loss 4.251712799072266\n",
      "Batch 1220 , Loss 4.4414143562316895\n",
      "Batch 1221 , Loss 4.473089694976807\n",
      "Batch 1222 , Loss 4.5723161697387695\n",
      "Batch 1223 , Loss 4.303958892822266\n",
      "Batch 1224 , Loss 4.433812618255615\n",
      "Batch 1225 , Loss 4.467782974243164\n",
      "Batch 1226 , Loss 4.331805229187012\n",
      "Batch 1227 , Loss 4.43588399887085\n",
      "Batch 1228 , Loss 4.401662349700928\n",
      "Batch 1229 , Loss 4.528375625610352\n",
      "Batch 1230 , Loss 4.455822944641113\n",
      "Batch 1231 , Loss 4.454257488250732\n",
      "Batch 1232 , Loss 4.485532760620117\n",
      "Batch 1233 , Loss 4.620355606079102\n",
      "Batch 1234 , Loss 4.630509376525879\n",
      "Batch 1235 , Loss 4.597288608551025\n",
      "Batch 1236 , Loss 4.398067474365234\n",
      "Batch 1237 , Loss 4.212261199951172\n",
      "Batch 1238 , Loss 4.393604278564453\n",
      "Batch 1239 , Loss 4.829493045806885\n",
      "Batch 1240 , Loss 4.524133682250977\n",
      "Batch 1241 , Loss 4.514578342437744\n",
      "Batch 1242 , Loss 4.431586265563965\n",
      "Batch 1243 , Loss 4.258716106414795\n",
      "Batch 1244 , Loss 4.527945041656494\n",
      "Batch 1245 , Loss 4.4389142990112305\n",
      "Batch 1246 , Loss 4.68839168548584\n",
      "Batch 1247 , Loss 4.283367156982422\n",
      "Batch 1248 , Loss 4.491569995880127\n",
      "Batch 1249 , Loss 4.578153610229492\n",
      "Batch 1250 , Loss 4.3081254959106445\n",
      "Batch 1251 , Loss 4.503840923309326\n",
      "Batch 1252 , Loss 4.781763076782227\n",
      "Batch 1253 , Loss 4.302724838256836\n",
      "Batch 1254 , Loss 4.14449405670166\n",
      "Batch 1255 , Loss 4.585128307342529\n",
      "Batch 1256 , Loss 4.491034507751465\n",
      "Batch 1257 , Loss 4.402591228485107\n",
      "Batch 1258 , Loss 4.388906478881836\n",
      "Batch 1259 , Loss 4.473033428192139\n",
      "Batch 1260 , Loss 4.7715935707092285\n",
      "Batch 1261 , Loss 4.764354705810547\n",
      "Batch 1262 , Loss 4.667807579040527\n",
      "Batch 1263 , Loss 4.4140191078186035\n",
      "Batch 1264 , Loss 4.476940155029297\n",
      "Batch 1265 , Loss 4.349144458770752\n",
      "Batch 1266 , Loss 4.387573719024658\n",
      "Batch 1267 , Loss 4.463191509246826\n",
      "Batch 1268 , Loss 4.490061283111572\n",
      "Batch 1269 , Loss 4.409032821655273\n",
      "Batch 1270 , Loss 4.282320976257324\n",
      "Batch 1271 , Loss 4.405366897583008\n",
      "Batch 1272 , Loss 4.1982598304748535\n",
      "Batch 1273 , Loss 4.44244384765625\n",
      "Batch 1274 , Loss 4.45542049407959\n",
      "Batch 1275 , Loss 4.317061901092529\n",
      "Batch 1276 , Loss 4.786903381347656\n",
      "Batch 1277 , Loss 4.425252437591553\n",
      "Batch 1278 , Loss 4.615031719207764\n",
      "Batch 1279 , Loss 4.431110858917236\n",
      "Batch 1280 , Loss 4.491277694702148\n",
      "Batch 1281 , Loss 4.426424026489258\n",
      "Batch 1282 , Loss 4.538021087646484\n",
      "Batch 1283 , Loss 4.343618392944336\n",
      "Batch 1284 , Loss 4.379415512084961\n",
      "Batch 1285 , Loss 4.679415225982666\n",
      "Batch 1286 , Loss 4.497158050537109\n",
      "Batch 1287 , Loss 5.0409088134765625\n",
      "Batch 1288 , Loss 4.393355846405029\n",
      "Batch 1289 , Loss 4.422771453857422\n",
      "Batch 1290 , Loss 4.411576271057129\n",
      "Batch 1291 , Loss 4.494996070861816\n",
      "Batch 1292 , Loss 4.470593452453613\n",
      "Batch 1293 , Loss 4.6591410636901855\n",
      "Batch 1294 , Loss 4.602522850036621\n",
      "Batch 1295 , Loss 4.379101753234863\n",
      "Batch 1296 , Loss 4.22246789932251\n",
      "Batch 1297 , Loss 4.564470291137695\n",
      "Batch 1298 , Loss 4.515048027038574\n",
      "Batch 1299 , Loss 4.620846271514893\n",
      "Batch 1300 , Loss 4.574406147003174\n",
      "Batch 1301 , Loss 4.406366348266602\n",
      "Batch 1302 , Loss 4.4917497634887695\n",
      "Batch 1303 , Loss 4.255887985229492\n",
      "Batch 1304 , Loss 4.331539630889893\n",
      "Batch 1305 , Loss 4.444013595581055\n",
      "Batch 1306 , Loss 4.333780288696289\n",
      "Batch 1307 , Loss 4.437447547912598\n",
      "Batch 1308 , Loss 4.594780921936035\n",
      "Batch 1309 , Loss 4.498610973358154\n",
      "Batch 1310 , Loss 4.590723991394043\n",
      "Batch 1311 , Loss 4.446009159088135\n",
      "Batch 1312 , Loss 4.568272590637207\n",
      "Batch 1313 , Loss 4.329129219055176\n",
      "Batch 1314 , Loss 4.2148308753967285\n",
      "Batch 1315 , Loss 4.341175079345703\n",
      "Batch 1316 , Loss 4.577950477600098\n",
      "Batch 1317 , Loss 4.430730819702148\n",
      "Batch 1318 , Loss 4.4480695724487305\n",
      "Batch 1319 , Loss 4.653010845184326\n",
      "Batch 1320 , Loss 4.305233478546143\n",
      "Batch 1321 , Loss 4.507279396057129\n",
      "Batch 1322 , Loss 4.313323020935059\n",
      "Batch 1323 , Loss 4.392510890960693\n",
      "Batch 1324 , Loss 4.337994575500488\n",
      "Batch 1325 , Loss 4.581895351409912\n",
      "Batch 1326 , Loss 4.219974517822266\n",
      "Batch 1327 , Loss 4.535458087921143\n",
      "Batch 1328 , Loss 4.421264171600342\n",
      "Batch 1329 , Loss 4.391749858856201\n",
      "Batch 1330 , Loss 4.450113773345947\n",
      "Batch 1331 , Loss 4.44390344619751\n",
      "Batch 1332 , Loss 4.4476447105407715\n",
      "Batch 1333 , Loss 4.399855613708496\n",
      "Batch 1334 , Loss 4.674717903137207\n",
      "Batch 1335 , Loss 4.442084312438965\n",
      "Batch 1336 , Loss 4.214686393737793\n",
      "Batch 1337 , Loss 4.436446666717529\n",
      "Batch 1338 , Loss 4.53452205657959\n",
      "Batch 1339 , Loss 4.299546241760254\n",
      "Batch 1340 , Loss 4.592456340789795\n",
      "Batch 1341 , Loss 4.4752655029296875\n",
      "Batch 1342 , Loss 4.476273536682129\n",
      "Batch 1343 , Loss 4.588167190551758\n",
      "Batch 1344 , Loss 4.546999931335449\n",
      "Batch 1345 , Loss 4.186121463775635\n",
      "Batch 1346 , Loss 4.702996730804443\n",
      "Batch 1347 , Loss 4.334750175476074\n",
      "Batch 1348 , Loss 4.449456214904785\n",
      "Batch 1349 , Loss 4.2500200271606445\n",
      "Batch 1350 , Loss 4.586087703704834\n",
      "Batch 1351 , Loss 4.621923923492432\n",
      "Batch 1352 , Loss 4.265011787414551\n",
      "Batch 1353 , Loss 4.605724334716797\n",
      "Batch 1354 , Loss 4.558023452758789\n",
      "Batch 1355 , Loss 4.339538097381592\n",
      "Batch 1356 , Loss 4.314277648925781\n",
      "Batch 1357 , Loss 4.394752502441406\n",
      "Batch 1358 , Loss 4.42470645904541\n",
      "Batch 1359 , Loss 4.417436122894287\n",
      "Batch 1360 , Loss 4.342748165130615\n",
      "Batch 1361 , Loss 4.325952053070068\n",
      "Batch 1362 , Loss 4.049714088439941\n",
      "Batch 1363 , Loss 4.1318182945251465\n",
      "Batch 1364 , Loss 4.118946075439453\n",
      "Batch 1365 , Loss 4.011424541473389\n",
      "Batch 1366 , Loss 4.291208744049072\n",
      "Batch 1367 , Loss 4.389954566955566\n",
      "Batch 1368 , Loss 4.484285354614258\n",
      "Batch 1369 , Loss 4.506389617919922\n",
      "Batch 1370 , Loss 4.397483825683594\n",
      "Batch 1371 , Loss 4.300190448760986\n",
      "Batch 1372 , Loss 4.307549953460693\n",
      "Batch 1373 , Loss 4.150360584259033\n",
      "Batch 1374 , Loss 4.288313865661621\n",
      "Batch 1375 , Loss 4.61309289932251\n",
      "Batch 1376 , Loss 4.422662734985352\n",
      "Batch 1377 , Loss 4.642311096191406\n",
      "Batch 1378 , Loss 4.313570022583008\n",
      "Batch 1379 , Loss 4.1761322021484375\n",
      "Batch 1380 , Loss 4.590777397155762\n",
      "Batch 1381 , Loss 4.249860763549805\n",
      "Batch 1382 , Loss 4.294466018676758\n",
      "Batch 1383 , Loss 4.138325214385986\n",
      "Batch 1384 , Loss 4.301155090332031\n",
      "Batch 1385 , Loss 4.384496688842773\n",
      "Batch 1386 , Loss 4.284010887145996\n",
      "Batch 1387 , Loss 4.402776718139648\n",
      "Batch 1388 , Loss 4.4674577713012695\n",
      "Batch 1389 , Loss 4.138950824737549\n",
      "Batch 1390 , Loss 4.433041572570801\n",
      "Batch 1391 , Loss 4.440961837768555\n",
      "Batch 1392 , Loss 4.3485236167907715\n",
      "Batch 1393 , Loss 4.345152854919434\n",
      "Batch 1394 , Loss 4.270290374755859\n",
      "Batch 1395 , Loss 4.328035831451416\n",
      "Batch 1396 , Loss 4.372092247009277\n",
      "Batch 1397 , Loss 4.333706855773926\n",
      "Batch 1398 , Loss 4.278395175933838\n",
      "Batch 1399 , Loss 4.270499229431152\n",
      "Batch 1400 , Loss 4.458741188049316\n",
      "Batch 1401 , Loss 4.225492477416992\n",
      "Batch 1402 , Loss 4.3494462966918945\n",
      "Batch 1403 , Loss 4.389864921569824\n",
      "Batch 1404 , Loss 4.114497184753418\n",
      "Batch 1405 , Loss 4.578305244445801\n",
      "Batch 1406 , Loss 4.264504432678223\n",
      "Batch 1407 , Loss 4.176268577575684\n",
      "Batch 1408 , Loss 4.3501505851745605\n",
      "Batch 1409 , Loss 4.311612129211426\n",
      "Batch 1410 , Loss 4.445192813873291\n",
      "Batch 1411 , Loss 4.190169334411621\n",
      "Batch 1412 , Loss 4.294320583343506\n",
      "Batch 1413 , Loss 4.4168524742126465\n",
      "Batch 1414 , Loss 4.131399154663086\n",
      "Batch 1415 , Loss 4.56820821762085\n",
      "Batch 1416 , Loss 4.494345188140869\n",
      "Batch 1417 , Loss 4.114294528961182\n",
      "Batch 1418 , Loss 4.443477630615234\n",
      "Batch 1419 , Loss 4.3610944747924805\n",
      "Batch 1420 , Loss 4.162293434143066\n",
      "Batch 1421 , Loss 4.4306640625\n",
      "Batch 1422 , Loss 4.225979328155518\n",
      "Batch 1423 , Loss 4.346193313598633\n",
      "Batch 1424 , Loss 4.451757431030273\n",
      "Batch 1425 , Loss 4.490235805511475\n",
      "Batch 1426 , Loss 4.108998775482178\n",
      "Batch 1427 , Loss 4.281824111938477\n",
      "Batch 1428 , Loss 4.287250995635986\n",
      "Batch 1429 , Loss 4.330214977264404\n",
      "Batch 1430 , Loss 4.297813892364502\n",
      "Batch 1431 , Loss 4.368220329284668\n",
      "Batch 1432 , Loss 4.378615379333496\n",
      "Batch 1433 , Loss 4.451616287231445\n",
      "Batch 1434 , Loss 4.4990105628967285\n",
      "Batch 1435 , Loss 4.459217071533203\n",
      "Batch 1436 , Loss 4.387506484985352\n",
      "Batch 1437 , Loss 4.529082775115967\n",
      "Batch 1438 , Loss 4.243309020996094\n",
      "Batch 1439 , Loss 4.2474870681762695\n",
      "Batch 1440 , Loss 4.294812202453613\n",
      "Batch 1441 , Loss 4.297656536102295\n",
      "Batch 1442 , Loss 4.336003303527832\n",
      "Batch 1443 , Loss 4.614995956420898\n",
      "Batch 1444 , Loss 4.530709743499756\n",
      "Batch 1445 , Loss 4.2721052169799805\n",
      "Batch 1446 , Loss 4.323494911193848\n",
      "Batch 1447 , Loss 4.282198429107666\n",
      "Batch 1448 , Loss 4.4504852294921875\n",
      "Batch 1449 , Loss 4.357742786407471\n",
      "Batch 1450 , Loss 4.39729642868042\n",
      "Batch 1451 , Loss 4.515357494354248\n",
      "Batch 1452 , Loss 4.240089416503906\n",
      "Batch 1453 , Loss 4.5182695388793945\n",
      "Batch 1454 , Loss 4.563165187835693\n",
      "Batch 1455 , Loss 4.531904220581055\n",
      "Batch 1456 , Loss 4.363829135894775\n",
      "Batch 1457 , Loss 4.686495780944824\n",
      "Batch 1458 , Loss 4.42680549621582\n",
      "Batch 1459 , Loss 4.1371660232543945\n",
      "Batch 1460 , Loss 4.060972690582275\n",
      "Batch 1461 , Loss 4.270959377288818\n",
      "Batch 1462 , Loss 4.15915060043335\n",
      "Batch 1463 , Loss 4.365460395812988\n",
      "Batch 1464 , Loss 4.368084907531738\n",
      "Batch 1465 , Loss 3.9883856773376465\n",
      "Batch 1466 , Loss 4.406332015991211\n",
      "Batch 1467 , Loss 4.440945148468018\n",
      "Batch 1468 , Loss 4.459129333496094\n",
      "Batch 1469 , Loss 4.408253192901611\n",
      "Batch 1470 , Loss 4.3883280754089355\n",
      "Batch 1471 , Loss 4.364406585693359\n",
      "Batch 1472 , Loss 4.420502662658691\n",
      "Batch 1473 , Loss 4.403182029724121\n",
      "Batch 1474 , Loss 4.370411396026611\n",
      "Batch 1475 , Loss 4.026241302490234\n",
      "Batch 1476 , Loss 4.308333396911621\n",
      "Batch 1477 , Loss 4.41923713684082\n",
      "Batch 1478 , Loss 4.2951812744140625\n",
      "Batch 1479 , Loss 4.6584882736206055\n",
      "Batch 1480 , Loss 4.075225830078125\n",
      "Batch 1481 , Loss 4.405595779418945\n",
      "Batch 1482 , Loss 4.4062886238098145\n",
      "Batch 1483 , Loss 4.316253662109375\n",
      "Batch 1484 , Loss 4.616988182067871\n",
      "Batch 1485 , Loss 4.173959255218506\n",
      "Batch 1486 , Loss 4.451082706451416\n",
      "Batch 1487 , Loss 4.399511337280273\n",
      "Batch 1488 , Loss 4.377504348754883\n",
      "Batch 1489 , Loss 4.408370494842529\n",
      "Batch 1490 , Loss 4.491671562194824\n",
      "Batch 1491 , Loss 4.347285747528076\n",
      "Batch 1492 , Loss 4.500223636627197\n",
      "Batch 1493 , Loss 4.532464027404785\n",
      "Batch 1494 , Loss 4.352741241455078\n",
      "Batch 1495 , Loss 4.539972305297852\n",
      "Batch 1496 , Loss 4.2485857009887695\n",
      "Batch 1497 , Loss 4.277134895324707\n",
      "Batch 1498 , Loss 4.1593427658081055\n",
      "Batch 1499 , Loss 4.296957492828369\n",
      "Batch 1500 , Loss 4.091400146484375\n",
      "Batch 1501 , Loss 4.5624213218688965\n",
      "Batch 1502 , Loss 4.388750076293945\n",
      "Batch 1503 , Loss 4.149147033691406\n",
      "Batch 1504 , Loss 4.1898393630981445\n",
      "Batch 1505 , Loss 4.495290756225586\n",
      "Batch 1506 , Loss 4.445054531097412\n",
      "Batch 1507 , Loss 4.239449501037598\n",
      "Batch 1508 , Loss 4.445802688598633\n",
      "Batch 1509 , Loss 4.306740760803223\n",
      "Batch 1510 , Loss 4.275323867797852\n",
      "Batch 1511 , Loss 4.3658342361450195\n",
      "Batch 1512 , Loss 4.274435043334961\n",
      "Batch 1513 , Loss 4.188772201538086\n",
      "Batch 1514 , Loss 4.144970893859863\n",
      "Batch 1515 , Loss 4.325873374938965\n",
      "Batch 1516 , Loss 4.267265319824219\n",
      "Batch 1517 , Loss 4.6140313148498535\n",
      "Batch 1518 , Loss 4.139048099517822\n",
      "Batch 1519 , Loss 4.5055365562438965\n",
      "Batch 1520 , Loss 4.605099678039551\n",
      "Batch 1521 , Loss 4.605014801025391\n",
      "Batch 1522 , Loss 4.161659240722656\n",
      "Batch 1523 , Loss 4.014039993286133\n",
      "Batch 1524 , Loss 4.329537391662598\n",
      "Batch 1525 , Loss 4.271618366241455\n",
      "Batch 1526 , Loss 4.263015270233154\n",
      "Batch 1527 , Loss 4.305267333984375\n",
      "Batch 1528 , Loss 4.399480819702148\n",
      "Batch 1529 , Loss 4.084486961364746\n",
      "Batch 1530 , Loss 4.215394020080566\n",
      "Batch 1531 , Loss 4.141848564147949\n",
      "Batch 1532 , Loss 4.685596942901611\n",
      "Batch 1533 , Loss 4.2348127365112305\n",
      "Batch 1534 , Loss 4.295145034790039\n",
      "Batch 1535 , Loss 4.2427215576171875\n",
      "Batch 1536 , Loss 4.661389350891113\n",
      "Batch 1537 , Loss 4.253539085388184\n",
      "Batch 1538 , Loss 3.9679503440856934\n",
      "Batch 1539 , Loss 4.3393988609313965\n",
      "Batch 1540 , Loss 4.4724225997924805\n",
      "Batch 1541 , Loss 4.335460186004639\n",
      "Batch 1542 , Loss 4.124322414398193\n",
      "Batch 1543 , Loss 4.334768295288086\n",
      "Batch 1544 , Loss 4.228941440582275\n",
      "Batch 1545 , Loss 4.294531345367432\n",
      "Batch 1546 , Loss 4.301844596862793\n",
      "Batch 1547 , Loss 4.2934489250183105\n",
      "Batch 1548 , Loss 4.145938396453857\n",
      "Batch 1549 , Loss 4.389395713806152\n",
      "Batch 1550 , Loss 4.079806327819824\n",
      "Batch 1551 , Loss 4.3085551261901855\n",
      "Batch 1552 , Loss 4.567102432250977\n",
      "Batch 1553 , Loss 4.2532548904418945\n",
      "Batch 1554 , Loss 4.170662879943848\n",
      "Batch 1555 , Loss 4.243031978607178\n",
      "Batch 1556 , Loss 4.298999786376953\n",
      "Batch 1557 , Loss 4.593451499938965\n",
      "Batch 1558 , Loss 4.29073429107666\n",
      "Batch 1559 , Loss 4.367476463317871\n",
      "Batch 1560 , Loss 4.326661109924316\n",
      "Batch 1561 , Loss 3.9986579418182373\n",
      "Batch 1562 , Loss 4.074930667877197\n",
      "Batch 1563 , Loss 4.169527053833008\n",
      "Batch 1564 , Loss 4.540963172912598\n",
      "Batch 1565 , Loss 4.325905799865723\n",
      "Batch 1566 , Loss 4.252889633178711\n",
      "Batch 1567 , Loss 4.059166431427002\n",
      "Batch 1568 , Loss 4.1403069496154785\n",
      "Batch 1569 , Loss 4.134923934936523\n",
      "Batch 1570 , Loss 4.184775352478027\n",
      "Batch 1571 , Loss 4.216119766235352\n",
      "Batch 1572 , Loss 4.100376129150391\n",
      "Batch 1573 , Loss 4.432549476623535\n",
      "Batch 1574 , Loss 4.301320552825928\n",
      "Batch 1575 , Loss 4.172924995422363\n",
      "Batch 1576 , Loss 4.152740001678467\n",
      "Batch 1577 , Loss 4.325379371643066\n",
      "Batch 1578 , Loss 4.4253997802734375\n",
      "Batch 1579 , Loss 4.286854267120361\n",
      "Batch 1580 , Loss 4.125728607177734\n",
      "Batch 1581 , Loss 4.1031975746154785\n",
      "Batch 1582 , Loss 4.426466464996338\n",
      "Batch 1583 , Loss 4.305754661560059\n",
      "Batch 1584 , Loss 4.131547927856445\n",
      "Batch 1585 , Loss 4.170698165893555\n",
      "Batch 1586 , Loss 4.245326995849609\n",
      "Batch 1587 , Loss 4.282424449920654\n",
      "Batch 1588 , Loss 4.302506446838379\n",
      "Batch 1589 , Loss 4.524161338806152\n",
      "Batch 1590 , Loss 4.168715953826904\n",
      "Batch 1591 , Loss 4.323721885681152\n",
      "Batch 1592 , Loss 4.358283042907715\n",
      "Batch 1593 , Loss 4.272091865539551\n",
      "Batch 1594 , Loss 4.267266750335693\n",
      "Batch 1595 , Loss 4.4059224128723145\n",
      "Batch 1596 , Loss 4.2869768142700195\n",
      "Batch 1597 , Loss 4.077988147735596\n",
      "Batch 1598 , Loss 4.2371344566345215\n",
      "Batch 1599 , Loss 4.102752685546875\n",
      "Batch 1600 , Loss 4.752522945404053\n",
      "Batch 1601 , Loss 4.2063374519348145\n",
      "Batch 1602 , Loss 4.133089065551758\n",
      "Batch 1603 , Loss 4.3882551193237305\n",
      "Batch 1604 , Loss 4.021880149841309\n",
      "Batch 1605 , Loss 4.401294708251953\n",
      "Batch 1606 , Loss 4.135619163513184\n",
      "Batch 1607 , Loss 4.5482940673828125\n",
      "Batch 1608 , Loss 4.270406723022461\n",
      "Batch 1609 , Loss 4.483007907867432\n",
      "Batch 1610 , Loss 4.472216606140137\n",
      "Batch 1611 , Loss 4.202508926391602\n",
      "Batch 1612 , Loss 4.240760326385498\n",
      "Batch 1613 , Loss 4.660780906677246\n",
      "Batch 1614 , Loss 4.675441265106201\n",
      "Batch 1615 , Loss 4.280217170715332\n",
      "Batch 1616 , Loss 4.525913238525391\n",
      "Batch 1617 , Loss 4.33222770690918\n",
      "Batch 1618 , Loss 4.540935039520264\n",
      "Batch 1619 , Loss 3.9855198860168457\n",
      "Batch 1620 , Loss 3.9547481536865234\n",
      "Batch 1621 , Loss 4.222784996032715\n",
      "Batch 1622 , Loss 4.189786434173584\n",
      "Batch 1623 , Loss 4.183779716491699\n",
      "Batch 1624 , Loss 4.111947059631348\n",
      "Batch 1625 , Loss 4.552786827087402\n",
      "Batch 1626 , Loss 4.247405052185059\n",
      "Batch 1627 , Loss 4.2370829582214355\n",
      "Batch 1628 , Loss 4.174762725830078\n",
      "Batch 1629 , Loss 4.148614883422852\n",
      "Batch 1630 , Loss 4.57126522064209\n",
      "Batch 1631 , Loss 4.1611762046813965\n",
      "Batch 1632 , Loss 4.137751579284668\n",
      "Batch 1633 , Loss 4.229147434234619\n",
      "Batch 1634 , Loss 4.37584924697876\n",
      "Batch 1635 , Loss 4.24152135848999\n",
      "Batch 1636 , Loss 4.225851535797119\n",
      "Batch 1637 , Loss 4.33489465713501\n",
      "Batch 1638 , Loss 4.2276458740234375\n",
      "Batch 1639 , Loss 4.637189865112305\n",
      "Batch 1640 , Loss 4.27853536605835\n",
      "Batch 1641 , Loss 4.018369674682617\n",
      "Batch 1642 , Loss 4.328193664550781\n",
      "Batch 1643 , Loss 4.267673492431641\n",
      "Batch 1644 , Loss 4.010434627532959\n",
      "Batch 1645 , Loss 4.4569597244262695\n",
      "Batch 1646 , Loss 4.388518333435059\n",
      "Batch 1647 , Loss 4.339455604553223\n",
      "Batch 1648 , Loss 4.315729141235352\n",
      "Batch 1649 , Loss 4.395901679992676\n",
      "Batch 1650 , Loss 4.268158912658691\n",
      "Batch 1651 , Loss 4.209565162658691\n",
      "Batch 1652 , Loss 4.441144943237305\n",
      "Batch 1653 , Loss 4.031900405883789\n",
      "Batch 1654 , Loss 4.1685261726379395\n",
      "Batch 1655 , Loss 4.684159755706787\n",
      "Batch 1656 , Loss 4.323612689971924\n",
      "Batch 1657 , Loss 4.105309009552002\n",
      "Batch 1658 , Loss 4.1311936378479\n",
      "Batch 1659 , Loss 4.401223659515381\n",
      "Batch 1660 , Loss 4.146882057189941\n",
      "Batch 1661 , Loss 4.195843696594238\n",
      "Batch 1662 , Loss 4.426272392272949\n",
      "Batch 1663 , Loss 4.34532356262207\n",
      "Batch 1664 , Loss 4.239152431488037\n",
      "Batch 1665 , Loss 4.462040901184082\n",
      "Batch 1666 , Loss 4.083256244659424\n",
      "Batch 1667 , Loss 4.163832664489746\n",
      "Batch 1668 , Loss 4.518826961517334\n",
      "Batch 1669 , Loss 4.499762058258057\n",
      "Batch 1670 , Loss 4.402429580688477\n",
      "Batch 1671 , Loss 4.151124000549316\n",
      "Batch 1672 , Loss 4.236187934875488\n",
      "Batch 1673 , Loss 4.307615280151367\n",
      "Batch 1674 , Loss 4.152174949645996\n",
      "Batch 1675 , Loss 4.415192604064941\n",
      "Batch 1676 , Loss 4.469437599182129\n",
      "Batch 1677 , Loss 4.127536296844482\n",
      "Batch 1678 , Loss 4.426466941833496\n",
      "Batch 1679 , Loss 4.3958353996276855\n",
      "Batch 1680 , Loss 4.4823198318481445\n",
      "Batch 1681 , Loss 4.07319974899292\n",
      "Batch 1682 , Loss 4.161812782287598\n",
      "Batch 1683 , Loss 4.282230854034424\n",
      "Batch 1684 , Loss 4.351789474487305\n",
      "Batch 1685 , Loss 4.205111980438232\n",
      "Batch 1686 , Loss 4.309319496154785\n",
      "Batch 1687 , Loss 4.085735321044922\n",
      "Batch 1688 , Loss 4.126309871673584\n",
      "Batch 1689 , Loss 4.275697231292725\n",
      "Batch 1690 , Loss 4.083172798156738\n",
      "Batch 1691 , Loss 4.085968971252441\n",
      "Batch 1692 , Loss 4.132253170013428\n",
      "Batch 1693 , Loss 4.165790557861328\n",
      "Batch 1694 , Loss 4.410646915435791\n",
      "Batch 1695 , Loss 4.382236480712891\n",
      "Batch 1696 , Loss 3.9264113903045654\n",
      "Batch 1697 , Loss 4.305911064147949\n",
      "Batch 1698 , Loss 4.287512302398682\n",
      "Batch 1699 , Loss 3.9463272094726562\n",
      "Batch 1700 , Loss 4.343373775482178\n",
      "Batch 1701 , Loss 4.103078365325928\n",
      "Batch 1702 , Loss 4.052158832550049\n",
      "Batch 1703 , Loss 3.8995676040649414\n",
      "Batch 1704 , Loss 4.44254207611084\n",
      "Batch 1705 , Loss 4.139089584350586\n",
      "Batch 1706 , Loss 4.1616106033325195\n",
      "Batch 1707 , Loss 4.025028705596924\n",
      "Batch 1708 , Loss 4.246283531188965\n",
      "Batch 1709 , Loss 4.377612113952637\n",
      "Batch 1710 , Loss 4.346549987792969\n",
      "Batch 1711 , Loss 3.962923765182495\n",
      "Batch 1712 , Loss 4.078197479248047\n",
      "Batch 1713 , Loss 4.2500410079956055\n",
      "Batch 1714 , Loss 4.226132869720459\n",
      "Batch 1715 , Loss 4.248516082763672\n",
      "Batch 1716 , Loss 4.113330841064453\n",
      "Batch 1717 , Loss 4.098152160644531\n",
      "Batch 1718 , Loss 4.496042251586914\n",
      "Batch 1719 , Loss 4.108804702758789\n",
      "Batch 1720 , Loss 4.151474952697754\n",
      "Batch 1721 , Loss 4.099874496459961\n",
      "Batch 1722 , Loss 3.932591676712036\n",
      "Batch 1723 , Loss 4.343959808349609\n",
      "Batch 1724 , Loss 3.9787135124206543\n",
      "Batch 1725 , Loss 4.065604209899902\n",
      "Batch 1726 , Loss 4.210459232330322\n",
      "Batch 1727 , Loss 4.194841384887695\n",
      "Batch 1728 , Loss 4.225612163543701\n",
      "Batch 1729 , Loss 4.265202522277832\n",
      "Batch 1730 , Loss 4.216551303863525\n",
      "Batch 1731 , Loss 4.139481544494629\n",
      "Batch 1732 , Loss 4.086824893951416\n",
      "Batch 1733 , Loss 4.0165839195251465\n",
      "Batch 1734 , Loss 4.2486724853515625\n",
      "Batch 1735 , Loss 4.146549224853516\n",
      "Batch 1736 , Loss 4.207347869873047\n",
      "Batch 1737 , Loss 4.467841148376465\n",
      "Batch 1738 , Loss 4.298914432525635\n",
      "Batch 1739 , Loss 4.278707981109619\n",
      "Batch 1740 , Loss 4.192409515380859\n",
      "Batch 1741 , Loss 4.208898544311523\n",
      "Batch 1742 , Loss 4.076963901519775\n",
      "Batch 1743 , Loss 4.129954814910889\n",
      "Batch 1744 , Loss 4.386691093444824\n",
      "Batch 1745 , Loss 4.175495147705078\n",
      "Batch 1746 , Loss 4.011786937713623\n",
      "Batch 1747 , Loss 3.9652421474456787\n",
      "Batch 1748 , Loss 4.09468936920166\n",
      "Batch 1749 , Loss 4.015246391296387\n",
      "Batch 1750 , Loss 4.159158706665039\n",
      "Batch 1751 , Loss 4.311159133911133\n",
      "Batch 1752 , Loss 4.164399147033691\n",
      "Batch 1753 , Loss 4.230361461639404\n",
      "Batch 1754 , Loss 4.162782669067383\n",
      "Batch 1755 , Loss 4.081194877624512\n",
      "Batch 1756 , Loss 4.258514404296875\n",
      "Batch 1757 , Loss 4.4005937576293945\n",
      "Batch 1758 , Loss 4.206640243530273\n",
      "Batch 1759 , Loss 4.164941787719727\n",
      "Batch 1760 , Loss 4.225311756134033\n",
      "Batch 1761 , Loss 4.2547149658203125\n",
      "Batch 1762 , Loss 4.185323715209961\n",
      "Batch 1763 , Loss 4.085719108581543\n",
      "Batch 1764 , Loss 4.32415771484375\n",
      "Batch 1765 , Loss 4.138509750366211\n",
      "Batch 1766 , Loss 4.5352463722229\n",
      "Batch 1767 , Loss 4.266266822814941\n",
      "Batch 1768 , Loss 4.126372337341309\n",
      "Batch 1769 , Loss 4.124759674072266\n",
      "Batch 1770 , Loss 4.323530673980713\n",
      "Batch 1771 , Loss 3.8545899391174316\n",
      "Batch 1772 , Loss 4.311519145965576\n",
      "Batch 1773 , Loss 4.150195598602295\n",
      "Batch 1774 , Loss 4.187108993530273\n",
      "Batch 1775 , Loss 4.195926189422607\n",
      "Batch 1776 , Loss 3.921495199203491\n",
      "Batch 1777 , Loss 4.247463703155518\n",
      "Batch 1778 , Loss 3.9099836349487305\n",
      "Batch 1779 , Loss 4.283936500549316\n",
      "Batch 1780 , Loss 4.181011199951172\n",
      "Batch 1781 , Loss 4.373294353485107\n",
      "Batch 1782 , Loss 4.193620204925537\n",
      "Batch 1783 , Loss 4.2460551261901855\n",
      "Batch 1784 , Loss 4.286955833435059\n",
      "Batch 1785 , Loss 4.127625465393066\n",
      "Batch 1786 , Loss 4.013832092285156\n",
      "Batch 1787 , Loss 3.8293652534484863\n",
      "Batch 1788 , Loss 4.218601226806641\n",
      "Batch 1789 , Loss 4.191061496734619\n",
      "Batch 1790 , Loss 3.838982105255127\n",
      "Batch 1791 , Loss 4.246238708496094\n",
      "Batch 1792 , Loss 3.8975892066955566\n",
      "Batch 1793 , Loss 3.997856855392456\n",
      "Batch 1794 , Loss 4.028275489807129\n",
      "Batch 1795 , Loss 4.3238749504089355\n",
      "Batch 1796 , Loss 4.159449577331543\n",
      "Batch 1797 , Loss 3.985523223876953\n",
      "Batch 1798 , Loss 4.136871814727783\n",
      "Batch 1799 , Loss 4.097999572753906\n",
      "Batch 1800 , Loss 4.084282875061035\n",
      "Batch 1801 , Loss 4.151848316192627\n",
      "Batch 1802 , Loss 4.221112251281738\n",
      "Batch 1803 , Loss 4.234827518463135\n",
      "Batch 1804 , Loss 4.05061674118042\n",
      "Batch 1805 , Loss 4.044898986816406\n",
      "Batch 1806 , Loss 4.265138626098633\n",
      "Batch 1807 , Loss 4.257066249847412\n",
      "Batch 1808 , Loss 4.198450088500977\n",
      "Batch 1809 , Loss 3.9540696144104004\n",
      "Batch 1810 , Loss 4.115067958831787\n",
      "Batch 1811 , Loss 4.099916458129883\n",
      "Batch 1812 , Loss 4.234859943389893\n",
      "Batch 1813 , Loss 4.082949638366699\n",
      "Batch 1814 , Loss 4.168664932250977\n",
      "Batch 1815 , Loss 4.17328405380249\n",
      "Batch 1816 , Loss 4.180912971496582\n",
      "Batch 1817 , Loss 3.944927930831909\n",
      "Batch 1818 , Loss 4.200519561767578\n",
      "Batch 1819 , Loss 3.926933526992798\n",
      "Batch 1820 , Loss 4.071418285369873\n",
      "Batch 1821 , Loss 4.305296897888184\n",
      "Batch 1822 , Loss 4.136848449707031\n",
      "Batch 1823 , Loss 4.3174567222595215\n",
      "Batch 1824 , Loss 4.175402641296387\n",
      "Batch 1825 , Loss 4.165539741516113\n",
      "Batch 1826 , Loss 4.084228515625\n",
      "Batch 1827 , Loss 4.048362731933594\n",
      "Batch 1828 , Loss 4.103102684020996\n",
      "Batch 1829 , Loss 4.22025728225708\n",
      "Batch 1830 , Loss 4.2449846267700195\n",
      "Batch 1831 , Loss 4.062414646148682\n",
      "Batch 1832 , Loss 3.8898873329162598\n",
      "Batch 1833 , Loss 4.144832134246826\n",
      "Batch 1834 , Loss 4.070106029510498\n",
      "Batch 1835 , Loss 4.144250869750977\n",
      "Batch 1836 , Loss 4.2429423332214355\n",
      "Batch 1837 , Loss 4.100563049316406\n",
      "Batch 1838 , Loss 4.115608215332031\n",
      "Batch 1839 , Loss 4.165294647216797\n",
      "Batch 1840 , Loss 4.242414474487305\n",
      "Batch 1841 , Loss 4.382198333740234\n",
      "Batch 1842 , Loss 4.127807140350342\n",
      "Batch 1843 , Loss 4.342264652252197\n",
      "Batch 1844 , Loss 4.086325645446777\n",
      "Batch 1845 , Loss 4.249943256378174\n",
      "Batch 1846 , Loss 4.250507354736328\n",
      "Batch 1847 , Loss 4.076362609863281\n",
      "Batch 1848 , Loss 4.54660177230835\n",
      "Batch 1849 , Loss 4.089626312255859\n",
      "Batch 1850 , Loss 4.186140060424805\n",
      "Batch 1851 , Loss 4.042450904846191\n",
      "Batch 1852 , Loss 4.347695827484131\n",
      "Batch 1853 , Loss 4.205531597137451\n",
      "Batch 1854 , Loss 4.370905876159668\n",
      "Batch 1855 , Loss 3.696840763092041\n",
      "Batch 1856 , Loss 4.177124500274658\n",
      "Batch 1857 , Loss 4.212217330932617\n",
      "Batch 1858 , Loss 4.024538040161133\n",
      "Batch 1859 , Loss 4.18800163269043\n",
      "Batch 1860 , Loss 4.165005683898926\n",
      "Batch 1861 , Loss 4.157496929168701\n",
      "Batch 1862 , Loss 4.434390068054199\n",
      "Batch 1863 , Loss 4.114685535430908\n",
      "Batch 1864 , Loss 4.190583229064941\n",
      "Batch 1865 , Loss 3.9728479385375977\n",
      "Batch 1866 , Loss 3.988584280014038\n",
      "Batch 1867 , Loss 4.214266300201416\n",
      "Batch 1868 , Loss 4.03305721282959\n",
      "Batch 1869 , Loss 4.233990669250488\n",
      "Batch 1870 , Loss 4.261706352233887\n",
      "Batch 1871 , Loss 4.185041427612305\n",
      "Batch 1872 , Loss 3.941516399383545\n",
      "Batch 1873 , Loss 4.093768119812012\n",
      "Batch 1874 , Loss 3.9843556880950928\n",
      "Batch 1875 , Loss 4.131306171417236\n",
      "Batch 1876 , Loss 4.356002330780029\n",
      "Batch 1877 , Loss 4.1814422607421875\n",
      "Batch 1878 , Loss 4.2303147315979\n",
      "Batch 1879 , Loss 4.122178077697754\n",
      "Batch 1880 , Loss 4.069190979003906\n",
      "Batch 1881 , Loss 3.9464192390441895\n",
      "Batch 1882 , Loss 4.1246747970581055\n",
      "Batch 1883 , Loss 3.9269871711730957\n",
      "Batch 1884 , Loss 3.977492332458496\n",
      "Batch 1885 , Loss 4.311239242553711\n",
      "Batch 1886 , Loss 4.053001403808594\n",
      "Batch 1887 , Loss 4.006447792053223\n",
      "Batch 1888 , Loss 4.050281047821045\n",
      "Batch 1889 , Loss 4.029262542724609\n",
      "Batch 1890 , Loss 4.082099914550781\n",
      "Batch 1891 , Loss 4.128157615661621\n",
      "Batch 1892 , Loss 4.122866630554199\n",
      "Batch 1893 , Loss 4.358240127563477\n",
      "Batch 1894 , Loss 4.214642524719238\n",
      "Batch 1895 , Loss 4.0988545417785645\n",
      "Batch 1896 , Loss 3.9743025302886963\n",
      "Batch 1897 , Loss 4.1606550216674805\n",
      "Batch 1898 , Loss 4.0267157554626465\n",
      "Batch 1899 , Loss 4.078891277313232\n",
      "Batch 1900 , Loss 4.210366249084473\n",
      "Batch 1901 , Loss 4.033703804016113\n",
      "Batch 1902 , Loss 3.8802244663238525\n",
      "Batch 1903 , Loss 3.9676473140716553\n",
      "Batch 1904 , Loss 4.094597816467285\n",
      "Batch 1905 , Loss 4.094657897949219\n",
      "Batch 1906 , Loss 4.2556352615356445\n",
      "Batch 1907 , Loss 4.162337303161621\n",
      "Batch 1908 , Loss 3.9106240272521973\n",
      "Batch 1909 , Loss 3.9364140033721924\n",
      "Batch 1910 , Loss 3.9729714393615723\n",
      "Batch 1911 , Loss 4.089576721191406\n",
      "Batch 1912 , Loss 4.102771759033203\n",
      "Batch 1913 , Loss 3.9379525184631348\n",
      "Batch 1914 , Loss 3.8885672092437744\n",
      "Batch 1915 , Loss 3.8438665866851807\n",
      "Batch 1916 , Loss 3.7831292152404785\n",
      "Batch 1917 , Loss 4.287038326263428\n",
      "Batch 1918 , Loss 4.078645706176758\n",
      "Batch 1919 , Loss 4.103547096252441\n",
      "Batch 1920 , Loss 3.953205108642578\n",
      "Batch 1921 , Loss 4.004842758178711\n",
      "Batch 1922 , Loss 4.422785758972168\n",
      "Batch 1923 , Loss 4.106264114379883\n",
      "Batch 1924 , Loss 4.088732719421387\n",
      "Batch 1925 , Loss 4.487381935119629\n",
      "Batch 1926 , Loss 3.9576549530029297\n",
      "Batch 1927 , Loss 4.026703834533691\n",
      "Batch 1928 , Loss 3.9864139556884766\n",
      "Batch 1929 , Loss 4.010421276092529\n",
      "Batch 1930 , Loss 4.037723064422607\n",
      "Batch 1931 , Loss 4.104515552520752\n",
      "Batch 1932 , Loss 4.290871620178223\n",
      "Batch 1933 , Loss 4.125847816467285\n",
      "Batch 1934 , Loss 3.895616054534912\n",
      "Batch 1935 , Loss 4.108353137969971\n",
      "Batch 1936 , Loss 4.1515302658081055\n",
      "Batch 1937 , Loss 4.185680389404297\n",
      "Batch 1938 , Loss 4.326973915100098\n",
      "Batch 1939 , Loss 4.067507266998291\n",
      "Batch 1940 , Loss 4.352654457092285\n",
      "Batch 1941 , Loss 4.150537014007568\n",
      "Batch 1942 , Loss 3.9947242736816406\n",
      "Batch 1943 , Loss 4.075155735015869\n",
      "Batch 1944 , Loss 3.992520809173584\n",
      "Batch 1945 , Loss 4.157235145568848\n",
      "Batch 1946 , Loss 4.067268371582031\n",
      "Batch 1947 , Loss 4.282869815826416\n",
      "Batch 1948 , Loss 3.9106454849243164\n",
      "Batch 1949 , Loss 4.034650802612305\n",
      "Batch 1950 , Loss 3.9511289596557617\n",
      "Batch 1951 , Loss 4.066437244415283\n",
      "Batch 1952 , Loss 4.217364311218262\n",
      "Batch 1953 , Loss 4.047709941864014\n",
      "Batch 1954 , Loss 4.140443325042725\n",
      "Batch 1955 , Loss 4.03057861328125\n",
      "Batch 1956 , Loss 3.982851266860962\n",
      "Batch 1957 , Loss 3.983964443206787\n",
      "Batch 1958 , Loss 4.297171115875244\n",
      "Batch 1959 , Loss 3.9981448650360107\n",
      "Batch 1960 , Loss 4.308741092681885\n",
      "Batch 1961 , Loss 3.8550169467926025\n",
      "Batch 1962 , Loss 4.468474388122559\n",
      "Batch 1963 , Loss 4.148049354553223\n",
      "Batch 1964 , Loss 4.158754348754883\n",
      "Batch 1965 , Loss 4.173548221588135\n",
      "Batch 1966 , Loss 4.109264373779297\n",
      "Batch 1967 , Loss 4.192707538604736\n",
      "Batch 1968 , Loss 3.7852284908294678\n",
      "Batch 1969 , Loss 4.1263227462768555\n",
      "Batch 1970 , Loss 4.425964832305908\n",
      "Batch 1971 , Loss 3.9573073387145996\n",
      "Batch 1972 , Loss 4.04124116897583\n",
      "Batch 1973 , Loss 4.2972564697265625\n",
      "Batch 1974 , Loss 3.872224807739258\n",
      "Batch 1975 , Loss 4.039687156677246\n",
      "Batch 1976 , Loss 4.117881774902344\n",
      "Batch 1977 , Loss 4.049443244934082\n",
      "Batch 1978 , Loss 4.045773506164551\n",
      "Batch 1979 , Loss 3.9065089225769043\n",
      "Batch 1980 , Loss 4.0732808113098145\n",
      "Batch 1981 , Loss 4.177455902099609\n",
      "Batch 1982 , Loss 4.165858268737793\n",
      "Batch 1983 , Loss 4.2191691398620605\n",
      "Batch 1984 , Loss 4.376279354095459\n",
      "Batch 1985 , Loss 4.186023235321045\n",
      "Batch 1986 , Loss 4.30833101272583\n",
      "Batch 1987 , Loss 4.159996032714844\n",
      "Batch 1988 , Loss 4.21070671081543\n",
      "Batch 1989 , Loss 4.015354156494141\n",
      "Batch 1990 , Loss 3.861032485961914\n",
      "Batch 1991 , Loss 4.009893417358398\n",
      "Batch 1992 , Loss 3.8935749530792236\n",
      "Batch 1993 , Loss 4.3620734214782715\n",
      "Batch 1994 , Loss 3.9924917221069336\n",
      "Batch 1995 , Loss 4.077824115753174\n",
      "Batch 1996 , Loss 4.431086540222168\n",
      "Batch 1997 , Loss 3.9769201278686523\n",
      "Batch 1998 , Loss 4.0385918617248535\n",
      "Batch 1999 , Loss 4.154239654541016\n",
      "Batch 2000 , Loss 3.9065964221954346\n",
      "Batch 2001 , Loss 4.160501480102539\n",
      "Batch 2002 , Loss 3.9290452003479004\n",
      "Batch 2003 , Loss 3.9387242794036865\n",
      "Batch 2004 , Loss 4.071660995483398\n",
      "Batch 2005 , Loss 4.054582118988037\n",
      "Batch 2006 , Loss 4.206545829772949\n",
      "Batch 2007 , Loss 3.9822258949279785\n",
      "Batch 2008 , Loss 3.8924591541290283\n",
      "Batch 2009 , Loss 3.8509604930877686\n",
      "Batch 2010 , Loss 4.342501163482666\n",
      "Batch 2011 , Loss 4.156795501708984\n",
      "Batch 2012 , Loss 4.220365524291992\n",
      "Batch 2013 , Loss 4.311007499694824\n",
      "Batch 2014 , Loss 4.152869701385498\n",
      "Batch 2015 , Loss 3.9395813941955566\n",
      "Batch 2016 , Loss 3.7396960258483887\n",
      "Batch 2017 , Loss 3.976905345916748\n",
      "Batch 2018 , Loss 4.253889083862305\n",
      "Batch 2019 , Loss 3.9323792457580566\n",
      "Batch 2020 , Loss 3.9434642791748047\n",
      "Batch 2021 , Loss 3.997802734375\n",
      "Batch 2022 , Loss 3.8944101333618164\n",
      "Batch 2023 , Loss 4.207985877990723\n",
      "Batch 2024 , Loss 3.871732711791992\n",
      "Batch 2025 , Loss 4.181822776794434\n",
      "Batch 2026 , Loss 3.9319238662719727\n",
      "Batch 2027 , Loss 4.166642189025879\n",
      "Batch 2028 , Loss 3.994180679321289\n",
      "Batch 2029 , Loss 4.146886825561523\n",
      "Batch 2030 , Loss 3.922074794769287\n",
      "Batch 2031 , Loss 4.065511703491211\n",
      "Batch 2032 , Loss 3.852508068084717\n",
      "Batch 2033 , Loss 3.826319456100464\n",
      "Batch 2034 , Loss 4.29740571975708\n",
      "Batch 2035 , Loss 4.28061580657959\n",
      "Batch 2036 , Loss 4.167306900024414\n",
      "Batch 2037 , Loss 4.236850261688232\n",
      "Batch 2038 , Loss 4.1582136154174805\n",
      "Batch 2039 , Loss 4.204620361328125\n",
      "Batch 2040 , Loss 4.097268104553223\n",
      "Batch 2041 , Loss 3.9648852348327637\n",
      "Batch 2042 , Loss 4.031815528869629\n",
      "Batch 2043 , Loss 4.003055095672607\n",
      "Batch 2044 , Loss 4.183480262756348\n",
      "Batch 2045 , Loss 4.257277488708496\n",
      "Batch 2046 , Loss 4.3397932052612305\n",
      "Batch 2047 , Loss 4.281371116638184\n",
      "Batch 2048 , Loss 4.116855621337891\n",
      "Batch 2049 , Loss 4.283373832702637\n",
      "Batch 2050 , Loss 4.003840446472168\n",
      "Batch 2051 , Loss 3.942533493041992\n",
      "Batch 2052 , Loss 4.018111228942871\n",
      "Batch 2053 , Loss 3.959628105163574\n",
      "Batch 2054 , Loss 3.9521591663360596\n",
      "Batch 2055 , Loss 4.13707160949707\n",
      "Batch 2056 , Loss 4.021570682525635\n",
      "Batch 2057 , Loss 4.113043785095215\n",
      "Batch 2058 , Loss 4.036676406860352\n",
      "Batch 2059 , Loss 4.129055976867676\n",
      "Batch 2060 , Loss 4.026853561401367\n",
      "Batch 2061 , Loss 4.401571273803711\n",
      "Batch 2062 , Loss 4.009313106536865\n",
      "Batch 2063 , Loss 4.144781112670898\n",
      "Batch 2064 , Loss 3.9360601902008057\n",
      "Batch 2065 , Loss 3.9922304153442383\n",
      "Batch 2066 , Loss 4.278119087219238\n",
      "Batch 2067 , Loss 4.097503662109375\n",
      "Batch 2068 , Loss 3.978498935699463\n",
      "Batch 2069 , Loss 4.048457145690918\n",
      "Batch 2070 , Loss 3.9447271823883057\n",
      "Batch 2071 , Loss 3.8305583000183105\n",
      "Batch 2072 , Loss 4.333107948303223\n",
      "Batch 2073 , Loss 4.178465366363525\n",
      "Batch 2074 , Loss 3.858680248260498\n",
      "Batch 2075 , Loss 4.154494285583496\n",
      "Batch 2076 , Loss 4.1090497970581055\n",
      "Batch 2077 , Loss 4.13152551651001\n",
      "Batch 2078 , Loss 3.960611343383789\n",
      "Batch 2079 , Loss 4.026122093200684\n",
      "Batch 2080 , Loss 3.895128011703491\n",
      "Batch 2081 , Loss 3.9164786338806152\n",
      "Batch 2082 , Loss 3.9037070274353027\n",
      "Batch 2083 , Loss 3.947817802429199\n",
      "Batch 2084 , Loss 3.991098403930664\n",
      "Batch 2085 , Loss 4.147561073303223\n",
      "Batch 2086 , Loss 3.959104299545288\n",
      "Batch 2087 , Loss 4.102782249450684\n",
      "Batch 2088 , Loss 4.0848798751831055\n",
      "Batch 2089 , Loss 4.052333831787109\n",
      "Batch 2090 , Loss 3.886241912841797\n",
      "Batch 2091 , Loss 4.063114643096924\n",
      "Batch 2092 , Loss 4.419480323791504\n",
      "Batch 2093 , Loss 3.9326910972595215\n",
      "Batch 2094 , Loss 3.8425655364990234\n",
      "Batch 2095 , Loss 3.998009204864502\n",
      "Batch 2096 , Loss 4.38419246673584\n",
      "Batch 2097 , Loss 4.106503486633301\n",
      "Batch 2098 , Loss 3.867743730545044\n",
      "Batch 2099 , Loss 4.16752815246582\n",
      "Batch 2100 , Loss 3.7664856910705566\n",
      "Batch 2101 , Loss 3.7745320796966553\n",
      "Batch 2102 , Loss 3.8506548404693604\n",
      "Batch 2103 , Loss 3.999189853668213\n",
      "Batch 2104 , Loss 3.9902210235595703\n",
      "Batch 2105 , Loss 4.197667121887207\n",
      "Batch 2106 , Loss 3.923049211502075\n",
      "Batch 2107 , Loss 4.1478352546691895\n",
      "Batch 2108 , Loss 3.9819858074188232\n",
      "Batch 2109 , Loss 4.046293258666992\n",
      "Batch 2110 , Loss 3.6805758476257324\n",
      "Batch 2111 , Loss 4.090052127838135\n",
      "Batch 2112 , Loss 4.115301132202148\n",
      "Batch 2113 , Loss 4.129631519317627\n",
      "Batch 2114 , Loss 4.101834297180176\n",
      "Batch 2115 , Loss 4.092360019683838\n",
      "Batch 2116 , Loss 3.970231771469116\n",
      "Batch 2117 , Loss 4.1987690925598145\n",
      "Batch 2118 , Loss 4.183387756347656\n",
      "Batch 2119 , Loss 3.947711229324341\n",
      "Batch 2120 , Loss 3.982003688812256\n",
      "Batch 2121 , Loss 4.215761661529541\n",
      "Batch 2122 , Loss 4.0505218505859375\n",
      "Batch 2123 , Loss 4.22608757019043\n",
      "Batch 2124 , Loss 4.115509986877441\n",
      "Batch 2125 , Loss 3.8962202072143555\n",
      "Batch 2126 , Loss 3.891602039337158\n",
      "Batch 2127 , Loss 3.8401360511779785\n",
      "Batch 2128 , Loss 4.1434712409973145\n",
      "Batch 2129 , Loss 3.887641429901123\n",
      "Batch 2130 , Loss 4.139340400695801\n",
      "Batch 2131 , Loss 4.07060432434082\n",
      "Batch 2132 , Loss 4.009511947631836\n",
      "Batch 2133 , Loss 4.008645534515381\n",
      "Batch 2134 , Loss 4.069556713104248\n",
      "Batch 2135 , Loss 4.151018142700195\n",
      "Batch 2136 , Loss 4.055146217346191\n",
      "Batch 2137 , Loss 4.22524356842041\n",
      "Batch 2138 , Loss 4.13587760925293\n",
      "Batch 2139 , Loss 4.05265474319458\n",
      "Batch 2140 , Loss 3.93780255317688\n",
      "Batch 2141 , Loss 3.8325963020324707\n",
      "Batch 2142 , Loss 4.05677604675293\n",
      "Batch 2143 , Loss 4.213807106018066\n",
      "Batch 2144 , Loss 3.8161978721618652\n",
      "Batch 2145 , Loss 4.25596284866333\n",
      "Batch 2146 , Loss 3.8114395141601562\n",
      "Batch 2147 , Loss 3.825328826904297\n",
      "Batch 2148 , Loss 3.879964590072632\n",
      "Batch 2149 , Loss 3.947819232940674\n",
      "Batch 2150 , Loss 4.180790901184082\n",
      "Batch 2151 , Loss 3.750476360321045\n",
      "Batch 2152 , Loss 4.001607894897461\n",
      "Batch 2153 , Loss 4.157339096069336\n",
      "Batch 2154 , Loss 3.8146770000457764\n",
      "Batch 2155 , Loss 4.168035507202148\n",
      "Batch 2156 , Loss 3.86397647857666\n",
      "Batch 2157 , Loss 3.9084718227386475\n",
      "Batch 2158 , Loss 4.168025970458984\n",
      "Batch 2159 , Loss 3.961449146270752\n",
      "Batch 2160 , Loss 4.016563892364502\n",
      "Batch 2161 , Loss 3.9448509216308594\n",
      "Batch 2162 , Loss 4.308228492736816\n",
      "Batch 2163 , Loss 3.6463117599487305\n",
      "Batch 2164 , Loss 4.04951810836792\n",
      "Batch 2165 , Loss 3.939295768737793\n",
      "Batch 2166 , Loss 3.8643884658813477\n",
      "Batch 2167 , Loss 3.9584741592407227\n",
      "Batch 2168 , Loss 4.24171257019043\n",
      "Batch 2169 , Loss 3.8917484283447266\n",
      "Batch 2170 , Loss 4.122539520263672\n",
      "Batch 2171 , Loss 4.0500946044921875\n",
      "Batch 2172 , Loss 4.045247554779053\n",
      "Batch 2173 , Loss 4.128902435302734\n",
      "Batch 2174 , Loss 3.892683506011963\n",
      "Batch 2175 , Loss 4.034379959106445\n",
      "Batch 2176 , Loss 4.228126525878906\n",
      "Batch 2177 , Loss 4.028702735900879\n",
      "Batch 2178 , Loss 4.375771999359131\n",
      "Batch 2179 , Loss 4.086167335510254\n",
      "Batch 2180 , Loss 4.175231456756592\n",
      "Batch 2181 , Loss 3.8437368869781494\n",
      "Batch 2182 , Loss 3.979038715362549\n",
      "Batch 2183 , Loss 3.8069629669189453\n",
      "Batch 2184 , Loss 4.040889739990234\n",
      "Batch 2185 , Loss 3.9855239391326904\n",
      "Batch 2186 , Loss 3.8832664489746094\n",
      "Batch 2187 , Loss 4.168492317199707\n",
      "Batch 2188 , Loss 3.863250732421875\n",
      "Batch 2189 , Loss 4.063364028930664\n",
      "Batch 2190 , Loss 3.969033718109131\n",
      "Batch 2191 , Loss 3.9326348304748535\n",
      "Batch 2192 , Loss 4.113467693328857\n",
      "Batch 2193 , Loss 4.036654949188232\n",
      "Batch 2194 , Loss 4.1077094078063965\n",
      "Batch 2195 , Loss 4.028507232666016\n",
      "Batch 2196 , Loss 3.8401527404785156\n",
      "Batch 2197 , Loss 3.9914073944091797\n",
      "Batch 2198 , Loss 4.038854598999023\n",
      "Batch 2199 , Loss 3.852198600769043\n",
      "Batch 2200 , Loss 4.1255083084106445\n",
      "Batch 2201 , Loss 4.141613960266113\n",
      "Batch 2202 , Loss 4.018034934997559\n",
      "Batch 2203 , Loss 3.9817631244659424\n",
      "Batch 2204 , Loss 3.963888645172119\n",
      "Batch 2205 , Loss 4.008351802825928\n",
      "Batch 2206 , Loss 4.019045829772949\n",
      "Batch 2207 , Loss 3.8867106437683105\n",
      "Batch 2208 , Loss 4.309556007385254\n",
      "Batch 2209 , Loss 4.0494842529296875\n",
      "Batch 2210 , Loss 3.9410157203674316\n",
      "Batch 2211 , Loss 3.9791100025177\n",
      "Batch 2212 , Loss 3.949521064758301\n",
      "Batch 2213 , Loss 4.234053134918213\n",
      "Batch 2214 , Loss 3.826455593109131\n",
      "Batch 2215 , Loss 3.9807069301605225\n",
      "Batch 2216 , Loss 3.7546629905700684\n",
      "Batch 2217 , Loss 3.837034225463867\n",
      "Batch 2218 , Loss 4.245601654052734\n",
      "Batch 2219 , Loss 4.230977535247803\n",
      "Batch 2220 , Loss 4.044559001922607\n",
      "Batch 2221 , Loss 3.8646888732910156\n",
      "Batch 2222 , Loss 3.9950432777404785\n",
      "Batch 2223 , Loss 3.9994444847106934\n",
      "Batch 2224 , Loss 3.985661745071411\n",
      "Batch 2225 , Loss 3.9852781295776367\n",
      "Batch 2226 , Loss 4.0109052658081055\n",
      "Batch 2227 , Loss 3.9666380882263184\n",
      "Batch 2228 , Loss 3.9295849800109863\n",
      "Batch 2229 , Loss 4.214286804199219\n",
      "Batch 2230 , Loss 3.7498536109924316\n",
      "Batch 2231 , Loss 3.9633588790893555\n",
      "Batch 2232 , Loss 4.061471462249756\n",
      "Batch 2233 , Loss 4.1869096755981445\n",
      "Batch 2234 , Loss 3.824751853942871\n",
      "Batch 2235 , Loss 3.758255958557129\n",
      "Batch 2236 , Loss 3.790022134780884\n",
      "Batch 2237 , Loss 4.15240478515625\n",
      "Batch 2238 , Loss 4.1614885330200195\n",
      "Batch 2239 , Loss 3.944150924682617\n",
      "Batch 2240 , Loss 3.986804485321045\n",
      "Batch 2241 , Loss 4.086808681488037\n",
      "Batch 2242 , Loss 3.9210472106933594\n",
      "Batch 2243 , Loss 4.217777252197266\n",
      "Batch 2244 , Loss 4.123493671417236\n",
      "Batch 2245 , Loss 3.9475934505462646\n",
      "Batch 2246 , Loss 3.87243390083313\n",
      "Batch 2247 , Loss 3.964066743850708\n",
      "Batch 2248 , Loss 4.069934844970703\n",
      "Batch 2249 , Loss 4.026030540466309\n",
      "Batch 2250 , Loss 4.1529364585876465\n",
      "Batch 2251 , Loss 4.110781192779541\n",
      "Batch 2252 , Loss 3.8354387283325195\n",
      "Batch 2253 , Loss 3.7981972694396973\n",
      "Batch 2254 , Loss 4.005491256713867\n",
      "Batch 2255 , Loss 3.973320960998535\n",
      "Batch 2256 , Loss 4.075843334197998\n",
      "Batch 2257 , Loss 3.8230443000793457\n",
      "Batch 2258 , Loss 4.106968879699707\n",
      "Batch 2259 , Loss 3.9506916999816895\n",
      "Batch 2260 , Loss 3.876382827758789\n",
      "Batch 2261 , Loss 4.154938697814941\n",
      "Batch 2262 , Loss 4.136487007141113\n",
      "Batch 2263 , Loss 4.039801597595215\n",
      "Batch 2264 , Loss 3.917172431945801\n",
      "Batch 2265 , Loss 4.1877241134643555\n",
      "Batch 2266 , Loss 4.150644302368164\n",
      "Batch 2267 , Loss 3.967525005340576\n",
      "Batch 2268 , Loss 4.130929470062256\n",
      "Batch 2269 , Loss 4.040716171264648\n",
      "Batch 2270 , Loss 3.807795524597168\n",
      "Batch 2271 , Loss 4.207634449005127\n",
      "Batch 2272 , Loss 4.084774494171143\n",
      "Batch 2273 , Loss 3.8743033409118652\n",
      "Batch 2274 , Loss 3.736628532409668\n",
      "Batch 2275 , Loss 4.158748626708984\n",
      "Batch 2276 , Loss 4.215129375457764\n",
      "Batch 2277 , Loss 3.8636322021484375\n",
      "Batch 2278 , Loss 3.867062568664551\n",
      "Batch 2279 , Loss 4.052835941314697\n",
      "Batch 2280 , Loss 4.183647155761719\n",
      "Batch 2281 , Loss 3.7697434425354004\n",
      "Batch 2282 , Loss 3.9086833000183105\n",
      "Batch 2283 , Loss 4.198489189147949\n",
      "Batch 2284 , Loss 4.303797721862793\n",
      "Batch 2285 , Loss 3.8128628730773926\n",
      "Batch 2286 , Loss 4.10042667388916\n",
      "Batch 2287 , Loss 4.104001998901367\n",
      "Batch 2288 , Loss 4.383615970611572\n",
      "Batch 2289 , Loss 3.864269256591797\n",
      "Batch 2290 , Loss 3.8641796112060547\n",
      "Batch 2291 , Loss 3.826422691345215\n",
      "Batch 2292 , Loss 4.081241607666016\n",
      "Batch 2293 , Loss 3.9150454998016357\n",
      "Batch 2294 , Loss 3.7090766429901123\n",
      "Batch 2295 , Loss 4.032449722290039\n",
      "Batch 2296 , Loss 3.75966215133667\n",
      "Batch 2297 , Loss 3.9140100479125977\n",
      "Batch 2298 , Loss 3.798096179962158\n",
      "Batch 2299 , Loss 4.058048725128174\n",
      "Batch 2300 , Loss 3.9724955558776855\n",
      "Batch 2301 , Loss 3.784247875213623\n",
      "Batch 2302 , Loss 4.077786445617676\n",
      "Batch 2303 , Loss 4.030059814453125\n",
      "Batch 2304 , Loss 3.876513957977295\n",
      "Batch 2305 , Loss 3.947542190551758\n",
      "Batch 2306 , Loss 4.214236736297607\n",
      "Batch 2307 , Loss 3.8627524375915527\n",
      "Batch 2308 , Loss 3.8351306915283203\n",
      "Batch 2309 , Loss 4.072020530700684\n",
      "Batch 2310 , Loss 3.95168399810791\n",
      "Batch 2311 , Loss 4.1597700119018555\n",
      "Batch 2312 , Loss 3.95723032951355\n",
      "Batch 2313 , Loss 3.914867401123047\n",
      "Batch 2314 , Loss 4.119626998901367\n",
      "Batch 2315 , Loss 4.14022159576416\n",
      "Batch 2316 , Loss 4.11251163482666\n",
      "Batch 2317 , Loss 3.938509225845337\n",
      "Batch 2318 , Loss 3.95536732673645\n",
      "Batch 2319 , Loss 3.718801498413086\n",
      "Batch 2320 , Loss 3.86367130279541\n",
      "Batch 2321 , Loss 3.9367666244506836\n",
      "Batch 2322 , Loss 3.8170619010925293\n",
      "Batch 2323 , Loss 4.228457450866699\n",
      "Batch 2324 , Loss 3.674645185470581\n",
      "Batch 2325 , Loss 3.8922228813171387\n",
      "Batch 2326 , Loss 3.903928518295288\n",
      "Batch 2327 , Loss 4.130738258361816\n",
      "Batch 2328 , Loss 3.9209470748901367\n",
      "Batch 2329 , Loss 4.153449535369873\n",
      "Batch 2330 , Loss 4.004278659820557\n",
      "Batch 2331 , Loss 3.8205668926239014\n",
      "Batch 2332 , Loss 4.0249528884887695\n",
      "Batch 2333 , Loss 4.334323883056641\n",
      "Batch 2334 , Loss 4.079257488250732\n",
      "Batch 2335 , Loss 4.160985946655273\n",
      "Batch 2336 , Loss 3.962674379348755\n",
      "Batch 2337 , Loss 3.8309073448181152\n",
      "Batch 2338 , Loss 3.9151241779327393\n",
      "Batch 2339 , Loss 3.869619846343994\n",
      "Batch 2340 , Loss 3.8805036544799805\n",
      "Batch 2341 , Loss 3.761303424835205\n",
      "Batch 2342 , Loss 3.779012680053711\n",
      "Batch 2343 , Loss 4.056168079376221\n",
      "Batch 2344 , Loss 3.9282333850860596\n",
      "Batch 2345 , Loss 3.7949976921081543\n",
      "Batch 2346 , Loss 4.042931079864502\n",
      "Batch 2347 , Loss 4.093591690063477\n",
      "Batch 2348 , Loss 4.073514938354492\n",
      "Batch 2349 , Loss 3.961587905883789\n",
      "Batch 2350 , Loss 3.808309555053711\n",
      "Batch 2351 , Loss 3.681729316711426\n",
      "Batch 2352 , Loss 4.213381290435791\n",
      "Batch 2353 , Loss 4.081499099731445\n",
      "Batch 2354 , Loss 4.067451477050781\n",
      "Batch 2355 , Loss 4.194708824157715\n",
      "Batch 2356 , Loss 3.833385705947876\n",
      "Batch 2357 , Loss 3.948974370956421\n",
      "Batch 2358 , Loss 4.04034948348999\n",
      "Batch 2359 , Loss 3.662679672241211\n",
      "Batch 2360 , Loss 4.004881858825684\n",
      "Batch 2361 , Loss 4.021368503570557\n",
      "Batch 2362 , Loss 3.9027600288391113\n",
      "Batch 2363 , Loss 3.7607436180114746\n",
      "Batch 2364 , Loss 4.024492263793945\n",
      "Batch 2365 , Loss 4.071291923522949\n",
      "Batch 2366 , Loss 3.535545825958252\n",
      "Batch 2367 , Loss 4.059149265289307\n",
      "Batch 2368 , Loss 4.01580810546875\n",
      "Batch 2369 , Loss 3.5863916873931885\n",
      "Batch 2370 , Loss 3.9218006134033203\n",
      "Batch 2371 , Loss 3.893430233001709\n",
      "Batch 2372 , Loss 4.163092613220215\n",
      "Batch 2373 , Loss 4.224147319793701\n",
      "Batch 2374 , Loss 3.960362672805786\n",
      "Batch 2375 , Loss 3.9787631034851074\n",
      "Batch 2376 , Loss 3.961905002593994\n",
      "Batch 2377 , Loss 3.8249552249908447\n",
      "Batch 2378 , Loss 4.150991439819336\n",
      "Batch 2379 , Loss 4.0325517654418945\n",
      "Batch 2380 , Loss 3.8332324028015137\n",
      "Batch 2381 , Loss 3.9128434658050537\n",
      "Batch 2382 , Loss 3.8542914390563965\n",
      "Batch 2383 , Loss 3.834656238555908\n",
      "Batch 2384 , Loss 3.9613900184631348\n",
      "Batch 2385 , Loss 3.996739625930786\n",
      "Batch 2386 , Loss 3.761122703552246\n",
      "Batch 2387 , Loss 4.131996154785156\n",
      "Batch 2388 , Loss 4.182411193847656\n",
      "Batch 2389 , Loss 3.89701247215271\n",
      "Batch 2390 , Loss 3.8182568550109863\n",
      "Batch 2391 , Loss 3.895646095275879\n",
      "Batch 2392 , Loss 3.8862786293029785\n",
      "Batch 2393 , Loss 4.302029132843018\n",
      "Batch 2394 , Loss 4.190945625305176\n",
      "Batch 2395 , Loss 3.7491073608398438\n",
      "Batch 2396 , Loss 3.9621829986572266\n",
      "Batch 2397 , Loss 4.003337383270264\n",
      "Batch 2398 , Loss 3.824828624725342\n",
      "Batch 2399 , Loss 4.211893081665039\n",
      "Batch 2400 , Loss 3.9714512825012207\n",
      "Batch 2401 , Loss 3.9616305828094482\n",
      "Batch 2402 , Loss 3.880884885787964\n",
      "Batch 2403 , Loss 3.8516087532043457\n",
      "Batch 2404 , Loss 3.8450469970703125\n",
      "Batch 2405 , Loss 3.9472391605377197\n",
      "Batch 2406 , Loss 4.0359649658203125\n",
      "Batch 2407 , Loss 4.080445289611816\n",
      "Batch 2408 , Loss 4.0209879875183105\n",
      "Batch 2409 , Loss 4.310947418212891\n",
      "Batch 2410 , Loss 4.0225043296813965\n",
      "Batch 2411 , Loss 3.6711440086364746\n",
      "Batch 2412 , Loss 3.9273204803466797\n",
      "Batch 2413 , Loss 4.238935947418213\n",
      "Batch 2414 , Loss 3.9792799949645996\n",
      "Batch 2415 , Loss 3.8096189498901367\n",
      "Batch 2416 , Loss 4.231130123138428\n",
      "Batch 2417 , Loss 3.795644760131836\n",
      "Batch 2418 , Loss 3.8240859508514404\n",
      "Batch 2419 , Loss 4.020147323608398\n",
      "Batch 2420 , Loss 4.002926826477051\n",
      "Batch 2421 , Loss 4.136369228363037\n",
      "Batch 2422 , Loss 4.106152534484863\n",
      "Batch 2423 , Loss 3.6424202919006348\n",
      "Batch 2424 , Loss 4.4131011962890625\n",
      "Batch 2425 , Loss 4.030081748962402\n",
      "Batch 2426 , Loss 3.9611494541168213\n",
      "Batch 2427 , Loss 4.009054183959961\n",
      "Batch 2428 , Loss 4.0150651931762695\n",
      "Batch 2429 , Loss 3.716930389404297\n",
      "Batch 2430 , Loss 3.8487958908081055\n",
      "Batch 2431 , Loss 4.0236968994140625\n",
      "Batch 2432 , Loss 3.781407594680786\n",
      "Batch 2433 , Loss 4.217430114746094\n",
      "Batch 2434 , Loss 3.801237106323242\n",
      "Batch 2435 , Loss 4.224723815917969\n",
      "Batch 2436 , Loss 4.070852279663086\n",
      "Batch 2437 , Loss 4.219876766204834\n",
      "Batch 2438 , Loss 4.0967326164245605\n",
      "Batch 2439 , Loss 4.132696151733398\n",
      "Batch 2440 , Loss 3.7317538261413574\n",
      "Batch 2441 , Loss 3.6729326248168945\n",
      "Batch 2442 , Loss 3.9899425506591797\n",
      "Batch 2443 , Loss 4.291007041931152\n",
      "Batch 2444 , Loss 3.645292043685913\n",
      "Batch 2445 , Loss 4.068553924560547\n",
      "Batch 2446 , Loss 4.004993915557861\n",
      "Batch 2447 , Loss 3.838930606842041\n",
      "Batch 2448 , Loss 4.077401638031006\n",
      "Batch 2449 , Loss 3.9384589195251465\n",
      "Batch 2450 , Loss 3.7853193283081055\n",
      "Batch 2451 , Loss 3.9131293296813965\n",
      "Batch 2452 , Loss 3.7510433197021484\n",
      "Batch 2453 , Loss 3.8292713165283203\n",
      "Batch 2454 , Loss 3.9120917320251465\n",
      "Batch 2455 , Loss 3.8275628089904785\n",
      "Batch 2456 , Loss 3.9113800525665283\n",
      "Batch 2457 , Loss 4.003737926483154\n",
      "Batch 2458 , Loss 4.18695068359375\n",
      "Batch 2459 , Loss 3.925811529159546\n",
      "Batch 2460 , Loss 3.8564953804016113\n",
      "Batch 2461 , Loss 3.9005796909332275\n",
      "Batch 2462 , Loss 3.7997007369995117\n",
      "Batch 2463 , Loss 4.099318981170654\n",
      "Batch 2464 , Loss 3.9956202507019043\n",
      "Batch 2465 , Loss 4.101439476013184\n",
      "Batch 2466 , Loss 4.04210090637207\n",
      "Batch 2467 , Loss 4.055966854095459\n",
      "Batch 2468 , Loss 3.7808327674865723\n",
      "Batch 2469 , Loss 3.8351430892944336\n",
      "Batch 2470 , Loss 4.245584487915039\n",
      "Batch 2471 , Loss 3.858335018157959\n",
      "Batch 2472 , Loss 3.9960570335388184\n",
      "Batch 2473 , Loss 4.020808219909668\n",
      "Batch 2474 , Loss 3.7757463455200195\n",
      "Batch 2475 , Loss 4.155974864959717\n",
      "Batch 2476 , Loss 4.185408592224121\n",
      "Batch 2477 , Loss 3.8636341094970703\n",
      "Batch 2478 , Loss 3.9509849548339844\n",
      "Batch 2479 , Loss 3.9496726989746094\n",
      "Batch 2480 , Loss 3.694575786590576\n",
      "Batch 2481 , Loss 3.820347309112549\n",
      "Batch 2482 , Loss 4.1720075607299805\n",
      "Batch 2483 , Loss 3.8951826095581055\n",
      "Batch 2484 , Loss 4.079577445983887\n",
      "Batch 2485 , Loss 4.111225128173828\n",
      "Batch 2486 , Loss 3.8744122982025146\n",
      "Batch 2487 , Loss 4.124935626983643\n",
      "Batch 2488 , Loss 3.688807964324951\n",
      "Batch 2489 , Loss 3.9853343963623047\n",
      "Batch 2490 , Loss 3.823436737060547\n",
      "Batch 2491 , Loss 4.03883695602417\n",
      "Batch 2492 , Loss 3.698460578918457\n",
      "Batch 2493 , Loss 3.992309093475342\n",
      "Batch 2494 , Loss 3.7966248989105225\n",
      "Batch 2495 , Loss 4.067662239074707\n",
      "Batch 2496 , Loss 3.9395360946655273\n",
      "Batch 2497 , Loss 3.9306726455688477\n",
      "Batch 2498 , Loss 3.9642817974090576\n",
      "Batch 2499 , Loss 4.179924488067627\n",
      "Batch 2500 , Loss 3.668642520904541\n",
      "Batch 2501 , Loss 3.8251399993896484\n",
      "Batch 2502 , Loss 4.051953315734863\n",
      "Batch 2503 , Loss 4.099827766418457\n",
      "Batch 2504 , Loss 3.9034366607666016\n",
      "Batch 2505 , Loss 3.5812180042266846\n",
      "Batch 2506 , Loss 3.8069653511047363\n",
      "Batch 2507 , Loss 4.023931503295898\n",
      "Batch 2508 , Loss 3.974331855773926\n",
      "Batch 2509 , Loss 4.0240254402160645\n",
      "Batch 2510 , Loss 4.1778059005737305\n",
      "Batch 2511 , Loss 3.7714409828186035\n",
      "Batch 2512 , Loss 3.795710802078247\n",
      "Batch 2513 , Loss 3.7968428134918213\n",
      "Batch 2514 , Loss 3.9143693447113037\n",
      "Batch 2515 , Loss 3.816514730453491\n",
      "Batch 2516 , Loss 3.9471263885498047\n",
      "Batch 2517 , Loss 4.021218776702881\n",
      "Batch 2518 , Loss 4.259998321533203\n",
      "Batch 2519 , Loss 4.07084321975708\n",
      "Batch 2520 , Loss 3.8760080337524414\n",
      "Batch 2521 , Loss 3.821413516998291\n",
      "Batch 2522 , Loss 3.8581342697143555\n",
      "Batch 2523 , Loss 4.149662971496582\n",
      "Batch 2524 , Loss 3.804266929626465\n",
      "Batch 2525 , Loss 3.659587860107422\n",
      "Batch 2526 , Loss 3.9203567504882812\n",
      "Batch 2527 , Loss 3.760814666748047\n",
      "Batch 2528 , Loss 3.835808038711548\n",
      "Batch 2529 , Loss 3.9229910373687744\n",
      "Batch 2530 , Loss 3.992177963256836\n",
      "Batch 2531 , Loss 4.026356220245361\n",
      "Batch 2532 , Loss 4.056741714477539\n",
      "Batch 2533 , Loss 3.981304407119751\n",
      "Batch 2534 , Loss 4.03154182434082\n",
      "Batch 2535 , Loss 3.964704751968384\n",
      "Batch 2536 , Loss 3.8437461853027344\n",
      "Batch 2537 , Loss 4.18557596206665\n",
      "Batch 2538 , Loss 4.004647731781006\n",
      "Batch 2539 , Loss 4.071157455444336\n",
      "Batch 2540 , Loss 3.856555700302124\n",
      "Batch 2541 , Loss 4.1252827644348145\n",
      "Batch 2542 , Loss 3.8795876502990723\n",
      "Batch 2543 , Loss 4.020470142364502\n",
      "Batch 2544 , Loss 3.8144772052764893\n",
      "Batch 2545 , Loss 3.759084701538086\n",
      "Batch 2546 , Loss 3.750422477722168\n",
      "Batch 2547 , Loss 3.7282655239105225\n",
      "Batch 2548 , Loss 3.937649726867676\n",
      "Batch 2549 , Loss 3.687302350997925\n",
      "Batch 2550 , Loss 4.142409324645996\n",
      "Batch 2551 , Loss 4.138242721557617\n",
      "Batch 2552 , Loss 3.7935261726379395\n",
      "Batch 2553 , Loss 3.8610353469848633\n",
      "Batch 2554 , Loss 3.7024598121643066\n",
      "Batch 2555 , Loss 3.9317362308502197\n",
      "Batch 2556 , Loss 3.9584879875183105\n",
      "Batch 2557 , Loss 3.81040358543396\n",
      "Batch 2558 , Loss 3.712526798248291\n",
      "Batch 2559 , Loss 3.7624783515930176\n",
      "Batch 2560 , Loss 3.8657498359680176\n",
      "Batch 2561 , Loss 4.204171657562256\n",
      "Batch 2562 , Loss 4.0004377365112305\n",
      "Batch 2563 , Loss 3.936363697052002\n",
      "Batch 2564 , Loss 4.022156715393066\n",
      "Batch 2565 , Loss 4.039341926574707\n",
      "Batch 2566 , Loss 4.021480083465576\n",
      "Batch 2567 , Loss 3.888963222503662\n",
      "Batch 2568 , Loss 3.8094325065612793\n",
      "Batch 2569 , Loss 3.829771041870117\n",
      "Batch 2570 , Loss 3.8095502853393555\n",
      "Batch 2571 , Loss 4.344174385070801\n",
      "Batch 2572 , Loss 3.8288626670837402\n",
      "Batch 2573 , Loss 3.868474006652832\n",
      "Batch 2574 , Loss 3.7988035678863525\n",
      "Batch 2575 , Loss 3.8531064987182617\n",
      "Batch 2576 , Loss 4.08442497253418\n",
      "Batch 2577 , Loss 3.895951986312866\n",
      "Batch 2578 , Loss 4.113381385803223\n",
      "Batch 2579 , Loss 3.8842015266418457\n",
      "Batch 2580 , Loss 3.8901281356811523\n",
      "Batch 2581 , Loss 4.165818214416504\n",
      "Batch 2582 , Loss 4.095169544219971\n",
      "Batch 2583 , Loss 3.924832344055176\n",
      "Batch 2584 , Loss 4.216535568237305\n",
      "Batch 2585 , Loss 4.00886344909668\n",
      "Batch 2586 , Loss 3.998528480529785\n",
      "Batch 2587 , Loss 4.006720066070557\n",
      "Batch 2588 , Loss 4.018456935882568\n",
      "Batch 2589 , Loss 3.980801582336426\n",
      "Batch 2590 , Loss 3.905592679977417\n",
      "Batch 2591 , Loss 3.9257917404174805\n",
      "Batch 2592 , Loss 3.92537522315979\n",
      "Batch 2593 , Loss 3.8166184425354004\n",
      "Batch 2594 , Loss 4.215070724487305\n",
      "Batch 2595 , Loss 3.8680191040039062\n",
      "Batch 2596 , Loss 4.112854480743408\n",
      "Batch 2597 , Loss 4.015050888061523\n",
      "Batch 2598 , Loss 3.754117965698242\n",
      "Batch 2599 , Loss 3.9390344619750977\n",
      "Batch 2600 , Loss 3.801589012145996\n",
      "Batch 2601 , Loss 3.5217700004577637\n",
      "Batch 2602 , Loss 3.958472490310669\n",
      "Batch 2603 , Loss 3.8297393321990967\n",
      "Batch 2604 , Loss 4.026702880859375\n",
      "Batch 2605 , Loss 4.029483795166016\n",
      "Batch 2606 , Loss 4.259871959686279\n",
      "Batch 2607 , Loss 3.9701523780822754\n",
      "Batch 2608 , Loss 3.714510202407837\n",
      "Batch 2609 , Loss 3.9690823554992676\n",
      "Batch 2610 , Loss 4.160881042480469\n",
      "Batch 2611 , Loss 3.905208110809326\n",
      "Batch 2612 , Loss 3.8657002449035645\n",
      "Batch 2613 , Loss 4.401566982269287\n",
      "Batch 2614 , Loss 3.973099708557129\n",
      "Batch 2615 , Loss 3.8317668437957764\n",
      "Batch 2616 , Loss 3.5985474586486816\n",
      "Batch 2617 , Loss 4.166162490844727\n",
      "Batch 2618 , Loss 3.870220422744751\n",
      "Batch 2619 , Loss 3.862635374069214\n",
      "Batch 2620 , Loss 3.8466763496398926\n",
      "Batch 2621 , Loss 4.012724876403809\n",
      "Batch 2622 , Loss 3.9190006256103516\n",
      "Batch 2623 , Loss 4.124667644500732\n",
      "Batch 2624 , Loss 3.9268970489501953\n",
      "Batch 2625 , Loss 3.9009523391723633\n",
      "Batch 2626 , Loss 3.7123770713806152\n",
      "Batch 2627 , Loss 3.588064432144165\n",
      "Batch 2628 , Loss 3.898451089859009\n",
      "Batch 2629 , Loss 3.6964571475982666\n",
      "Batch 2630 , Loss 3.5954337120056152\n",
      "Batch 2631 , Loss 3.851522445678711\n",
      "Batch 2632 , Loss 3.9717695713043213\n",
      "Batch 2633 , Loss 3.7059011459350586\n",
      "Batch 2634 , Loss 3.849536180496216\n",
      "Batch 2635 , Loss 3.6820335388183594\n",
      "Batch 2636 , Loss 4.310394287109375\n",
      "Batch 2637 , Loss 3.860745429992676\n",
      "Batch 2638 , Loss 3.760164737701416\n",
      "Batch 2639 , Loss 4.230184555053711\n",
      "Batch 2640 , Loss 3.8316564559936523\n",
      "Batch 2641 , Loss 3.8832309246063232\n",
      "Batch 2642 , Loss 3.952118396759033\n",
      "Batch 2643 , Loss 3.906576156616211\n",
      "Batch 2644 , Loss 4.256645202636719\n",
      "Batch 2645 , Loss 3.7741923332214355\n",
      "Batch 2646 , Loss 3.7728705406188965\n",
      "Batch 2647 , Loss 3.6143641471862793\n",
      "Batch 2648 , Loss 3.921308994293213\n",
      "Batch 2649 , Loss 3.9895012378692627\n",
      "Batch 2650 , Loss 3.726623773574829\n",
      "Batch 2651 , Loss 3.584371566772461\n",
      "Batch 2652 , Loss 4.064755439758301\n",
      "Batch 2653 , Loss 3.716034412384033\n",
      "Batch 2654 , Loss 3.9570600986480713\n",
      "Batch 2655 , Loss 3.850313901901245\n",
      "Batch 2656 , Loss 3.9608983993530273\n",
      "Batch 2657 , Loss 3.862389087677002\n",
      "Batch 2658 , Loss 3.99721622467041\n",
      "Batch 2659 , Loss 3.8013992309570312\n",
      "Batch 2660 , Loss 3.6656219959259033\n",
      "Batch 2661 , Loss 3.674625873565674\n",
      "Batch 2662 , Loss 3.598175525665283\n",
      "Batch 2663 , Loss 3.697381019592285\n",
      "Batch 2664 , Loss 3.770291566848755\n",
      "Batch 2665 , Loss 4.043031215667725\n",
      "Batch 2666 , Loss 3.941892147064209\n",
      "Batch 2667 , Loss 3.9532272815704346\n",
      "Batch 2668 , Loss 3.8107662200927734\n",
      "Batch 2669 , Loss 4.049596786499023\n",
      "Batch 2670 , Loss 4.105628967285156\n",
      "Batch 2671 , Loss 3.8549582958221436\n",
      "Batch 2672 , Loss 4.231039524078369\n",
      "Batch 2673 , Loss 3.8243026733398438\n",
      "Batch 2674 , Loss 4.017557144165039\n",
      "Batch 2675 , Loss 3.847454309463501\n",
      "Batch 2676 , Loss 3.6577770709991455\n",
      "Batch 2677 , Loss 3.661508560180664\n",
      "Batch 2678 , Loss 4.117491245269775\n",
      "Batch 2679 , Loss 3.769674301147461\n",
      "Batch 2680 , Loss 4.213547706604004\n",
      "Batch 2681 , Loss 3.8409807682037354\n",
      "Batch 2682 , Loss 3.8062121868133545\n",
      "Batch 2683 , Loss 3.906449556350708\n",
      "Batch 2684 , Loss 4.0781097412109375\n",
      "Batch 2685 , Loss 3.9360861778259277\n",
      "Batch 2686 , Loss 3.9528393745422363\n",
      "Batch 2687 , Loss 4.058012962341309\n",
      "Batch 2688 , Loss 4.084111213684082\n",
      "Batch 2689 , Loss 3.702775001525879\n",
      "Batch 2690 , Loss 3.8815078735351562\n",
      "Batch 2691 , Loss 4.001957416534424\n",
      "Batch 2692 , Loss 3.7351980209350586\n",
      "Batch 2693 , Loss 3.8756422996520996\n",
      "Batch 2694 , Loss 3.958275079727173\n",
      "Batch 2695 , Loss 3.8779945373535156\n",
      "Batch 2696 , Loss 3.7347359657287598\n",
      "Batch 2697 , Loss 3.8465375900268555\n",
      "Batch 2698 , Loss 3.9629435539245605\n",
      "Batch 2699 , Loss 4.137737274169922\n",
      "Batch 2700 , Loss 4.096828937530518\n",
      "Batch 2701 , Loss 3.9034221172332764\n",
      "Batch 2702 , Loss 3.801863431930542\n",
      "Batch 2703 , Loss 3.864595413208008\n",
      "Batch 2704 , Loss 3.6925177574157715\n",
      "Batch 2705 , Loss 3.97011661529541\n",
      "Batch 2706 , Loss 3.9666433334350586\n",
      "Batch 2707 , Loss 3.9685893058776855\n",
      "Batch 2708 , Loss 3.9190847873687744\n",
      "Batch 2709 , Loss 3.7934787273406982\n",
      "Batch 2710 , Loss 3.963116407394409\n",
      "Batch 2711 , Loss 3.8296804428100586\n",
      "Batch 2712 , Loss 3.8171606063842773\n",
      "Batch 2713 , Loss 3.931060791015625\n",
      "Batch 2714 , Loss 3.556224822998047\n",
      "Batch 2715 , Loss 4.143160820007324\n",
      "Batch 2716 , Loss 3.873234510421753\n",
      "Batch 2717 , Loss 3.9580078125\n",
      "Batch 2718 , Loss 4.122272968292236\n",
      "Batch 2719 , Loss 4.000683307647705\n",
      "Batch 2720 , Loss 4.045620918273926\n",
      "Batch 2721 , Loss 3.953714370727539\n",
      "Batch 2722 , Loss 3.8896031379699707\n",
      "Batch 2723 , Loss 4.018457412719727\n",
      "Batch 2724 , Loss 3.6885323524475098\n",
      "Batch 2725 , Loss 4.002971649169922\n",
      "Batch 2726 , Loss 4.009227275848389\n",
      "Batch 2727 , Loss 3.8205385208129883\n",
      "Batch 2728 , Loss 4.216025352478027\n",
      "Batch 2729 , Loss 3.8341357707977295\n",
      "Batch 2730 , Loss 3.780378818511963\n",
      "Batch 2731 , Loss 3.821279525756836\n",
      "Batch 2732 , Loss 4.063439846038818\n",
      "Batch 2733 , Loss 4.109071731567383\n",
      "Batch 2734 , Loss 3.8815174102783203\n",
      "Batch 2735 , Loss 4.0084662437438965\n",
      "Batch 2736 , Loss 3.839236259460449\n",
      "Batch 2737 , Loss 3.927884101867676\n",
      "Batch 2738 , Loss 4.040048122406006\n",
      "Batch 2739 , Loss 3.8879971504211426\n",
      "Batch 2740 , Loss 3.948500871658325\n",
      "Batch 2741 , Loss 4.111537933349609\n",
      "Batch 2742 , Loss 3.9653990268707275\n",
      "Batch 2743 , Loss 4.041122913360596\n",
      "Batch 2744 , Loss 4.000746726989746\n",
      "Batch 2745 , Loss 3.988339900970459\n",
      "Batch 2746 , Loss 4.179671764373779\n",
      "Batch 2747 , Loss 4.008723258972168\n",
      "Batch 2748 , Loss 4.312689781188965\n",
      "Batch 2749 , Loss 3.824868679046631\n",
      "Batch 2750 , Loss 3.928170680999756\n",
      "Batch 2751 , Loss 3.898636817932129\n",
      "Batch 2752 , Loss 3.8088932037353516\n",
      "Batch 2753 , Loss 3.8387112617492676\n",
      "Batch 2754 , Loss 4.00920295715332\n",
      "Batch 2755 , Loss 3.897535562515259\n",
      "Batch 2756 , Loss 3.687213897705078\n",
      "Batch 2757 , Loss 3.6489691734313965\n",
      "Batch 2758 , Loss 4.040780544281006\n",
      "Batch 2759 , Loss 3.9183340072631836\n",
      "Batch 2760 , Loss 3.9190430641174316\n",
      "Batch 2761 , Loss 3.9905245304107666\n",
      "Batch 2762 , Loss 3.8222503662109375\n",
      "Batch 2763 , Loss 3.739558696746826\n",
      "Batch 2764 , Loss 3.801661968231201\n",
      "Batch 2765 , Loss 4.017195224761963\n",
      "Batch 2766 , Loss 4.1155781745910645\n",
      "Batch 2767 , Loss 3.8580760955810547\n",
      "Batch 2768 , Loss 3.727698802947998\n",
      "Batch 2769 , Loss 3.817610740661621\n",
      "Batch 2770 , Loss 3.756930351257324\n",
      "Batch 2771 , Loss 3.9205470085144043\n",
      "Batch 2772 , Loss 3.9629130363464355\n",
      "Batch 2773 , Loss 3.80515193939209\n",
      "Batch 2774 , Loss 3.650458812713623\n",
      "Batch 2775 , Loss 3.874099016189575\n",
      "Batch 2776 , Loss 3.8248844146728516\n",
      "Batch 2777 , Loss 3.6946208477020264\n",
      "Batch 2778 , Loss 3.890540599822998\n",
      "Batch 2779 , Loss 4.056931495666504\n",
      "Batch 2780 , Loss 3.9385766983032227\n",
      "Batch 2781 , Loss 3.663850784301758\n",
      "Batch 2782 , Loss 3.766529083251953\n",
      "Batch 2783 , Loss 3.8407604694366455\n",
      "Batch 2784 , Loss 3.7610011100769043\n",
      "Batch 2785 , Loss 3.9179539680480957\n",
      "Batch 2786 , Loss 4.045886516571045\n",
      "Batch 2787 , Loss 4.144123077392578\n",
      "Batch 2788 , Loss 3.937448740005493\n",
      "Batch 2789 , Loss 3.9554080963134766\n",
      "Batch 2790 , Loss 3.8186357021331787\n",
      "Batch 2791 , Loss 3.8830292224884033\n",
      "Batch 2792 , Loss 4.21589469909668\n",
      "Batch 2793 , Loss 4.161794185638428\n",
      "Batch 2794 , Loss 3.8089518547058105\n",
      "Batch 2795 , Loss 3.9563539028167725\n",
      "Batch 2796 , Loss 3.84987211227417\n",
      "Batch 2797 , Loss 3.816544532775879\n",
      "Batch 2798 , Loss 4.017650604248047\n",
      "Batch 2799 , Loss 3.788928508758545\n",
      "Batch 2800 , Loss 3.83270263671875\n",
      "Batch 2801 , Loss 3.926952838897705\n",
      "Batch 2802 , Loss 3.884122371673584\n",
      "Batch 2803 , Loss 4.221657752990723\n",
      "Batch 2804 , Loss 3.891075611114502\n",
      "Batch 2805 , Loss 3.612302303314209\n",
      "Batch 2806 , Loss 3.708281993865967\n",
      "Batch 2807 , Loss 3.9486751556396484\n",
      "Batch 2808 , Loss 3.6391425132751465\n",
      "Batch 2809 , Loss 3.641657829284668\n",
      "Batch 2810 , Loss 3.846937417984009\n",
      "Batch 2811 , Loss 3.658578395843506\n",
      "Batch 2812 , Loss 3.687211513519287\n",
      "Batch 2813 , Loss 4.036004543304443\n",
      "Batch 2814 , Loss 3.6145615577697754\n",
      "Batch 2815 , Loss 3.847118377685547\n",
      "Batch 2816 , Loss 3.879591941833496\n",
      "Batch 2817 , Loss 3.8112690448760986\n",
      "Batch 2818 , Loss 3.908749580383301\n",
      "Batch 2819 , Loss 4.316981315612793\n",
      "Batch 2820 , Loss 3.837268352508545\n",
      "Batch 2821 , Loss 3.7027454376220703\n",
      "Batch 2822 , Loss 3.944687604904175\n",
      "Batch 2823 , Loss 3.6450610160827637\n",
      "Batch 2824 , Loss 3.7158844470977783\n",
      "Batch 2825 , Loss 3.7275428771972656\n",
      "Batch 2826 , Loss 3.6836323738098145\n",
      "Batch 2827 , Loss 3.5748114585876465\n",
      "Batch 2828 , Loss 3.8303112983703613\n",
      "Batch 2829 , Loss 4.034067630767822\n",
      "Batch 2830 , Loss 4.37074089050293\n",
      "Batch 2831 , Loss 3.8892951011657715\n",
      "Batch 2832 , Loss 4.017549991607666\n",
      "Batch 2833 , Loss 3.960394859313965\n",
      "Batch 2834 , Loss 4.116621971130371\n",
      "Batch 2835 , Loss 3.629391670227051\n",
      "Batch 2836 , Loss 3.7321577072143555\n",
      "Batch 2837 , Loss 3.813037395477295\n",
      "Batch 2838 , Loss 3.8859739303588867\n",
      "Batch 2839 , Loss 3.773885726928711\n",
      "Batch 2840 , Loss 3.9404752254486084\n",
      "Batch 2841 , Loss 3.881269693374634\n",
      "Batch 2842 , Loss 3.939851999282837\n",
      "Batch 2843 , Loss 3.832402229309082\n",
      "Batch 2844 , Loss 3.5039095878601074\n",
      "Batch 2845 , Loss 3.980628490447998\n",
      "Batch 2846 , Loss 3.948719024658203\n",
      "Batch 2847 , Loss 3.8934226036071777\n",
      "Batch 2848 , Loss 4.05836820602417\n",
      "Batch 2849 , Loss 4.127220153808594\n",
      "Batch 2850 , Loss 4.059845447540283\n",
      "Batch 2851 , Loss 3.922537326812744\n",
      "Batch 2852 , Loss 3.846224308013916\n",
      "Batch 2853 , Loss 3.7433524131774902\n",
      "Batch 2854 , Loss 3.6479992866516113\n",
      "Batch 2855 , Loss 3.936854362487793\n",
      "Batch 2856 , Loss 3.775705337524414\n",
      "Batch 2857 , Loss 3.903110980987549\n",
      "Batch 2858 , Loss 3.8650968074798584\n",
      "Batch 2859 , Loss 4.156103134155273\n",
      "Batch 2860 , Loss 3.7344417572021484\n",
      "Batch 2861 , Loss 3.7829723358154297\n",
      "Batch 2862 , Loss 3.9446990489959717\n",
      "Batch 2863 , Loss 3.80354642868042\n",
      "Batch 2864 , Loss 4.0134429931640625\n",
      "Batch 2865 , Loss 3.8332629203796387\n",
      "Batch 2866 , Loss 4.001608848571777\n",
      "Batch 2867 , Loss 3.7076616287231445\n",
      "Batch 2868 , Loss 3.979461669921875\n",
      "Batch 2869 , Loss 3.8189992904663086\n",
      "Batch 2870 , Loss 3.954188346862793\n",
      "Batch 2871 , Loss 3.842796802520752\n",
      "Batch 2872 , Loss 3.9202260971069336\n",
      "Batch 2873 , Loss 3.931734323501587\n",
      "Batch 2874 , Loss 3.6833434104919434\n",
      "Batch 2875 , Loss 4.21720027923584\n",
      "Batch 2876 , Loss 3.8319430351257324\n",
      "Batch 2877 , Loss 3.8017427921295166\n",
      "Batch 2878 , Loss 4.067529201507568\n",
      "Batch 2879 , Loss 4.007809638977051\n",
      "Batch 2880 , Loss 3.8311614990234375\n",
      "Batch 2881 , Loss 3.678675651550293\n",
      "Batch 2882 , Loss 3.8708677291870117\n",
      "Batch 2883 , Loss 3.8321738243103027\n",
      "Batch 2884 , Loss 3.9344093799591064\n",
      "Batch 2885 , Loss 4.021814346313477\n",
      "Batch 2886 , Loss 3.9162039756774902\n",
      "Batch 2887 , Loss 4.1766510009765625\n",
      "Batch 2888 , Loss 3.7987170219421387\n",
      "Batch 2889 , Loss 3.99160099029541\n",
      "Batch 2890 , Loss 3.763571262359619\n",
      "Batch 2891 , Loss 3.8004026412963867\n",
      "Batch 2892 , Loss 4.0996246337890625\n",
      "Batch 2893 , Loss 3.87738299369812\n",
      "Batch 2894 , Loss 3.6826844215393066\n",
      "Batch 2895 , Loss 3.859766721725464\n",
      "Batch 2896 , Loss 4.046377658843994\n",
      "Batch 2897 , Loss 4.040895462036133\n",
      "Batch 2898 , Loss 3.9330577850341797\n",
      "Batch 2899 , Loss 4.054287433624268\n",
      "Batch 2900 , Loss 4.126352310180664\n",
      "Batch 2901 , Loss 3.9262332916259766\n",
      "Batch 2902 , Loss 3.8434081077575684\n",
      "Batch 2903 , Loss 3.66510009765625\n",
      "Batch 2904 , Loss 4.0299272537231445\n",
      "Batch 2905 , Loss 3.7432351112365723\n",
      "Batch 2906 , Loss 4.123205184936523\n",
      "Batch 2907 , Loss 3.773050308227539\n",
      "Batch 2908 , Loss 4.162041187286377\n",
      "Batch 2909 , Loss 3.9006800651550293\n",
      "Batch 2910 , Loss 3.95782470703125\n",
      "Batch 2911 , Loss 3.887620687484741\n",
      "Batch 2912 , Loss 3.8919315338134766\n",
      "Batch 2913 , Loss 3.7561354637145996\n",
      "Batch 2914 , Loss 3.7570507526397705\n",
      "Batch 2915 , Loss 3.981447219848633\n",
      "Batch 2916 , Loss 3.8150603771209717\n",
      "Batch 2917 , Loss 4.032731056213379\n",
      "Batch 2918 , Loss 4.076890468597412\n",
      "Batch 2919 , Loss 3.60233211517334\n",
      "Batch 2920 , Loss 4.067523002624512\n",
      "Batch 2921 , Loss 4.014970302581787\n",
      "Batch 2922 , Loss 3.9064087867736816\n",
      "Batch 2923 , Loss 3.8797788619995117\n",
      "Batch 2924 , Loss 3.6771187782287598\n",
      "Batch 2925 , Loss 3.8943281173706055\n",
      "Batch 2926 , Loss 3.9009838104248047\n",
      "Batch 2927 , Loss 3.8133544921875\n",
      "Batch 2928 , Loss 4.011378288269043\n",
      "Batch 2929 , Loss 3.975724458694458\n",
      "Batch 2930 , Loss 3.7943568229675293\n",
      "Batch 2931 , Loss 4.070378303527832\n",
      "Batch 2932 , Loss 3.95668888092041\n",
      "Batch 2933 , Loss 3.7150399684906006\n",
      "Batch 2934 , Loss 3.9066338539123535\n",
      "Batch 2935 , Loss 3.639169454574585\n",
      "Batch 2936 , Loss 3.5201430320739746\n",
      "Batch 2937 , Loss 3.8204939365386963\n",
      "Batch 2938 , Loss 3.971187114715576\n",
      "Batch 2939 , Loss 3.837383985519409\n",
      "Batch 2940 , Loss 3.7956104278564453\n",
      "Batch 2941 , Loss 3.77683687210083\n",
      "Batch 2942 , Loss 4.005786895751953\n",
      "Batch 2943 , Loss 3.9956259727478027\n",
      "Batch 2944 , Loss 3.8108417987823486\n",
      "Batch 2945 , Loss 3.901743173599243\n",
      "Batch 2946 , Loss 3.803532123565674\n",
      "Batch 2947 , Loss 4.009758949279785\n",
      "Batch 2948 , Loss 3.6219534873962402\n",
      "Batch 2949 , Loss 3.748972177505493\n",
      "Batch 2950 , Loss 3.7350516319274902\n",
      "Batch 2951 , Loss 3.857314109802246\n",
      "Batch 2952 , Loss 3.840510368347168\n",
      "Batch 2953 , Loss 3.818711280822754\n",
      "Batch 2954 , Loss 3.8390324115753174\n",
      "Batch 2955 , Loss 4.066673755645752\n",
      "Batch 2956 , Loss 3.6488847732543945\n",
      "Batch 2957 , Loss 3.6992337703704834\n",
      "Batch 2958 , Loss 3.960747241973877\n",
      "Batch 2959 , Loss 3.8062143325805664\n",
      "Batch 2960 , Loss 4.016359329223633\n",
      "Batch 2961 , Loss 3.9237446784973145\n",
      "Batch 2962 , Loss 3.764193534851074\n",
      "Batch 2963 , Loss 3.5245203971862793\n",
      "Batch 2964 , Loss 3.7765984535217285\n",
      "Batch 2965 , Loss 3.8370790481567383\n",
      "Batch 2966 , Loss 3.8170032501220703\n",
      "Batch 2967 , Loss 3.8874266147613525\n",
      "Batch 2968 , Loss 3.7680630683898926\n",
      "Batch 2969 , Loss 3.6400997638702393\n",
      "Batch 2970 , Loss 4.061519145965576\n",
      "Batch 2971 , Loss 3.659878730773926\n",
      "Batch 2972 , Loss 3.7357208728790283\n",
      "Batch 2973 , Loss 3.9752092361450195\n",
      "Batch 2974 , Loss 3.8060965538024902\n",
      "Batch 2975 , Loss 3.781317710876465\n",
      "Batch 2976 , Loss 3.6663801670074463\n",
      "Batch 2977 , Loss 3.698150157928467\n",
      "Batch 2978 , Loss 3.8095264434814453\n",
      "Batch 2979 , Loss 3.886444568634033\n",
      "Batch 2980 , Loss 3.994379997253418\n",
      "Batch 2981 , Loss 4.084535121917725\n",
      "Batch 2982 , Loss 3.8802669048309326\n",
      "Batch 2983 , Loss 3.7805237770080566\n",
      "Batch 2984 , Loss 3.849670648574829\n",
      "Batch 2985 , Loss 3.8588080406188965\n",
      "Batch 2986 , Loss 4.07929801940918\n",
      "Batch 2987 , Loss 3.81533145904541\n",
      "Batch 2988 , Loss 3.82582950592041\n",
      "Batch 2989 , Loss 3.7885050773620605\n",
      "Batch 2990 , Loss 3.828615188598633\n",
      "Batch 2991 , Loss 3.669626474380493\n",
      "Batch 2992 , Loss 4.034611225128174\n",
      "Batch 2993 , Loss 3.944777488708496\n",
      "Batch 2994 , Loss 3.5180227756500244\n",
      "Batch 2995 , Loss 3.903268337249756\n",
      "Batch 2996 , Loss 3.8772966861724854\n",
      "Batch 2997 , Loss 3.8579325675964355\n",
      "Batch 2998 , Loss 3.8519339561462402\n",
      "Batch 2999 , Loss 3.8187267780303955\n",
      "Batch 3000 , Loss 3.8977622985839844\n",
      "Batch 3001 , Loss 3.828077793121338\n",
      "Batch 3002 , Loss 3.908864974975586\n",
      "Batch 3003 , Loss 3.824850559234619\n",
      "Batch 3004 , Loss 3.5682733058929443\n",
      "Batch 3005 , Loss 3.694042205810547\n",
      "Batch 3006 , Loss 3.6945343017578125\n",
      "Batch 3007 , Loss 3.723055839538574\n",
      "Batch 3008 , Loss 3.861611843109131\n",
      "Batch 3009 , Loss 3.821157217025757\n",
      "Batch 3010 , Loss 3.724787712097168\n",
      "Batch 3011 , Loss 3.552574396133423\n",
      "Batch 3012 , Loss 3.7549028396606445\n",
      "Batch 3013 , Loss 3.887439012527466\n",
      "Batch 3014 , Loss 3.8754193782806396\n",
      "Batch 3015 , Loss 3.746079444885254\n",
      "Batch 3016 , Loss 3.776327610015869\n",
      "Batch 3017 , Loss 3.7646002769470215\n",
      "Batch 3018 , Loss 3.8026132583618164\n",
      "Batch 3019 , Loss 3.7944869995117188\n",
      "Batch 3020 , Loss 3.675065279006958\n",
      "Batch 3021 , Loss 3.7156896591186523\n",
      "Batch 3022 , Loss 3.433696746826172\n",
      "Batch 3023 , Loss 3.6927242279052734\n",
      "Batch 3024 , Loss 3.6700239181518555\n",
      "Batch 3025 , Loss 3.8149967193603516\n",
      "Batch 3026 , Loss 3.993417501449585\n",
      "Batch 3027 , Loss 4.040565490722656\n",
      "Batch 3028 , Loss 3.949176549911499\n",
      "Batch 3029 , Loss 4.132603168487549\n",
      "Batch 3030 , Loss 3.9595065116882324\n",
      "Batch 3031 , Loss 3.934215545654297\n",
      "Batch 3032 , Loss 3.92808198928833\n",
      "Batch 3033 , Loss 3.757908582687378\n",
      "Batch 3034 , Loss 3.830582618713379\n",
      "Batch 3035 , Loss 3.770732879638672\n",
      "Batch 3036 , Loss 4.0853590965271\n",
      "Batch 3037 , Loss 3.7660484313964844\n",
      "Batch 3038 , Loss 3.6924898624420166\n",
      "Batch 3039 , Loss 3.9079222679138184\n",
      "Batch 3040 , Loss 3.9184279441833496\n",
      "Batch 3041 , Loss 4.044900894165039\n",
      "Batch 3042 , Loss 3.8986308574676514\n",
      "Batch 3043 , Loss 3.8831474781036377\n",
      "Batch 3044 , Loss 3.9594337940216064\n",
      "Batch 3045 , Loss 3.915820360183716\n",
      "Batch 3046 , Loss 3.6362082958221436\n",
      "Batch 3047 , Loss 3.895155906677246\n",
      "Batch 3048 , Loss 4.0341691970825195\n",
      "Batch 3049 , Loss 3.7581868171691895\n",
      "Batch 3050 , Loss 3.7500314712524414\n",
      "Batch 3051 , Loss 3.928394317626953\n",
      "Batch 3052 , Loss 3.8465137481689453\n",
      "Batch 3053 , Loss 3.962617874145508\n",
      "Batch 3054 , Loss 3.9581170082092285\n",
      "Batch 3055 , Loss 3.5841729640960693\n",
      "Batch 3056 , Loss 3.7403907775878906\n",
      "Batch 3057 , Loss 3.828011989593506\n",
      "Batch 3058 , Loss 3.741560459136963\n",
      "Batch 3059 , Loss 4.101248264312744\n",
      "Batch 3060 , Loss 3.5975375175476074\n",
      "Batch 3061 , Loss 3.8538010120391846\n",
      "Batch 3062 , Loss 3.888981342315674\n",
      "Batch 3063 , Loss 3.8580384254455566\n",
      "Batch 3064 , Loss 3.6913161277770996\n",
      "Batch 3065 , Loss 3.890165090560913\n",
      "Batch 3066 , Loss 3.707078695297241\n",
      "Batch 3067 , Loss 3.830075263977051\n",
      "Batch 3068 , Loss 3.8530807495117188\n",
      "Batch 3069 , Loss 3.8013017177581787\n",
      "Batch 3070 , Loss 3.938951253890991\n",
      "Batch 3071 , Loss 3.9217538833618164\n",
      "Batch 3072 , Loss 4.219146728515625\n",
      "Batch 3073 , Loss 3.726353645324707\n",
      "Batch 3074 , Loss 3.8267855644226074\n",
      "Batch 3075 , Loss 3.8155055046081543\n",
      "Batch 3076 , Loss 3.8964662551879883\n",
      "Batch 3077 , Loss 4.029819011688232\n",
      "Batch 3078 , Loss 3.8380589485168457\n",
      "Batch 3079 , Loss 3.7654876708984375\n",
      "Batch 3080 , Loss 3.739309787750244\n",
      "Batch 3081 , Loss 3.586369037628174\n",
      "Batch 3082 , Loss 3.889936923980713\n",
      "Batch 3083 , Loss 3.8388242721557617\n",
      "Batch 3084 , Loss 3.9511656761169434\n",
      "Batch 3085 , Loss 3.7770867347717285\n",
      "Batch 3086 , Loss 3.978968620300293\n",
      "Batch 3087 , Loss 3.6405227184295654\n",
      "Batch 3088 , Loss 3.9176321029663086\n",
      "Batch 3089 , Loss 3.767195701599121\n",
      "Batch 3090 , Loss 3.6146020889282227\n",
      "Batch 3091 , Loss 3.843062400817871\n",
      "Batch 3092 , Loss 3.701000213623047\n",
      "Batch 3093 , Loss 3.6030867099761963\n",
      "Batch 3094 , Loss 3.7032079696655273\n",
      "Batch 3095 , Loss 3.702655076980591\n",
      "Batch 3096 , Loss 3.7273898124694824\n",
      "Batch 3097 , Loss 4.134128570556641\n",
      "Batch 3098 , Loss 3.8456079959869385\n",
      "Batch 3099 , Loss 3.6992673873901367\n",
      "Batch 3100 , Loss 4.198300838470459\n",
      "Batch 3101 , Loss 3.9311070442199707\n",
      "Batch 3102 , Loss 3.683851718902588\n",
      "Batch 3103 , Loss 3.9363620281219482\n",
      "Batch 3104 , Loss 3.7174248695373535\n",
      "Batch 3105 , Loss 3.8651983737945557\n",
      "Batch 3106 , Loss 3.596780300140381\n",
      "Batch 3107 , Loss 3.679509162902832\n",
      "Batch 3108 , Loss 3.8975586891174316\n",
      "Batch 3109 , Loss 3.440631628036499\n",
      "Batch 3110 , Loss 3.798558235168457\n",
      "Batch 3111 , Loss 3.7562103271484375\n",
      "Batch 3112 , Loss 3.9081530570983887\n",
      "Batch 3113 , Loss 3.5875988006591797\n",
      "Batch 3114 , Loss 3.6751041412353516\n",
      "Batch 3115 , Loss 3.686145782470703\n",
      "Batch 3116 , Loss 3.602726936340332\n",
      "Batch 3117 , Loss 3.851508855819702\n",
      "Batch 3118 , Loss 3.6278820037841797\n",
      "Batch 3119 , Loss 3.6733102798461914\n",
      "Batch 3120 , Loss 4.063689231872559\n",
      "Batch 3121 , Loss 3.742340087890625\n",
      "Batch 3122 , Loss 3.864483118057251\n",
      "Batch 3123 , Loss 3.7147059440612793\n",
      "Batch 3124 , Loss 3.6875338554382324\n",
      "Batch 3125 , Loss 3.908329725265503\n",
      "Batch 3126 , Loss 4.067079067230225\n",
      "Batch 3127 , Loss 3.8208632469177246\n",
      "Batch 3128 , Loss 3.6534640789031982\n",
      "Batch 3129 , Loss 3.8959341049194336\n",
      "Batch 3130 , Loss 3.8465209007263184\n",
      "Batch 3131 , Loss 3.599630832672119\n",
      "Batch 3132 , Loss 3.7884440422058105\n",
      "Batch 3133 , Loss 3.678630828857422\n",
      "Batch 3134 , Loss 3.749009132385254\n",
      "Batch 3135 , Loss 3.841604709625244\n",
      "Batch 3136 , Loss 3.477949619293213\n",
      "Batch 3137 , Loss 3.6638588905334473\n",
      "Batch 3138 , Loss 3.5798025131225586\n",
      "Batch 3139 , Loss 3.9896209239959717\n",
      "Batch 3140 , Loss 4.220110893249512\n",
      "Batch 3141 , Loss 3.88710355758667\n",
      "Batch 3142 , Loss 3.509826183319092\n",
      "Batch 3143 , Loss 3.705660343170166\n",
      "Batch 3144 , Loss 3.819406509399414\n",
      "Batch 3145 , Loss 3.87876296043396\n",
      "Batch 3146 , Loss 3.8741705417633057\n",
      "Batch 3147 , Loss 3.8436403274536133\n",
      "Batch 3148 , Loss 3.950742721557617\n",
      "Batch 3149 , Loss 3.9248909950256348\n",
      "Batch 3150 , Loss 3.734621524810791\n",
      "Batch 3151 , Loss 3.9664626121520996\n",
      "Batch 3152 , Loss 3.9835891723632812\n",
      "Batch 3153 , Loss 3.6920101642608643\n",
      "Batch 3154 , Loss 3.5952868461608887\n",
      "Batch 3155 , Loss 4.019758701324463\n",
      "Batch 3156 , Loss 3.7991950511932373\n",
      "Batch 3157 , Loss 3.9036927223205566\n",
      "Batch 3158 , Loss 3.9723706245422363\n",
      "Batch 3159 , Loss 3.9980735778808594\n",
      "Batch 3160 , Loss 4.188092231750488\n",
      "Batch 3161 , Loss 3.876544713973999\n",
      "Batch 3162 , Loss 3.8931260108947754\n",
      "Batch 3163 , Loss 3.7191476821899414\n",
      "Batch 3164 , Loss 3.6652350425720215\n",
      "Batch 3165 , Loss 4.175053596496582\n",
      "Batch 3166 , Loss 3.645369052886963\n",
      "Batch 3167 , Loss 3.7843031883239746\n",
      "Batch 3168 , Loss 3.8070497512817383\n",
      "Batch 3169 , Loss 3.886430263519287\n",
      "Batch 3170 , Loss 3.9051153659820557\n",
      "Batch 3171 , Loss 4.024465084075928\n",
      "Batch 3172 , Loss 3.9914298057556152\n",
      "Batch 3173 , Loss 3.7859580516815186\n",
      "Batch 3174 , Loss 3.92606782913208\n",
      "Batch 3175 , Loss 3.4663360118865967\n",
      "Batch 3176 , Loss 3.9140396118164062\n",
      "Batch 3177 , Loss 3.8353796005249023\n",
      "Batch 3178 , Loss 3.7464375495910645\n",
      "Batch 3179 , Loss 3.6559700965881348\n",
      "Batch 3180 , Loss 3.9007859230041504\n",
      "Batch 3181 , Loss 3.576986789703369\n",
      "Batch 3182 , Loss 3.7455761432647705\n",
      "Batch 3183 , Loss 3.713560104370117\n",
      "Batch 3184 , Loss 4.088826656341553\n",
      "Batch 3185 , Loss 3.808897018432617\n",
      "Batch 3186 , Loss 3.878201723098755\n",
      "Batch 3187 , Loss 3.8904902935028076\n",
      "Batch 3188 , Loss 3.6948447227478027\n",
      "Batch 3189 , Loss 3.7957351207733154\n",
      "Batch 3190 , Loss 3.6056113243103027\n",
      "Batch 3191 , Loss 3.6997883319854736\n",
      "Batch 3192 , Loss 3.718254566192627\n",
      "Batch 3193 , Loss 3.8311314582824707\n",
      "Batch 3194 , Loss 3.8710663318634033\n",
      "Batch 3195 , Loss 3.963930130004883\n",
      "Batch 3196 , Loss 3.70851993560791\n",
      "Batch 3197 , Loss 3.7637479305267334\n",
      "Batch 3198 , Loss 3.929884910583496\n",
      "Batch 3199 , Loss 3.5195348262786865\n",
      "Batch 3200 , Loss 3.679656744003296\n",
      "Batch 3201 , Loss 3.806976079940796\n",
      "Batch 3202 , Loss 3.6871299743652344\n",
      "Batch 3203 , Loss 3.759500026702881\n",
      "Batch 3204 , Loss 3.914907932281494\n",
      "Batch 3205 , Loss 3.8467822074890137\n",
      "Batch 3206 , Loss 3.71309494972229\n",
      "Batch 3207 , Loss 3.9812722206115723\n",
      "Batch 3208 , Loss 3.735758066177368\n",
      "Batch 3209 , Loss 3.7697253227233887\n",
      "Batch 3210 , Loss 3.6901679039001465\n",
      "Batch 3211 , Loss 3.6449875831604004\n",
      "Batch 3212 , Loss 3.4404244422912598\n",
      "Batch 3213 , Loss 3.703413486480713\n",
      "Batch 3214 , Loss 3.92818546295166\n",
      "Batch 3215 , Loss 4.272226333618164\n",
      "Batch 3216 , Loss 3.987802028656006\n",
      "Batch 3217 , Loss 3.5367441177368164\n",
      "Batch 3218 , Loss 3.630659580230713\n",
      "Batch 3219 , Loss 3.960697650909424\n",
      "Batch 3220 , Loss 3.8993730545043945\n",
      "Batch 3221 , Loss 3.840230941772461\n",
      "Batch 3222 , Loss 3.9438748359680176\n",
      "Batch 3223 , Loss 4.110667705535889\n",
      "Batch 3224 , Loss 4.058651924133301\n",
      "Batch 3225 , Loss 3.766824722290039\n",
      "Batch 3226 , Loss 3.7108116149902344\n",
      "Batch 3227 , Loss 3.632473945617676\n",
      "Batch 3228 , Loss 3.9299983978271484\n",
      "Batch 3229 , Loss 3.7233028411865234\n",
      "Batch 3230 , Loss 3.8908772468566895\n",
      "Batch 3231 , Loss 3.774420738220215\n",
      "Batch 3232 , Loss 3.739046573638916\n",
      "Batch 3233 , Loss 3.601205825805664\n",
      "Batch 3234 , Loss 4.042238235473633\n",
      "Batch 3235 , Loss 3.9674391746520996\n",
      "Batch 3236 , Loss 3.671466112136841\n",
      "Batch 3237 , Loss 3.6922364234924316\n",
      "Batch 3238 , Loss 3.6945950984954834\n",
      "Batch 3239 , Loss 3.840526580810547\n",
      "Batch 3240 , Loss 3.8553786277770996\n",
      "Batch 3241 , Loss 3.747335195541382\n",
      "Batch 3242 , Loss 3.9246113300323486\n",
      "Batch 3243 , Loss 3.699693441390991\n",
      "Batch 3244 , Loss 3.8482565879821777\n",
      "Batch 3245 , Loss 3.832672357559204\n",
      "Batch 3246 , Loss 4.1718339920043945\n",
      "Batch 3247 , Loss 3.9772000312805176\n",
      "Batch 3248 , Loss 3.5760977268218994\n",
      "Batch 3249 , Loss 3.819157600402832\n",
      "Batch 3250 , Loss 3.7692997455596924\n",
      "Batch 3251 , Loss 3.9782118797302246\n",
      "Batch 3252 , Loss 3.8752989768981934\n",
      "Batch 3253 , Loss 3.8994059562683105\n",
      "Batch 3254 , Loss 4.0414228439331055\n",
      "Batch 3255 , Loss 3.9279751777648926\n",
      "Batch 3256 , Loss 3.9967613220214844\n",
      "Batch 3257 , Loss 3.763723373413086\n",
      "Batch 3258 , Loss 3.9381160736083984\n",
      "Batch 3259 , Loss 3.8459081649780273\n",
      "Batch 3260 , Loss 3.984330177307129\n",
      "Batch 3261 , Loss 3.9431705474853516\n",
      "Batch 3262 , Loss 4.138631820678711\n",
      "Batch 3263 , Loss 3.883692741394043\n",
      "Batch 3264 , Loss 3.5810232162475586\n",
      "Batch 3265 , Loss 3.632016658782959\n",
      "Batch 3266 , Loss 3.6179871559143066\n",
      "Batch 3267 , Loss 3.7873778343200684\n",
      "Batch 3268 , Loss 3.7094228267669678\n",
      "Batch 3269 , Loss 4.118640899658203\n",
      "Batch 3270 , Loss 3.4587888717651367\n",
      "Batch 3271 , Loss 3.8053975105285645\n",
      "Batch 3272 , Loss 3.7514989376068115\n",
      "Batch 3273 , Loss 3.6103696823120117\n",
      "Batch 3274 , Loss 3.8552651405334473\n",
      "Batch 3275 , Loss 3.666858196258545\n",
      "Batch 3276 , Loss 3.763099193572998\n",
      "Batch 3277 , Loss 3.769794464111328\n",
      "Batch 3278 , Loss 4.028009414672852\n",
      "Batch 3279 , Loss 3.405622959136963\n",
      "Batch 3280 , Loss 3.8833937644958496\n",
      "Batch 3281 , Loss 3.7683117389678955\n",
      "Batch 3282 , Loss 3.8731422424316406\n",
      "Batch 3283 , Loss 3.971832275390625\n",
      "Batch 3284 , Loss 3.8984360694885254\n",
      "Batch 3285 , Loss 3.769552707672119\n",
      "Batch 3286 , Loss 3.925424337387085\n",
      "Batch 3287 , Loss 3.814556837081909\n",
      "Batch 3288 , Loss 3.8009538650512695\n",
      "Batch 3289 , Loss 3.571244716644287\n",
      "Batch 3290 , Loss 3.9399032592773438\n",
      "Batch 3291 , Loss 3.6565380096435547\n",
      "Batch 3292 , Loss 3.942173957824707\n",
      "Batch 3293 , Loss 3.6463887691497803\n",
      "Batch 3294 , Loss 3.73774790763855\n",
      "Batch 3295 , Loss 3.746060371398926\n",
      "Batch 3296 , Loss 3.638500690460205\n",
      "Batch 3297 , Loss 3.747809886932373\n",
      "Batch 3298 , Loss 3.7972970008850098\n",
      "Batch 3299 , Loss 3.952277421951294\n",
      "Batch 3300 , Loss 3.740356206893921\n",
      "Batch 3301 , Loss 3.72717022895813\n",
      "Batch 3302 , Loss 3.6867170333862305\n",
      "Batch 3303 , Loss 3.868983745574951\n",
      "Batch 3304 , Loss 3.8824257850646973\n",
      "Batch 3305 , Loss 3.957760810852051\n",
      "Batch 3306 , Loss 3.754274845123291\n",
      "Batch 3307 , Loss 3.624894618988037\n",
      "Batch 3308 , Loss 4.17490816116333\n",
      "Batch 3309 , Loss 3.93595027923584\n",
      "Batch 3310 , Loss 3.711559295654297\n",
      "Batch 3311 , Loss 3.857473373413086\n",
      "Batch 3312 , Loss 3.9713478088378906\n",
      "Batch 3313 , Loss 3.643799304962158\n",
      "Batch 3314 , Loss 3.671321392059326\n",
      "Batch 3315 , Loss 3.7349295616149902\n",
      "Batch 3316 , Loss 3.7541112899780273\n",
      "Batch 3317 , Loss 3.835404396057129\n",
      "Batch 3318 , Loss 3.913630247116089\n",
      "Batch 3319 , Loss 4.163444519042969\n",
      "Batch 3320 , Loss 3.7441253662109375\n",
      "Batch 3321 , Loss 3.9491524696350098\n",
      "Batch 3322 , Loss 3.844508171081543\n",
      "Batch 3323 , Loss 3.6072213649749756\n",
      "Batch 3324 , Loss 3.5693392753601074\n",
      "Batch 3325 , Loss 3.8833417892456055\n",
      "Batch 3326 , Loss 3.6282520294189453\n",
      "Batch 3327 , Loss 3.907954692840576\n",
      "Batch 3328 , Loss 3.7185685634613037\n",
      "Batch 3329 , Loss 3.587244749069214\n",
      "Batch 3330 , Loss 3.7339630126953125\n",
      "Batch 3331 , Loss 3.944516181945801\n",
      "Batch 3332 , Loss 3.6057753562927246\n",
      "Batch 3333 , Loss 3.9024558067321777\n",
      "Batch 3334 , Loss 3.683940887451172\n",
      "Batch 3335 , Loss 3.805870294570923\n",
      "Batch 3336 , Loss 3.6247787475585938\n",
      "Batch 3337 , Loss 3.9860448837280273\n",
      "Batch 3338 , Loss 3.709620952606201\n",
      "Batch 3339 , Loss 3.7392964363098145\n",
      "Batch 3340 , Loss 3.669620990753174\n",
      "Batch 3341 , Loss 3.8028156757354736\n",
      "Batch 3342 , Loss 3.943105459213257\n",
      "Batch 3343 , Loss 3.8545329570770264\n",
      "Batch 3344 , Loss 3.6425719261169434\n",
      "Batch 3345 , Loss 3.903571844100952\n",
      "Batch 3346 , Loss 4.033994197845459\n",
      "Batch 3347 , Loss 3.9080233573913574\n",
      "Batch 3348 , Loss 3.7484660148620605\n",
      "Batch 3349 , Loss 4.014286994934082\n",
      "Batch 3350 , Loss 4.0851569175720215\n",
      "Batch 3351 , Loss 3.65154767036438\n",
      "Batch 3352 , Loss 3.794128656387329\n",
      "Batch 3353 , Loss 3.6849732398986816\n",
      "Batch 3354 , Loss 3.9103236198425293\n",
      "Batch 3355 , Loss 3.8385324478149414\n",
      "Batch 3356 , Loss 3.8944783210754395\n",
      "Batch 3357 , Loss 4.001925468444824\n",
      "Batch 3358 , Loss 3.9232497215270996\n",
      "Batch 3359 , Loss 3.8977203369140625\n",
      "Batch 3360 , Loss 4.245462417602539\n",
      "Batch 3361 , Loss 3.7493414878845215\n",
      "Batch 3362 , Loss 4.046614646911621\n",
      "Batch 3363 , Loss 3.883042812347412\n",
      "Batch 3364 , Loss 3.9630603790283203\n",
      "Batch 3365 , Loss 3.615025043487549\n",
      "Batch 3366 , Loss 3.746779441833496\n",
      "Batch 3367 , Loss 3.6819052696228027\n",
      "Batch 3368 , Loss 3.628965377807617\n",
      "Batch 3369 , Loss 3.9122512340545654\n",
      "Batch 3370 , Loss 3.8195323944091797\n",
      "Batch 3371 , Loss 3.623635768890381\n",
      "Batch 3372 , Loss 3.765630006790161\n",
      "Batch 3373 , Loss 3.5962538719177246\n",
      "Batch 3374 , Loss 3.7622570991516113\n",
      "Batch 3375 , Loss 3.818295955657959\n",
      "Batch 3376 , Loss 3.7287521362304688\n",
      "Batch 3377 , Loss 3.6300086975097656\n",
      "Batch 3378 , Loss 3.7251007556915283\n",
      "Batch 3379 , Loss 3.6614136695861816\n",
      "Batch 3380 , Loss 3.585299015045166\n",
      "Batch 3381 , Loss 4.093950271606445\n",
      "Batch 3382 , Loss 4.002120018005371\n",
      "Batch 3383 , Loss 3.558094024658203\n",
      "Batch 3384 , Loss 3.7054481506347656\n",
      "Batch 3385 , Loss 3.723637342453003\n",
      "Batch 3386 , Loss 3.8992998600006104\n",
      "Batch 3387 , Loss 3.7256150245666504\n",
      "Batch 3388 , Loss 3.6730470657348633\n",
      "Batch 3389 , Loss 3.4950387477874756\n",
      "Batch 3390 , Loss 3.547004461288452\n",
      "Batch 3391 , Loss 4.103805065155029\n",
      "Batch 3392 , Loss 3.8514564037323\n",
      "Batch 3393 , Loss 3.8347558975219727\n",
      "Batch 3394 , Loss 3.790651798248291\n",
      "Batch 3395 , Loss 3.835988998413086\n",
      "Batch 3396 , Loss 3.62800931930542\n",
      "Batch 3397 , Loss 3.674278736114502\n",
      "Batch 3398 , Loss 3.989494562149048\n",
      "Batch 3399 , Loss 3.6378860473632812\n",
      "Batch 3400 , Loss 3.6196072101593018\n",
      "Batch 3401 , Loss 3.764554023742676\n",
      "Batch 3402 , Loss 3.7269821166992188\n",
      "Batch 3403 , Loss 3.483656406402588\n",
      "Batch 3404 , Loss 3.743821620941162\n",
      "Batch 3405 , Loss 3.7017312049865723\n",
      "Batch 3406 , Loss 3.803802967071533\n",
      "Batch 3407 , Loss 3.940643310546875\n",
      "Batch 3408 , Loss 3.8109521865844727\n",
      "Batch 3409 , Loss 3.7249765396118164\n",
      "Batch 3410 , Loss 3.831627368927002\n",
      "Batch 3411 , Loss 3.993964672088623\n",
      "Batch 3412 , Loss 3.611666679382324\n",
      "Batch 3413 , Loss 3.8859801292419434\n",
      "Batch 3414 , Loss 3.8338117599487305\n",
      "Batch 3415 , Loss 3.8297719955444336\n",
      "Batch 3416 , Loss 3.8262152671813965\n",
      "Batch 3417 , Loss 4.014367580413818\n",
      "Batch 3418 , Loss 3.6293554306030273\n",
      "Batch 3419 , Loss 3.683696985244751\n",
      "Batch 3420 , Loss 3.9333648681640625\n",
      "Batch 3421 , Loss 3.7926111221313477\n",
      "Batch 3422 , Loss 3.631934642791748\n",
      "Batch 3423 , Loss 3.871177911758423\n",
      "Batch 3424 , Loss 3.938199996948242\n",
      "Batch 3425 , Loss 3.888260841369629\n",
      "Batch 3426 , Loss 3.653201103210449\n",
      "Batch 3427 , Loss 3.90602970123291\n",
      "Batch 3428 , Loss 3.98225736618042\n",
      "Batch 3429 , Loss 3.7464678287506104\n",
      "Batch 3430 , Loss 3.624885082244873\n",
      "Batch 3431 , Loss 3.906144618988037\n",
      "Batch 3432 , Loss 3.9775776863098145\n",
      "Batch 3433 , Loss 3.9912238121032715\n",
      "Batch 3434 , Loss 3.7702319622039795\n",
      "Batch 3435 , Loss 3.7584733963012695\n",
      "Batch 3436 , Loss 3.7546322345733643\n",
      "Batch 3437 , Loss 3.5985631942749023\n",
      "Batch 3438 , Loss 3.4865479469299316\n",
      "Batch 3439 , Loss 4.0060224533081055\n",
      "Batch 3440 , Loss 3.7832207679748535\n",
      "Batch 3441 , Loss 3.7789978981018066\n",
      "Batch 3442 , Loss 3.8370485305786133\n",
      "Batch 3443 , Loss 3.7977218627929688\n",
      "Batch 3444 , Loss 3.949802875518799\n",
      "Batch 3445 , Loss 3.6680500507354736\n",
      "Batch 3446 , Loss 3.78369140625\n",
      "Batch 3447 , Loss 3.755624771118164\n",
      "Batch 3448 , Loss 3.761471748352051\n",
      "Batch 3449 , Loss 3.774991750717163\n",
      "Batch 3450 , Loss 3.8797430992126465\n",
      "Batch 3451 , Loss 3.595461368560791\n",
      "Batch 3452 , Loss 3.647301197052002\n",
      "Batch 3453 , Loss 3.609138011932373\n",
      "Batch 3454 , Loss 3.9865000247955322\n",
      "Batch 3455 , Loss 4.024386405944824\n",
      "Batch 3456 , Loss 3.8204402923583984\n",
      "Batch 3457 , Loss 3.727133274078369\n",
      "Batch 3458 , Loss 4.076907634735107\n",
      "Batch 3459 , Loss 3.7320239543914795\n",
      "Batch 3460 , Loss 3.4867959022521973\n",
      "Batch 3461 , Loss 3.5635275840759277\n",
      "Batch 3462 , Loss 3.7693567276000977\n",
      "Batch 3463 , Loss 3.5854153633117676\n",
      "Batch 3464 , Loss 3.7487142086029053\n",
      "Batch 3465 , Loss 3.7599406242370605\n",
      "Batch 3466 , Loss 3.7655558586120605\n",
      "Batch 3467 , Loss 3.9382693767547607\n",
      "Batch 3468 , Loss 3.745734214782715\n",
      "Batch 3469 , Loss 3.7233734130859375\n",
      "Batch 3470 , Loss 3.922471761703491\n",
      "Batch 3471 , Loss 3.5981931686401367\n",
      "Batch 3472 , Loss 3.7985286712646484\n",
      "Batch 3473 , Loss 3.9092183113098145\n",
      "Batch 3474 , Loss 4.032284259796143\n",
      "Batch 3475 , Loss 3.8183765411376953\n",
      "Batch 3476 , Loss 3.971123695373535\n",
      "Batch 3477 , Loss 3.7181642055511475\n",
      "Batch 3478 , Loss 3.8671631813049316\n",
      "Batch 3479 , Loss 3.9878952503204346\n",
      "Batch 3480 , Loss 3.6875762939453125\n",
      "Batch 3481 , Loss 3.8202590942382812\n",
      "Batch 3482 , Loss 3.6595687866210938\n",
      "Batch 3483 , Loss 3.827141761779785\n",
      "Batch 3484 , Loss 4.038081169128418\n",
      "Batch 3485 , Loss 3.7481298446655273\n",
      "Batch 3486 , Loss 3.82061767578125\n",
      "Batch 3487 , Loss 3.7351293563842773\n",
      "Batch 3488 , Loss 4.062016487121582\n",
      "Batch 3489 , Loss 3.830209732055664\n",
      "Batch 3490 , Loss 3.7590508460998535\n",
      "Batch 3491 , Loss 3.802436590194702\n",
      "Batch 3492 , Loss 3.931840419769287\n",
      "Batch 3493 , Loss 3.620502471923828\n",
      "Batch 3494 , Loss 3.688936948776245\n",
      "Batch 3495 , Loss 4.068886756896973\n",
      "Batch 3496 , Loss 4.099176406860352\n",
      "Batch 3497 , Loss 3.8135740756988525\n",
      "Batch 3498 , Loss 3.7784342765808105\n",
      "Batch 3499 , Loss 3.71010684967041\n",
      "Batch 3500 , Loss 3.727928876876831\n",
      "Batch 3501 , Loss 3.776373863220215\n",
      "Batch 3502 , Loss 4.01798152923584\n",
      "Batch 3503 , Loss 3.6442956924438477\n",
      "Batch 3504 , Loss 3.701810836791992\n",
      "Batch 3505 , Loss 3.8640573024749756\n",
      "Batch 3506 , Loss 3.792966842651367\n",
      "Batch 3507 , Loss 3.8158960342407227\n",
      "Batch 3508 , Loss 4.1865034103393555\n",
      "Batch 3509 , Loss 3.6274752616882324\n",
      "Batch 3510 , Loss 3.659506320953369\n",
      "Batch 3511 , Loss 4.12774658203125\n",
      "Batch 3512 , Loss 4.456945896148682\n",
      "Batch 3513 , Loss 3.6777186393737793\n",
      "Batch 3514 , Loss 3.9045581817626953\n",
      "Batch 3515 , Loss 3.8341524600982666\n",
      "Batch 3516 , Loss 3.5851011276245117\n",
      "Batch 3517 , Loss 3.873579502105713\n",
      "Batch 3518 , Loss 3.756351947784424\n",
      "Batch 3519 , Loss 3.834885597229004\n",
      "Batch 3520 , Loss 3.793243885040283\n",
      "Batch 3521 , Loss 3.712818145751953\n",
      "Batch 3522 , Loss 3.5719478130340576\n",
      "Batch 3523 , Loss 4.044610023498535\n",
      "Batch 3524 , Loss 3.771677017211914\n",
      "Batch 3525 , Loss 3.7444372177124023\n",
      "Batch 3526 , Loss 3.7094998359680176\n",
      "Batch 3527 , Loss 3.6695175170898438\n",
      "Batch 3528 , Loss 3.613476276397705\n",
      "Batch 3529 , Loss 3.586704730987549\n",
      "Batch 3530 , Loss 3.6618330478668213\n",
      "Batch 3531 , Loss 3.7745308876037598\n",
      "Batch 3532 , Loss 3.8483705520629883\n",
      "Batch 3533 , Loss 3.83687424659729\n",
      "Batch 3534 , Loss 3.818751573562622\n",
      "Batch 3535 , Loss 3.7779107093811035\n",
      "Batch 3536 , Loss 3.5840351581573486\n",
      "Batch 3537 , Loss 3.754315137863159\n",
      "Batch 3538 , Loss 3.868286609649658\n",
      "Batch 3539 , Loss 4.079906463623047\n",
      "Batch 3540 , Loss 3.6479921340942383\n",
      "Batch 3541 , Loss 3.6561598777770996\n",
      "Batch 3542 , Loss 3.88118314743042\n",
      "Batch 3543 , Loss 3.8792166709899902\n",
      "Batch 3544 , Loss 3.7208800315856934\n",
      "Batch 3545 , Loss 3.75766921043396\n",
      "Batch 3546 , Loss 3.796264886856079\n",
      "Batch 3547 , Loss 3.8634119033813477\n",
      "Batch 3548 , Loss 3.7266674041748047\n",
      "Batch 3549 , Loss 3.869169235229492\n",
      "Batch 3550 , Loss 4.195734024047852\n",
      "Batch 3551 , Loss 3.9707088470458984\n",
      "Batch 3552 , Loss 3.5705254077911377\n",
      "Batch 3553 , Loss 3.4850876331329346\n",
      "Batch 3554 , Loss 3.8569602966308594\n",
      "Batch 3555 , Loss 3.8270156383514404\n",
      "Batch 3556 , Loss 3.6978442668914795\n",
      "Batch 3557 , Loss 4.046036243438721\n",
      "Batch 3558 , Loss 3.8109583854675293\n",
      "Batch 3559 , Loss 3.7491469383239746\n",
      "Batch 3560 , Loss 3.677727222442627\n",
      "Batch 3561 , Loss 3.7541322708129883\n",
      "Batch 3562 , Loss 3.701469898223877\n",
      "Batch 3563 , Loss 3.7376132011413574\n",
      "Batch 3564 , Loss 4.1091742515563965\n",
      "Batch 3565 , Loss 4.036099433898926\n",
      "Batch 3566 , Loss 3.6310834884643555\n",
      "Batch 3567 , Loss 3.7614927291870117\n",
      "Batch 3568 , Loss 3.7651748657226562\n",
      "Batch 3569 , Loss 3.542247772216797\n",
      "Batch 3570 , Loss 3.6713428497314453\n",
      "Batch 3571 , Loss 3.6696553230285645\n",
      "Batch 3572 , Loss 3.7639472484588623\n",
      "Batch 3573 , Loss 3.694514513015747\n",
      "Batch 3574 , Loss 3.96079158782959\n",
      "Batch 3575 , Loss 3.4571752548217773\n",
      "Batch 3576 , Loss 3.74050235748291\n",
      "Batch 3577 , Loss 3.5986995697021484\n",
      "Batch 3578 , Loss 3.5782768726348877\n",
      "Batch 3579 , Loss 3.9029111862182617\n",
      "Batch 3580 , Loss 3.6450796127319336\n",
      "Batch 3581 , Loss 3.770840644836426\n",
      "Batch 3582 , Loss 3.823683023452759\n",
      "Batch 3583 , Loss 3.7255263328552246\n",
      "Batch 3584 , Loss 3.6146914958953857\n",
      "Batch 3585 , Loss 3.750208616256714\n",
      "Batch 3586 , Loss 3.901860237121582\n",
      "Batch 3587 , Loss 3.747298240661621\n",
      "Batch 3588 , Loss 3.841981887817383\n",
      "Batch 3589 , Loss 3.622419834136963\n",
      "Batch 3590 , Loss 3.6603198051452637\n",
      "Batch 3591 , Loss 3.9041223526000977\n",
      "Batch 3592 , Loss 3.8850460052490234\n",
      "Batch 3593 , Loss 3.772984027862549\n",
      "Batch 3594 , Loss 3.7189292907714844\n",
      "Batch 3595 , Loss 3.710176944732666\n",
      "Batch 3596 , Loss 3.814603328704834\n",
      "Batch 3597 , Loss 3.5205729007720947\n",
      "Batch 3598 , Loss 3.732687473297119\n",
      "Batch 3599 , Loss 3.4098455905914307\n",
      "Batch 3600 , Loss 3.750875473022461\n",
      "Batch 3601 , Loss 3.860520839691162\n",
      "Batch 3602 , Loss 4.065830230712891\n",
      "Batch 3603 , Loss 3.826101303100586\n",
      "Batch 3604 , Loss 4.132526397705078\n",
      "Batch 3605 , Loss 3.958587646484375\n",
      "Batch 3606 , Loss 4.1732330322265625\n",
      "Batch 3607 , Loss 3.6087539196014404\n",
      "Batch 3608 , Loss 3.675022602081299\n",
      "Batch 3609 , Loss 3.8234972953796387\n",
      "Batch 3610 , Loss 3.9350597858428955\n",
      "Batch 3611 , Loss 4.061799049377441\n",
      "Batch 3612 , Loss 3.878957748413086\n",
      "Batch 3613 , Loss 3.592484712600708\n",
      "Batch 3614 , Loss 3.8776121139526367\n",
      "Batch 3615 , Loss 3.8097500801086426\n",
      "Batch 3616 , Loss 3.68406343460083\n",
      "Batch 3617 , Loss 3.7570462226867676\n",
      "Batch 3618 , Loss 3.573798179626465\n",
      "Batch 3619 , Loss 4.376077651977539\n",
      "Batch 3620 , Loss 3.979949474334717\n",
      "Batch 3621 , Loss 3.6955983638763428\n",
      "Batch 3622 , Loss 3.7320632934570312\n",
      "Batch 3623 , Loss 3.949090003967285\n",
      "Batch 3624 , Loss 3.9267640113830566\n",
      "Batch 3625 , Loss 3.5571131706237793\n",
      "Batch 3626 , Loss 3.827582836151123\n",
      "Batch 3627 , Loss 4.072614669799805\n",
      "Batch 3628 , Loss 3.753309726715088\n",
      "Batch 3629 , Loss 3.862743377685547\n",
      "Batch 3630 , Loss 3.832166910171509\n",
      "Batch 3631 , Loss 3.6814680099487305\n",
      "Batch 3632 , Loss 3.7464911937713623\n",
      "Batch 3633 , Loss 3.9182567596435547\n",
      "Batch 3634 , Loss 3.8418192863464355\n",
      "Batch 3635 , Loss 3.7522170543670654\n",
      "Batch 3636 , Loss 3.6918280124664307\n",
      "Batch 3637 , Loss 3.675935745239258\n",
      "Batch 3638 , Loss 4.0693230628967285\n",
      "Batch 3639 , Loss 3.9720656871795654\n",
      "Batch 3640 , Loss 3.6650142669677734\n",
      "Batch 3641 , Loss 3.9833264350891113\n",
      "Batch 3642 , Loss 4.005337715148926\n",
      "Batch 3643 , Loss 3.7358245849609375\n",
      "Batch 3644 , Loss 3.656358480453491\n",
      "Batch 3645 , Loss 3.9837074279785156\n",
      "Batch 3646 , Loss 3.719027519226074\n",
      "Batch 3647 , Loss 3.7753796577453613\n",
      "Batch 3648 , Loss 3.8041882514953613\n",
      "Batch 3649 , Loss 3.4126272201538086\n",
      "Batch 3650 , Loss 3.5318093299865723\n",
      "Batch 3651 , Loss 3.6479668617248535\n",
      "Batch 3652 , Loss 3.6674399375915527\n",
      "Batch 3653 , Loss 3.6186249256134033\n",
      "Batch 3654 , Loss 3.709749221801758\n",
      "Batch 3655 , Loss 3.7589502334594727\n",
      "Batch 3656 , Loss 3.764054775238037\n",
      "Batch 3657 , Loss 4.177828788757324\n",
      "Batch 3658 , Loss 3.5351152420043945\n",
      "Batch 3659 , Loss 3.842170238494873\n",
      "Batch 3660 , Loss 3.766988754272461\n",
      "Batch 3661 , Loss 3.711066246032715\n",
      "Batch 3662 , Loss 4.015809059143066\n",
      "Batch 3663 , Loss 3.9219186305999756\n",
      "Batch 3664 , Loss 3.888512372970581\n",
      "Batch 3665 , Loss 3.5222418308258057\n",
      "Batch 3666 , Loss 3.9146175384521484\n",
      "Batch 3667 , Loss 3.7806859016418457\n",
      "Batch 3668 , Loss 3.841890335083008\n",
      "Batch 3669 , Loss 3.4147560596466064\n",
      "Batch 3670 , Loss 4.051328182220459\n",
      "Batch 3671 , Loss 3.885279417037964\n",
      "Batch 3672 , Loss 3.600155830383301\n",
      "Batch 3673 , Loss 3.804572582244873\n",
      "Batch 3674 , Loss 3.568190574645996\n",
      "Batch 3675 , Loss 3.864652156829834\n",
      "Batch 3676 , Loss 3.8470749855041504\n",
      "Batch 3677 , Loss 3.5979971885681152\n",
      "Batch 3678 , Loss 3.7446818351745605\n",
      "Batch 3679 , Loss 3.6710205078125\n",
      "Batch 3680 , Loss 3.6933035850524902\n",
      "Batch 3681 , Loss 3.7260897159576416\n",
      "Batch 3682 , Loss 4.116668224334717\n",
      "Batch 3683 , Loss 3.8509771823883057\n",
      "Batch 3684 , Loss 3.5672168731689453\n",
      "Batch 3685 , Loss 3.677053928375244\n",
      "Batch 3686 , Loss 3.8291468620300293\n",
      "Batch 3687 , Loss 3.6265697479248047\n",
      "Batch 3688 , Loss 3.9015419483184814\n",
      "Batch 3689 , Loss 3.876330852508545\n",
      "Batch 3690 , Loss 3.506392002105713\n",
      "Batch 3691 , Loss 3.6757540702819824\n",
      "Batch 3692 , Loss 3.9262452125549316\n",
      "Batch 3693 , Loss 3.6334738731384277\n",
      "Batch 3694 , Loss 4.011366367340088\n",
      "Batch 3695 , Loss 3.874351978302002\n",
      "Batch 3696 , Loss 3.7527928352355957\n",
      "Batch 3697 , Loss 3.6725656986236572\n",
      "Batch 3698 , Loss 3.5122365951538086\n",
      "Batch 3699 , Loss 3.79317569732666\n",
      "Batch 3700 , Loss 3.7987637519836426\n",
      "Batch 3701 , Loss 3.658482313156128\n",
      "Batch 3702 , Loss 4.038320541381836\n",
      "Batch 3703 , Loss 3.750084161758423\n",
      "Batch 3704 , Loss 3.5963282585144043\n",
      "Batch 3705 , Loss 3.9961791038513184\n",
      "Batch 3706 , Loss 3.8559420108795166\n",
      "Batch 3707 , Loss 3.898909568786621\n",
      "Batch 3708 , Loss 3.5537400245666504\n",
      "Batch 3709 , Loss 3.6939992904663086\n",
      "Batch 3710 , Loss 3.717155933380127\n",
      "Batch 3711 , Loss 3.539616346359253\n",
      "Batch 3712 , Loss 3.471951961517334\n",
      "Batch 3713 , Loss 3.78770112991333\n",
      "Batch 3714 , Loss 4.012655258178711\n",
      "Batch 3715 , Loss 3.7048659324645996\n",
      "Batch 3716 , Loss 3.9675700664520264\n",
      "Batch 3717 , Loss 3.860569715499878\n",
      "Batch 3718 , Loss 3.600454807281494\n",
      "Batch 3719 , Loss 4.078906059265137\n",
      "Batch 3720 , Loss 3.5350775718688965\n",
      "Batch 3721 , Loss 3.8787262439727783\n",
      "Batch 3722 , Loss 3.5826547145843506\n",
      "Batch 3723 , Loss 3.9013242721557617\n",
      "Batch 3724 , Loss 3.6717422008514404\n",
      "Batch 3725 , Loss 3.694474220275879\n",
      "Batch 3726 , Loss 3.8311140537261963\n",
      "Batch 3727 , Loss 3.9042129516601562\n",
      "Batch 3728 , Loss 3.967926025390625\n",
      "Batch 3729 , Loss 3.7525267601013184\n",
      "Batch 3730 , Loss 3.6845884323120117\n",
      "Batch 3731 , Loss 3.9212145805358887\n",
      "Batch 3732 , Loss 3.5570995807647705\n",
      "Batch 3733 , Loss 3.8155245780944824\n",
      "Batch 3734 , Loss 3.579977035522461\n",
      "Batch 3735 , Loss 3.857840061187744\n",
      "Batch 3736 , Loss 3.7735137939453125\n",
      "Batch 3737 , Loss 3.6854448318481445\n",
      "Batch 3738 , Loss 3.568819046020508\n",
      "Batch 3739 , Loss 3.636603355407715\n",
      "Batch 3740 , Loss 3.88208270072937\n",
      "Batch 3741 , Loss 3.7858335971832275\n",
      "Batch 3742 , Loss 3.5964927673339844\n",
      "Batch 3743 , Loss 3.6115267276763916\n",
      "Batch 3744 , Loss 3.5336649417877197\n",
      "Batch 3745 , Loss 3.8609626293182373\n",
      "Batch 3746 , Loss 3.8876500129699707\n",
      "Batch 3747 , Loss 3.5302894115448\n",
      "Batch 3748 , Loss 3.685032606124878\n",
      "Batch 3749 , Loss 3.8520989418029785\n",
      "Batch 3750 , Loss 3.832951545715332\n",
      "Batch 3751 , Loss 3.6623525619506836\n",
      "Batch 3752 , Loss 3.9912796020507812\n",
      "Batch 3753 , Loss 4.0940704345703125\n",
      "Batch 3754 , Loss 3.6315879821777344\n",
      "Batch 3755 , Loss 3.6906094551086426\n",
      "Batch 3756 , Loss 3.5472002029418945\n",
      "Batch 3757 , Loss 3.9146018028259277\n",
      "Batch 3758 , Loss 3.9750514030456543\n",
      "Batch 3759 , Loss 4.020122528076172\n",
      "Batch 3760 , Loss 4.104394912719727\n",
      "Batch 3761 , Loss 3.7338576316833496\n",
      "Batch 3762 , Loss 4.000925064086914\n",
      "Batch 3763 , Loss 3.6518635749816895\n",
      "Batch 3764 , Loss 3.5009336471557617\n",
      "Batch 3765 , Loss 3.573498249053955\n",
      "Batch 3766 , Loss 3.693297863006592\n",
      "Batch 3767 , Loss 3.8472652435302734\n",
      "Batch 3768 , Loss 3.828371524810791\n",
      "Batch 3769 , Loss 3.578770875930786\n",
      "Batch 3770 , Loss 3.7878105640411377\n",
      "Batch 3771 , Loss 3.959691047668457\n",
      "Batch 3772 , Loss 3.6071269512176514\n",
      "Batch 3773 , Loss 3.7587547302246094\n",
      "Batch 3774 , Loss 3.7845191955566406\n",
      "Batch 3775 , Loss 3.726156711578369\n",
      "Batch 3776 , Loss 3.7805168628692627\n",
      "Batch 3777 , Loss 3.7300376892089844\n",
      "Batch 3778 , Loss 3.7907400131225586\n",
      "Batch 3779 , Loss 3.91705322265625\n",
      "Batch 3780 , Loss 3.4795899391174316\n",
      "Batch 3781 , Loss 3.9494755268096924\n",
      "Batch 3782 , Loss 3.8003077507019043\n",
      "Batch 3783 , Loss 3.706343173980713\n",
      "Batch 3784 , Loss 3.3610072135925293\n",
      "Batch 3785 , Loss 3.6016054153442383\n",
      "Batch 3786 , Loss 3.7817957401275635\n",
      "Batch 3787 , Loss 3.6937801837921143\n",
      "Batch 3788 , Loss 3.653477191925049\n",
      "Batch 3789 , Loss 3.7782540321350098\n",
      "Batch 3790 , Loss 3.8636138439178467\n",
      "Batch 3791 , Loss 3.9265174865722656\n",
      "Batch 3792 , Loss 3.833366870880127\n",
      "Batch 3793 , Loss 3.979921340942383\n",
      "Batch 3794 , Loss 3.8510875701904297\n",
      "Batch 3795 , Loss 3.6383931636810303\n",
      "Batch 3796 , Loss 3.7412757873535156\n",
      "Batch 3797 , Loss 3.8567209243774414\n",
      "Batch 3798 , Loss 3.8162026405334473\n",
      "Batch 3799 , Loss 3.532533645629883\n",
      "Batch 3800 , Loss 3.8735156059265137\n",
      "Batch 3801 , Loss 3.7652511596679688\n",
      "Batch 3802 , Loss 3.594456672668457\n",
      "Batch 3803 , Loss 3.812559127807617\n",
      "Batch 3804 , Loss 3.764416456222534\n",
      "Batch 3805 , Loss 3.805107593536377\n",
      "Batch 3806 , Loss 3.672410726547241\n",
      "Batch 3807 , Loss 4.0573272705078125\n",
      "Batch 3808 , Loss 3.703644275665283\n",
      "Batch 3809 , Loss 3.7007927894592285\n",
      "Batch 3810 , Loss 4.130221843719482\n",
      "Batch 3811 , Loss 3.424055576324463\n",
      "Batch 3812 , Loss 3.823474407196045\n",
      "Batch 3813 , Loss 3.8108363151550293\n",
      "Batch 3814 , Loss 3.5760116577148438\n",
      "Batch 3815 , Loss 3.720367670059204\n",
      "Batch 3816 , Loss 3.7483184337615967\n",
      "Batch 3817 , Loss 3.768523693084717\n",
      "Batch 3818 , Loss 3.8725008964538574\n",
      "Batch 3819 , Loss 3.6113638877868652\n",
      "Batch 3820 , Loss 3.7555055618286133\n",
      "Batch 3821 , Loss 3.722614288330078\n",
      "Batch 3822 , Loss 3.512467384338379\n",
      "Batch 3823 , Loss 3.899775505065918\n",
      "Batch 3824 , Loss 3.748072624206543\n",
      "Batch 3825 , Loss 3.736574172973633\n",
      "Batch 3826 , Loss 3.722644805908203\n",
      "Batch 3827 , Loss 4.002389430999756\n",
      "Batch 3828 , Loss 3.990326404571533\n",
      "Batch 3829 , Loss 3.8054566383361816\n",
      "Batch 3830 , Loss 3.962155342102051\n",
      "Batch 3831 , Loss 3.5657317638397217\n",
      "Batch 3832 , Loss 4.082324028015137\n",
      "Batch 3833 , Loss 3.509566068649292\n",
      "Batch 3834 , Loss 3.728898048400879\n",
      "Batch 3835 , Loss 3.5780248641967773\n",
      "Batch 3836 , Loss 3.908653974533081\n",
      "Batch 3837 , Loss 3.5129075050354004\n",
      "Batch 3838 , Loss 3.7904043197631836\n",
      "Batch 3839 , Loss 3.860090494155884\n",
      "Batch 3840 , Loss 3.6858110427856445\n",
      "Batch 3841 , Loss 3.656005382537842\n",
      "Batch 3842 , Loss 3.777073383331299\n",
      "Batch 3843 , Loss 3.684223175048828\n",
      "Batch 3844 , Loss 3.7003531455993652\n",
      "Batch 3845 , Loss 3.6010985374450684\n",
      "Batch 3846 , Loss 3.8122920989990234\n",
      "Batch 3847 , Loss 3.8572258949279785\n",
      "Batch 3848 , Loss 3.875885248184204\n",
      "Batch 3849 , Loss 3.8268089294433594\n",
      "Batch 3850 , Loss 3.895188808441162\n",
      "Batch 3851 , Loss 3.734374761581421\n",
      "Batch 3852 , Loss 3.651615619659424\n",
      "Batch 3853 , Loss 3.90311861038208\n",
      "Batch 3854 , Loss 3.819897174835205\n",
      "Batch 3855 , Loss 3.6860036849975586\n",
      "Batch 3856 , Loss 3.7951040267944336\n",
      "Batch 3857 , Loss 3.486079216003418\n",
      "Batch 3858 , Loss 4.033493995666504\n",
      "Batch 3859 , Loss 3.8168461322784424\n",
      "Batch 3860 , Loss 3.477334499359131\n",
      "Batch 3861 , Loss 4.138095378875732\n",
      "Batch 3862 , Loss 3.776554584503174\n",
      "Batch 3863 , Loss 3.709601402282715\n",
      "Batch 3864 , Loss 3.6404666900634766\n",
      "Batch 3865 , Loss 3.9074690341949463\n",
      "Batch 3866 , Loss 3.6218953132629395\n",
      "Batch 3867 , Loss 3.732816219329834\n",
      "Batch 3868 , Loss 3.575204372406006\n",
      "Batch 3869 , Loss 3.6244044303894043\n",
      "Batch 3870 , Loss 3.4005627632141113\n",
      "Batch 3871 , Loss 4.006595134735107\n",
      "Batch 3872 , Loss 3.7443292140960693\n",
      "Batch 3873 , Loss 3.7291126251220703\n",
      "Batch 3874 , Loss 3.760697364807129\n",
      "Batch 3875 , Loss 3.825322151184082\n",
      "Batch 3876 , Loss 3.8984508514404297\n",
      "Batch 3877 , Loss 3.8999481201171875\n",
      "Batch 3878 , Loss 3.596554756164551\n",
      "Batch 3879 , Loss 4.122475624084473\n",
      "Batch 3880 , Loss 3.857311964035034\n",
      "Batch 3881 , Loss 3.5987343788146973\n",
      "Batch 3882 , Loss 3.8341293334960938\n",
      "Batch 3883 , Loss 3.782289505004883\n",
      "Batch 3884 , Loss 3.8001468181610107\n",
      "Batch 3885 , Loss 3.7068867683410645\n",
      "Batch 3886 , Loss 3.95158052444458\n",
      "Batch 3887 , Loss 3.719048500061035\n",
      "Batch 3888 , Loss 3.643014907836914\n",
      "Batch 3889 , Loss 3.9097957611083984\n",
      "Batch 3890 , Loss 3.6423492431640625\n",
      "Batch 3891 , Loss 3.5789313316345215\n",
      "Batch 3892 , Loss 3.5793325901031494\n",
      "Batch 3893 , Loss 3.9673097133636475\n",
      "Batch 3894 , Loss 3.7581729888916016\n",
      "Batch 3895 , Loss 3.7836408615112305\n",
      "Batch 3896 , Loss 3.6056289672851562\n",
      "Batch 3897 , Loss 3.5735299587249756\n",
      "Batch 3898 , Loss 4.021174430847168\n",
      "Batch 3899 , Loss 3.947150707244873\n",
      "Batch 3900 , Loss 3.6721010208129883\n",
      "Batch 3901 , Loss 3.819253921508789\n",
      "Batch 3902 , Loss 3.9285497665405273\n",
      "Batch 3903 , Loss 3.5217599868774414\n",
      "Batch 3904 , Loss 3.6651077270507812\n",
      "Batch 3905 , Loss 3.7671053409576416\n",
      "Batch 3906 , Loss 3.835561513900757\n",
      "Batch 3907 , Loss 3.720759153366089\n",
      "Batch 3908 , Loss 3.653728485107422\n",
      "Batch 3909 , Loss 3.8960676193237305\n",
      "Batch 3910 , Loss 3.8266263008117676\n",
      "Batch 3911 , Loss 4.071825981140137\n",
      "Batch 3912 , Loss 3.870819568634033\n",
      "Batch 3913 , Loss 3.548668384552002\n",
      "Batch 3914 , Loss 3.949474334716797\n",
      "Batch 3915 , Loss 3.968299388885498\n",
      "Batch 3916 , Loss 3.837512493133545\n",
      "Batch 3917 , Loss 3.973759174346924\n",
      "Batch 3918 , Loss 4.0245280265808105\n",
      "Batch 3919 , Loss 3.6207079887390137\n",
      "Batch 3920 , Loss 3.6864938735961914\n",
      "Batch 3921 , Loss 3.5745904445648193\n",
      "Batch 3922 , Loss 3.5507822036743164\n",
      "Batch 3923 , Loss 3.8536877632141113\n",
      "Batch 3924 , Loss 3.6722097396850586\n",
      "Batch 3925 , Loss 3.5813403129577637\n",
      "Batch 3926 , Loss 4.037466049194336\n",
      "Batch 3927 , Loss 3.802973747253418\n",
      "Batch 3928 , Loss 3.9061169624328613\n",
      "Batch 3929 , Loss 3.551939010620117\n",
      "Batch 3930 , Loss 3.6660196781158447\n",
      "Batch 3931 , Loss 3.8948311805725098\n",
      "Batch 3932 , Loss 3.7973897457122803\n",
      "Batch 3933 , Loss 3.917573928833008\n",
      "Batch 3934 , Loss 3.7505292892456055\n",
      "Batch 3935 , Loss 3.8205904960632324\n",
      "Batch 3936 , Loss 3.761775493621826\n",
      "Batch 3937 , Loss 4.098639965057373\n",
      "Batch 3938 , Loss 3.8957695960998535\n",
      "Batch 3939 , Loss 3.7371392250061035\n",
      "Batch 3940 , Loss 3.560176372528076\n",
      "Batch 3941 , Loss 3.9233028888702393\n",
      "Batch 3942 , Loss 3.6042580604553223\n",
      "Batch 3943 , Loss 3.703613042831421\n",
      "Batch 3944 , Loss 3.6999998092651367\n",
      "Batch 3945 , Loss 3.8675484657287598\n",
      "Batch 3946 , Loss 3.762331247329712\n",
      "Batch 3947 , Loss 3.6168713569641113\n",
      "Batch 3948 , Loss 3.591581106185913\n",
      "Batch 3949 , Loss 3.605299949645996\n",
      "Batch 3950 , Loss 3.835003137588501\n",
      "Batch 3951 , Loss 3.6444602012634277\n",
      "Batch 3952 , Loss 3.9448351860046387\n",
      "Batch 3953 , Loss 3.589867115020752\n",
      "Batch 3954 , Loss 3.9327898025512695\n",
      "Batch 3955 , Loss 4.006671905517578\n",
      "Batch 3956 , Loss 3.5783231258392334\n",
      "Batch 3957 , Loss 3.77902889251709\n",
      "Batch 3958 , Loss 3.7428505420684814\n",
      "Batch 3959 , Loss 3.636073589324951\n",
      "Batch 3960 , Loss 3.9809980392456055\n",
      "Batch 3961 , Loss 3.5028207302093506\n",
      "Batch 3962 , Loss 3.9508490562438965\n",
      "Batch 3963 , Loss 3.988762378692627\n",
      "Batch 3964 , Loss 3.903585433959961\n",
      "Batch 3965 , Loss 3.731571674346924\n",
      "Batch 3966 , Loss 3.8160736560821533\n",
      "Batch 3967 , Loss 3.8881115913391113\n",
      "Batch 3968 , Loss 3.874157428741455\n",
      "Batch 3969 , Loss 3.667360305786133\n",
      "Batch 3970 , Loss 4.018692493438721\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformer = Transformer(N, dmodel, h, dff, pdrop, src_vocab_size, trg_vocab_size, max_length, PAD_trg, PAD_trg, BOS_trg, EOS_trg).to(device)\n",
    "transformer.trainer(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Je vous aime.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.predict(['I love you'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
